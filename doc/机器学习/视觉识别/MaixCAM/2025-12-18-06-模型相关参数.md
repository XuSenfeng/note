# 模型相关参数

## dual_buff

模型运行相关的的代码初始化时有一个参数`dual_buff=True`, 使能这个功能后运行的效率会提升，即帧率会提升（以上代码假设摄像头的帧率没有限制的情况下，在 MaixCAM 上会减少循环一半的时间即帧率翻倍）

但是也有缺点，`detect`函数返回的结果是上一次调用`detect`函数的图的结果，所以结果和输入会有一帧的时间差, 另外由于准备了双份缓冲区，也会加大内存的使用

### 原理

模型检测物体分为了几步：

- 获取图像
- 图像预处理
- 模型运行
- 结果后处理

其中只有模型运行这一步是硬件NPU 上运行的，其它步骤都在 CPU 运行

如果`dual_buff`设置为`False`，在`detect`的时候，CPU 先预处理（此时 NPU 空闲）， 然后给 NPU 运算（此时 CPU 空闲等待 NPU 运算结束），然后 CPU 后处理（NPU 空闲）， 整过过程是线性的，比较简单。
但是这里发现了问题，就是 CPU 和 NPU 两者总有一个空闲着的，当加了`dual_buff=True`， CPU 预处理后交给 NPU 运算，此时 CPU 不再等待 NPU 出结果，二是直接退出`detect`函数进行下一次摄像头读取和预处理，等 NPU 运算完成后， CPU 已经准备好了下一次的数据直接交给 NPU 继续运算，不给 NPU 喘息的机会，这样就充分利用了 CPU 和 NPU 高效地同时进行运算

## 模型转换

[tpu-mlir/README_cn.md at master · sophgo/tpu-mlir](https://github.com/sophgo/tpu-mlir/blob/master/README_cn.md)

实际运行的模型文件需要是使用算能的TPU框架, 所以需要一个专门的格式bmodel, 算能提供专门的转换工具

### 大模型

按照文档按照对应的docker环境, 使用以下的命令进行转换

```bash
llm_convert.py -m /workspace/Qwen2.5-VL-3B-Instruct-AWQ -s 2048 -q w4bf16  -c bm1684x  --max_pixels 672,896 -o qwen2.5vl_3b
```

| **参数名**   | **简写** | 必选？ | **说明**                                                     |
| ------------ | -------- | ------ | ------------------------------------------------------------ |
| model_path   | m        | 是     | 指定权重路径, 下载的源文件                                   |
| seq_length   | s        | 是     | 指定模型推理时支持的**最大序列长度（上下文长度）** 为 2048, 序列长度决定模型能处理的文本 Token 总数上限（输入 prompt + 生成的回复 Token 数之和不能超过 2048） |
| quantize     | q        | 是     | 指定量化类型, w4bf16/w4f16/bf16/f16等等, `w4`：模型权重（weight）采用 4bit 量化, `bf16`：模型激活值（activation）采用 bfloat16（脑浮点数）精度 |
| q_group_size | g        | 否     | 指定每组量化的组大小, 默认64                                 |
| chip         | c        | 是     | 指定平台, 如bm1684x/bm1688/cv186ah                           |
| max_pixels   | -        | 否     | 多模态参数, 指定最大尺寸, 可以是`672,896`,也可以是`602112`, 672,896 表示图像高 672 像素、宽 896 像素（也可直接写总像素数 602112，即 672×896），限制图像分辨率上限，适配模型的视觉处理能力 |
| out_dir      | o        | 是     | 指定输出目录                                                 |

#### 模型权重

首先要明确：**大模型本质是超大规模的神经网络**，这两个概念是神经网络运行的核心 —— 权重是 “静态的知识参数”，激活值是 “动态的计算中间结果”

权重是神经网络中**神经元之间连接的 “强度系数”**，是模型从海量数据中 “学” 到的核心参数：

- 训练阶段：模型通过反向传播不断调整权重，让权重适配任务（比如理解语义、识别图像特征）；
- 推理阶段：权重一旦训练完成就固定不变（静态），是模型 “记住” 的所有知识的载体。

这些权重是模型体积的主要构成 —— 比如 “3B 模型” 指权重参数总量约 30 亿个，权重文件通常占 GB 级存储空间（未量化的 Qwen2.5-VL-3B 权重约 6GB，bf16 精度下每个参数占 2 字节）

权重是**静态数据**，对量化的容忍度相对高（低比特量化后精度损失可控），因此是模型压缩的核心目标：

之前命令中的`w4`即 “权重 4bit 量化”：把原本用 16bit/32bit 存储的权重，压缩到 4bit 存储，能大幅降低显存占用、提升推理速度（比如 4bit 量化后权重体积仅为 16bit 的 1/4）。

#### 激活值

激活值是神经网络中**神经元接收到输入后，经过计算输出的动态值**：

- 推理阶段：输入不同的文本 / 图像（比如问 “今天天气如何” vs “这张图里有什么”），激活值会完全不同；
- 计算逻辑：某一层的激活值 = 上一层激活值 × 本层权重 + 偏置（Bias） → 经过激活函数（如 ReLU、SwiGLU）后的输出。

整个过程中，激活值是 “流动的信号”，每一步计算都会产生新的激活值，且仅在推理时临时生成（用完即释放，不长期存储）

激活值是**动态计算数据**，对精度极其敏感：如果用过低比特（比如 4bit）量化激活值，会导致计算误差快速累积，模型输出完全失真（比如答非所问、图像识别错误）

| 维度       | 模型权重（Weight）                      | 模型激活值（Activation）                                     |
| ---------- | --------------------------------------- | ------------------------------------------------------------ |
| 性质       | 静态（训练后固定，推理时不变）          | 动态（推理时随输入变化，临时生成）                           |
| 存储方式   | 长期存储在硬盘 / 显存（模型文件的核心） | 仅推理时临时存在显存（用完释放）                             |
| 数量规模   | 固定（如 3B 模型约 30 亿个）            | 随输入长度 / 图像尺寸变化（比如序列 2048 时，激活值规模远大于权重） |
| 量化容忍度 | 高（4bit 量化仍能保持核心精度）         | 低（需 16bit 以上精度，否则误差累积）                        |
| 核心作用   | 决定模型 “知道什么”（核心知识）         | 决定模型 “如何处理当前输入”（实时计算）                      |

#### 每组量化的组大小

组大小（也叫量化分组尺寸，对应参数`-g`）是**低比特量化模型权重时的核心粒度参数**：在对权重做 4bit/8bit 等低比特量化时，不会把整个权重矩阵当成一个整体处理，而是将连续的权重值划分成若干个 “小组”（每组包含`q_group_size`个权重值），**以 “组” 为单位计算量化所需的缩放因子（scale）、零点（zero point）**，再基于这些参数对组内的每个权重值做低比特编码

把权重量化比作 “给商品定价打包”：

- 权重值 = 不同价格的商品（比如 10 元、25 元、18 元、30 元…）；

- 低比特量化 = 用 “区间标签” 代替具体价格（比如用 1bit 表示 “≤20 元”/“＞20 元”，用 4bit 表示 16 个价格区间）；

- 组大小 = 每次打包的商品数量（比如每组 64 件）：

    

    如果不分组（整矩阵算），商品价格跨度太大（10 元～1000 元），用 4bit 标签会完全失真；

推理时，用每组的 scale 和 zero point，把 4bit 编码值还原成接近原始 32bit 的数值（保证计算精度）。

| 组大小取值       | 量化精度             | 推理速度 / 显存占用     | 适用场景                                         |
| ---------------- | -------------------- | ----------------------- | ------------------------------------------------ |
| 小（如 32）      | 更高                 | 稍慢 / 稍高             | 对精度敏感的场景（比如小模型、核心任务）         |
| 中（64，默认）   | 平衡（精度损失可控） | 最优（速度 / 显存平衡） | 绝大多数通用场景（如你用的 Qwen2.5-VL-3B）       |
| 大（如 128/256） | 稍低                 | 更快 / 更低             | 对速度要求高、精度要求低的场景（比如边缘端推理） |

## Yolo模型

如果模型是图片输入，在转模型之前我们需要了解模型的预处理。如果模型用预处理后的npz文件做输入，则不需要考虑预处理。 预处理过程用公式表达如下（x代表输入) `y = （x - mean） \times scale`

官网yolov5的图片是rgb，每个值会乘以`1/255`，转换成mean和scale对应为`0.0,0.0,0.0`和`0.0039216,0.0039216,0.0039216`

> 因为 YOLOv5 没有做 “减均值” 的中心化操作（比如 ImageNet 预训练模型会减 (0.485,0.456,0.406)，但 YOLOv5 追求简洁，无需这一步）。三个通道均为`1/255 ≈ 0.0039215686`，四舍五入后就是`0.0039216`

#### 模型转F16

这个命令是**YOLOv5s 模型部署流程的 “前置转换步骤”** —— 核心是把「ONNX 格式的 YOLOv5s 模型」转换成「跨框架 / 跨硬件的 MLIR 通用中间表示格式」，同时完成模型输入的标准化预处理、指定输出节点、生成浮点模型的基准输出（用于后续量化精度验证），为后续的校准、量化生成 bmodel 打下基础

> ONNX 是 “框架级” 中间格式（比如 PyTorch→ONNX），但不同硬件厂商（如比特大陆、英伟达）的部署工具链难以直接适配；而 MLIR 是 “硬件无关” 的底层中间表示，能统一处理不同框架的模型，还能方便地插入量化、优化等操作，是连接 “框架模型” 和 “硬件专用模型（bmodel）” 的桥梁

```bash
model_transform.py \
    --model_name yolov5s \
    --model_def ../yolov5s.onnx \
    --input_shapes [[1,3,640,640]] \
    --mean 0.0,0.0,0.0 \
    --scale 0.0039216,0.0039216,0.0039216 \
    --keep_aspect_ratio \
    --pixel_format rgb \
    --output_names 350,498,646 \
    --test_input ../image/dog.jpg \
    --test_result yolov5s_top_outputs.npz \
    --mlir yolov5s.mlir
```

| **参数名**        | 必选？ | **说明**                                                     |
| ----------------- | ------ | ------------------------------------------------------------ |
| model_name        | 是     | 指定模型名称, 用于标记, 之后方便使用                         |
| model_def         | 是     | 指定模型定义文件，比如`.onnx`或`.pt`或`.tflite`或`.prototxt`文件, ONNX 是跨框架的标准化模型格式，这里是 YOLOv5s 导出的 ONNX 模型文件，包含模型的网络结构和权重 |
| model_data        | 否     | 指定模型权重文件，caffe模型需要，对应`.caffemodel`文件       |
| input_shapes      | 否     | 指定输入的shape，例如`[[1,3,640,640]]`；二维数组，可以支持多输入情况, 指定模型输入张量的形状（shape），二维数组格式（支持多输入，此处为单输入）： |
| resize_dims       | 否     | `1`= 批大小（batch_size），表示一次处理 1 张图片；原始图片需要resize之后的尺寸；如果不指定，则resize成模型的输入尺寸 |
| keep_aspect_ratio | 否     | `3`= 通道数（RGB）；在Resize时是否保持长宽比，默认为false；设置时会对不足部分补0, 否则进行直接拉伸 |
| mean              | 否     | `640,640`= 输入图片的高 / 宽（YOLOv5 默认输入尺寸）；图像每个通道的均值，默认为0.0,0.0,0.0, 指定图像三个通道（RGB）的预处理均值，与 YOLOv5 的预处理逻辑一致 |
| scale             | 否     | 整体表示模型接收`1×3×640×640`的张量输入图片每个通道的比值，默认为1.0,1.0,1.0 |
| pixel_format      | 否     | 图片类型，可以是rgb、bgr、gray、rgbd四种情况                 |
| output_names      | 否     | 指定输出的名称，如果不指定，则用模型的输出；指定后用该指定名称做输出 |
| test_input        | 否     | 指定输入文件用于验证，可以是图片或npy或npz；可以不指定，则不会正确性验证 |
| test_result       | 否     | 指定验证后的输出文件                                         |
| excepts           | 否     | 指定需要排除验证的网络层的名称，多个用,隔开                  |
| debug             | 否     | 指定后保留中间临时文件；否则会清理掉中间临时文件             |
| mlir              | 是     | 指定输出的mlir文件路径                                       |

#### 输出节点选择

YOLOv8/YOLO11 的推理流程是「特征提取→多尺度融合→输出解码」，文档中选择的节点都是模型「最终可解码的输出层」，不同节点对应不同的输出阶段

**Concat 类节点**（方案一核心）：是「多尺度特征融合后的原始输出」。YOLO 模型为了检测不同大小的目标（大 / 中 / 小），会在多个尺度上提取特征，Concat 节点就是将这些不同尺度的特征图拼接后的结果（如 Concat_1/2/3 对应 3 个尺度）。CPU 可以直接对这些原始特征图进行解码（计算边界框、类别置信度、关键点坐标等）

**dfl/conv/Conv_output_0**：是「分布焦点损失（DFL）的卷积输出」。DFL 是 YOLO 用于边界框回归的核心模块，该节点输出的是边界框坐标的预测特征，NPU 对卷积运算有硬件优化，能快速处理

**Sigmoid_output_0**：是「类别 / 置信度的激活输出」。Sigmoid 函数将模型预测的原始分数映射到 0-1 之间（表示置信度），NPU 可直接处理激活运算，减少 CPU 负担

**额外节点（如 output1、Concat_output_0）**：对应扩展任务的输出。例如：

- 分割（seg）模型的「output1」：是分割掩码的预测输出（分割任务需要同时输出边界框和掩码，因此多一个节点）；
- 关键点（pose）模型的「Concat_output_0」：是关键点坐标的特征输出（除了边界框，还需预测关键点，因此增加该节点）

实际是CPU以及NPU的工作分配问题

- 方案一（CPU 多干活）

    - 输出节点是「原始 Concat 特征」，后续的解码（边界框回归、类别判断、关键点计算）全部交给 CPU 处理；
    - 优势：CPU 处理的解码逻辑更灵活，量化时（将模型权重从浮点转整型，适配边缘设备）不容易出现精度损失（量化失败风险低）；
    - 劣势：CPU 运算速度比 NPU 慢，整体推理速度略逊于方案二。

- 方案二（NPU 多干活）

    - 输出节点是「DFL 卷积 + Sigmoid 激活后的结果」，这些是模型推理中计算量最大的部分（卷积、激活），直接交给 NPU 处理（NPU 擅长并行计算，速度远快于 CPU）；
    - 优势：NPU 承担核心计算，整体推理速度更快（MaixCAM 推荐方案）；
    - 劣势：NPU 参与量化时，部分硬件（如 MaixCAM2）对 DFL+Sigmoid 的量化兼容性不好，容易出现量化失败（模型无法运行或精度暴跌）。

    > **绝对不可以随便选输出节点**—— 随便选的结果大概率是「模型无法运行」「输出结果完全错误（如无检测框、关键点 / 分割掩码丢失）」「量化失败」，甚至直接触发硬件推理报错。
    >
    > 核心原因在于：输出节点的选择不是 “任选”，而是**和模型结构、硬件能力、MaixPy 的后处理逻辑强绑定**，每一个节点都对应 YOLO 推理的核心维度，少选、错选、乱选都会破坏完整的推理链路

##### 选择测试问题

测试直接使用output0作为输出, 实际结果失败

MaixPy 运行的硬件（MaixCAM/MaixCAM2）依赖 “模型量化”（将浮点权重转整型）才能高效运行，而原生 output0 有两个致命问题：

- 量化精度损失：output0 是 “浮点型端到端输出”，直接量化会导致框坐标、置信度的精度暴跌（比如目标置信度从 0.9 量化后变成 0.1，漏检 / 错检严重）；
- 量化兼容性：文档中的方案一 / 二节点（Concat/dfl/Sigmoid）是 “低维度、易量化的中间特征”（比如 Concat 节点输出的是特征图，而非最终锚框），量化时数值波动小、稳定性高；而 output0 是高维度张量，量化时极易触发硬件兼容问题（比如 MaixCAM2 直接量化失败）。

#### MLIR转Int8

这段操作的核心是**将 YOLOv5s 浮点模型量化为适配 BM1684x 芯片的 INT8 低精度模型**，目的是在保证精度损失可控的前提下，提升模型在专用 AI 硬件上的推理速度、降低存储开销和功耗。

> 深度学习模型默认是 FP32（32 位浮点）格式，精度高但计算 / 存储成本大；而边缘端 / 专用 AI 芯片（如比特大陆 BM1684x）对低精度（INT8）计算的支持更高效 ——INT8 的计算量仅为 FP32 的 1/4，存储占用仅为 1/4，推理速度能提升 3~5 倍（甚至更高）

转INT8模型前需要跑calibration，得到量化表；输入数据的数量根据情况准备100~1000张左右。

> 用一批真实数据（COCO2017）跑模型，统计模型各层张量的数值分布（比如最大值 / 最小值、数据分布规律），计算出**浮点值→INT8 值的最优映射参数**（即 “量化表”）

然后用量化表，生成对称或非对称bmodel。如果对称符合需求，一般不建议用非对称，因为非对称的性能会略差与对称模型

```
run_calibration.py yolov5s.mlir \
  --dataset ../COCO2017 \
  --input_num 100 \
  -o yolov5s_cali_table
```

> `yolov5s.mlir`：输入的模型文件（MLIR 是跨框架 / 跨硬件的通用模型中间表示，解决不同框架（PyTorch/TensorFlow）和硬件的适配问题）
>
> `--dataset ../COCO2017`：校准用的数据集（选 COCO2017 是因为 YOLOv5 的训练 / 测试基于该数据集，数据分布匹配，校准结果更可靠）；
>
> `--input_num 100`：用 100 张图做校准（100~1000 是经验值，数量太少统计不准，太多耗时）；
>
> `-o yolov5s_cali_table`：输出量化表（核心文件，包含各层的缩放因子（scale）、量化范围等关键参数）

转成INT8对称量化模型，执行如下命令：

```
model_deploy.py \
  --mlir yolov5s.mlir \
  --quantize INT8 \
  --calibration_table yolov5s_cali_table \
  --processor bm1684x \
  --test_input yolov5s_in_f32.npz \
  --test_reference yolov5s_top_outputs.npz \
  --tolerance 0.85,0.45 \
  --model yolov5s_1684x_int8.bmodel
```

基于第一步的量化表，将 MLIR 格式的浮点模型转换成**BM1684x 芯片专用的 INT8 量化模型（bmodel 格式）**

| 参数                                       | 作用                                                         |
| ------------------------------------------ | ------------------------------------------------------------ |
| `--mlir yolov5s.mlir`                      | 原始的 FP32 浮点模型（MLIR 格式）                            |
| `--quantize INT8`                          | 指定量化类型为 INT8（对比 FP32/FP16，优先选 INT8 平衡性能和精度） |
| `--calibration_table yolov5s_cali_table`   | 加载第一步生成的量化表（确定浮点→INT8 的映射关系）           |
| `--processor bm1684x`                      | 指定目标硬件为比特大陆 BM1684x 芯片（bmodel 会适配该芯片的指令集） |
| `--test_input yolov5s_in_f32.npz`          | 测试用输入数据（FP32 格式，npz 是 numpy 压缩格式，模拟真实推理输入） |
| `--test_reference yolov5s_top_outputs.npz` | 浮点模型的输出结果（作为 “标准答案”，用于对比量化后模型的精度） |
| `--tolerance 0.85,0.45`                    | 精度容忍度（量化后模型与浮点模型的精度差异阈值，比如第一个值可能是 mAP 容忍度，第二个是分类精度容忍度；若差异超过该值，量化失败） |
| `--model yolov5s_1684x_int8.bmodel`        | 输出最终的 INT8 量化模型（bmodel 是 BM 系列芯片的专用格式，可直接在 BM1684x 上推理） |

- **对称量化**：将浮点的正负范围对称映射到 INT8 的 [-127,127]（舍去 - 128），计算逻辑简单（仅需缩放因子 scale），硬件执行效率最高；
- **非对称量化**：映射到 [0,255]（无符号）或 [-128,127]（有符号），需额外计算偏移量（zero point），精度可能略高但计算步骤多，性能略差；
- 因此优先选对称量化，仅当对称量化精度不达标时才用非对称