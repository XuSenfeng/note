# Yolo模型转换

https://www.zhihu.com/tardis/zm/art/26328918714?source_id=1005

https://wiki.sipeed.com/maixpy/doc/zh/ai_model_converter/maixcam.html

MaixCam使用的模型是MUD描述模型, 模型统一描述文件, model universal description file, 本身是一个 `ini`格式的文本文件，可以使用文本编辑器编辑

一般 MUD 文件会伴随一个或者多个实际的模型文件，比如对于 MaixCAM， 实际的模型文件是`.cvimodel`格式， MUD 文件则是对它做了一些描述说明

```ini
[basic]
type = cvimodel
model = yolov8n.cvimodel

[extra]
model_type = yolov8
input_type = rgb
mean = 0, 0, 0
scale = 0.00392156862745098, 0.00392156862745098, 0.00392156862745098
labels = person, bicycle, car, 
```

## 转换

准备好你的 onnx 模型， 然后在https://netron.app/ 查看你的模型，确保你的模型使用的算子在转换工具的支持列表中，转换工具的支持列表可以在[算能 TPU SDK](https://developer.sophgo.com/thread/473.html)的 **CVITEK_TPU_SDK开发指南.pdf** 中看到列表

```python
from ultralytics import YOLO
import sys

print(sys.path)
net_name = '../ultralytics/runs/detect/train4/weights/best.pt'
input_width = 320
input_height = 224

# Load a model
model = YOLO(net_name)  # load an official model
path = model.export(format="onnx", imgsz=[input_height, input_width], dynamic=False, simplify=True, opset=17)   # export the model to ONNX format
print(path)
```

### 安装docker

```bash
docker pull sophgo/tpuc_dev:latest
# 挂载一下mac的一个目录
docker run --privileged --name tpu-env -v /home/$USER/data:/home/${USER}/data -it sophgo/tpuc_dev
# mac
docker run --privileged --name tpu-env -v /Users/$USER/data:/home/${USER}/data -it sophgo/tpuc_dev

wget https://github.com/sophgo/tpu-mlir/releases#:~:text=tpu_mlir%2D1.25%2Dpy3%2Dnone%2Dany.whl

pip install tpu_mlir*.whl
```

文件路径

```bash
.
├── 1.jpg
├── best.onnx
├── best.pt
├── calling20221009.mp4
├── conver.sh
├── tpu_mlir-1.25-py3-none-any.whl
├── val
│   ├── frame_510.jpg
│   ├── frame_540.jpg
│   └── frame_780.jpg
└── workspace
    ├── _weight_map.csv
    ├── best.mlir
    ├── best.ref_files.json
    ├── best_bf16
    │   ├── final.mlir
    │   └── ref_files.json
    ├── best_bf16.cvimodel
    ├── best_bf16_tensor_info.txt
    ├── best_cali_table
    ├── best_cv181x_bf16.layer_group_cache.json
    ├── best_cv181x_bf16.layer_group_config.json
    ├── best_cv181x_bf16_final.mlir
    ├── best_cv181x_bf16_tpu.mlir
    ├── best_cv181x_int8.layer_group_cache.json
    ├── best_cv181x_int8.layer_group_config.json
    ├── best_cv181x_int8_sym_final.mlir
    ├── best_cv181x_int8_sym_tpu.mlir
    ├── best_in_f32.npz
    ├── best_int8
    │   ├── final.mlir
    │   └── ref_files.json
    ├── best_int8.cvimodel
    ├── best_int8_tensor_info.txt
    ├── best_opt.onnx.prototxt
    ├── best_origin.mlir
    ├── best_top_f32_all_weight.npz
    ├── best_top_outputs.npz
    ├── best_tpu_addressed_cv181x_bf16_weight.npz
    ├── best_tpu_addressed_cv181x_bf16_weight_fix.npz
    ├── best_tpu_addressed_cv181x_int8_sym_weight.npz
    ├── best_tpu_addressed_cv181x_int8_sym_weight_fix.npz
    └── shape_pattern_qtable
```

使用脚本

这里选取的节点是下面的两个

![image-20251214153346489](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/mac-picture/image-20251214153346489.png)

>   “截断模型输出到 CPU 后处理前的核心层”，让量化只作用于真正需要加速的 GPU / 推理芯片运算部分
>
>   

```bash
#!/bin/bash

set -e

net_name=best
input_w=320
input_h=224

# mean: 0, 0, 0
# std: 255, 255, 255

# mean
# 1/std

# mean: 0, 0, 0
# scale: 0.00392156862745098, 0.00392156862745098, 0.00392156862745098

mkdir -p workspace
cd workspace

# convert to mlir
model_transform.py \
--model_name ${net_name} \
--model_def ../${net_name}.onnx \
--input_shapes [[1,3,${input_h},${input_w}]] \
--mean "0,0,0" \
--scale "0.00392156862745098,0.00392156862745098,0.00392156862745098" \
--keep_aspect_ratio \
--pixel_format rgb \
--channel_format nchw \
--output_names "/model.22/dfl/conv/Conv_output_0,/model.22/Sigmoid_output_0" \
--test_input ../1.jpg \
--test_result ${net_name}_top_outputs.npz \
--tolerance 0.99,0.99 \
--mlir ${net_name}.mlir

# export bf16 model
#   not use --quant_input, use float32 for easy coding
model_deploy.py \
--mlir ${net_name}.mlir \
--quantize BF16 \
--processor cv181x \
--test_input ${net_name}_in_f32.npz \
--test_reference ${net_name}_top_outputs.npz \
--model ${net_name}_bf16.cvimodel

echo "calibrate for int8 model"
# export int8 model
run_calibration.py ${net_name}.mlir \
--dataset ../val \
--input_num 3 \
-o ${net_name}_cali_table

echo "convert to int8 model"
# export int8 model
#    add --quant_input, use int8 for faster processing in maix.nn.NN.forward_image
model_deploy.py \
--mlir ${net_name}.mlir \
--quantize INT8 \
--quant_input \
--calibration_table ${net_name}_cali_table \
--processor cv181x \
--test_input ${net_name}_in_f32.npz \
--test_reference ${net_name}_top_outputs.npz \
--tolerance 0.9,0.6 \
--model ${net_name}_int8.cvimodel
```

-   `output_names` 就是我们前面说到的输出节点的输出名。
-   `mean, scale` 就是训练时使用的预处理方法，比如 `YOLOv5` 官方代码的预处理是把图像 RGB 3个通道分别 `-mean`再除以`std`，并且默认`mean`

为`0`， `std`为`255`，即将图像的值归一，这里`scale`就是`1/std`。你的模型需要根据实际的预处理方法修改。

-   `test_input` 就是转换时用来测试的图像，这里是`../dog.jpg`，所以实际模型转换时我们需要在此脚本所在同目录放一张`dog.jpg`的图，你的模型根据你的实际情况替换图像。
-   `tolerance` 就是量化前后允许的误差，如果转换模型时报错提示值小于设置的这个值，说明转出来的模型可能相比 onnx 模型误差较大，如果你能够容忍，可以适当调小这个阈值让模型转换通过，不过大多数时候都是因为模型结构导致的，需要优化模型，以及仔细看后处理，把能去除的后处理去除了。
-   `quantize` 即量化的数据类型，在 MaixCAM 上我们一般用 INT8 模型，这里我们虽然也顺便转换了一个 BF16 模型，BF16 模型的好处时精度高，不过运行速率比较慢，能转成 INT8 就推荐先用 INT8,实在不能转换的或者精度要求高速度要求不高的再考虑 BF16。
-   `dataset` 表示用来量化的数据集，也是放在转换脚本同目录下，比如这里是`images`文件夹，里面放数据即可，对于 YOLOv5 来说就是图片，从 coco 数据集中复制一部分典型场景的图片过来即可。 用`--input_num` 可以指定实际使用图片的数量（小于等于 images 目录下实际的图片）

>   能在`workspace`文件夹下看到有`**_int8.cvimodel` 文件了

## 配置文件

```ini
[basic]
type = cvimodel
model = best_bf16.cvimodel

[extra]
model_type = yolov8
input_type = rgb
mean = 0, 0, 0
scale = 0.00392156862745098, 0.00392156862745098, 0.00392156862745098
labels = man, girl, car
```

这里`basic`部分指定了模型文件类别和模型文件路径，是必要的参数，有了这个参数就能用`MaixPy`或者`MaixCDK`中的`maix.nn.NN`类来加载并运行模型了。

`extra`则根据不同模型的需求设计不同参数。
比如这里对`YOLOv5`设计了这些参数，主要是 预处理、后处理、标签等参数。
对于 `MaixPy` 已经支持了的模型可以直接下载其模型复制修改。
也可以看具体的代码，比如[YOLOv5 的源码](https://github.com/sipeed/MaixCDK/blob/71d5b3980788e6b35514434bd84cd6eeee80d085/components/nn/include/maix_nn_yolov5.hpp#L73-L223)，可以看到源码使用了哪些参数。

比如你用`YOLOv5`训练了检测数字`0~9`的模型，那么需要将`labels`改成`0,1,2,3,4,5,6,7,8,9`，其它参数如果你没改训练代码保持即可。

如果你需要移植 `MaixPy` 没有支持的模型，则可以根据模型的预处理和后处理情况定义 `extra`, 然后编写对应的解码类。如果你不想用C++修改 MaixPy 源码，你也可以用MaixPy 的`maix.nn.NN`类加载模型，然后用 `forward` 或者 `forward_image` 方法或者原始输出，在 Python 层面写后处理也可以，只是运行效率比较低不太推荐

## Maix

```python
from maix import camera, display, image, nn, app

# detector = nn.YOLO11(model="/root/models/maixhub/my/model.mud", dual_buff=True)

# cam = camera.Camera(detector.input_width(), detector.input_height(), detector.input_format())
detector = nn.YOLOv8(model="/root/models/maixhub/my/my_model.mud", dual_buff=True)

cam = camera.Camera(detector.input_width(), detector.input_height(), detector.input_format())
disp = display.Display()

while not app.need_exit():
    img = cam.read()
    objs = detector.detect(img, conf_th = 0.5, iou_th = 0.45)
    for obj in objs:
        img.draw_rect(obj.x, obj.y, obj.w, obj.h, color = image.COLOR_RED)
        msg = f'{detector.labels[obj.class_id]}: {obj.score:.2f}'
        img.draw_string(obj.x, obj.y, msg, color = image.COLOR_RED)
    disp.show(img)
```

