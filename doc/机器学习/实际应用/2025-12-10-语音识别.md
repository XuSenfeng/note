# 语音识别

FunAudioLLM是阿里巴巴通义实验室推出的开源语音大模型项目，包含**SenseVoice**和**CosyVoice**两个模型。SenseVoice擅长多语言语音识别和情感辨识，支持超过50种语言，特别在中文和粤语上表现优异。CosyVoice则专注于自然语音生成，能够控制音色和情感，支持中英日粤韩五种语言。FunAudioLLM适用于多语言翻译、情绪语音对话等场景。相关模型和代码已在Modelscope和Huggingface平台开源

-   [SenseVoice模型](https://github.com/FunAudioLLM/SenseVoice)
    -   专注于多语言的高精度语音识别。
    -   支持超过50种语言，特别是在中文和粤语上识别效果优于现有模型。
    -   具备情感识别功能，能够辨识多种人机交互事件。
    -   提供轻量级和大型两个版本，适应不同应用场景。
-   [CosyVoice模型](https://github.com/FunAudioLLM/CosyVoice)
    -   专注于自然语音生成，支持多语言、音色和情感控制。
    -   能够根据少量原始音频快速生成模拟音色，包括韵律和情感细节。
    -   支持跨语种语音生成和细粒度的情感控制。

## SenseVoice

SenseVoice 是具有音频理解能力的音频基础模型，包括语音识别(ASR)、语种识别(LID)、语音情感识别(SER)和声学事件分类(AEC)或声学事件检测(AED)

### 使用示例

https://github.com/modelscope/FunASR/blob/main/docs/tutorial/README_zh.md

```python
from funasr import AutoModel
from funasr.utils.postprocess_utils import rich_transcription_postprocess

model_dir = "iic/SenseVoiceSmall"


model = AutoModel(
    model=model_dir,
    trust_remote_code=True,# 表示 model 代码实现从 remote_code 处加载
    remote_code="./model.py",  
    vad_model="fsmn-vad",
    vad_kwargs={"max_single_segment_time": 30000},
    device="cuda:0",
)

# en
res = model.generate(
    input=f"{model.model_path}/example/en.mp3",
    cache={},
    language="auto",  # "zh", "en", "yue", "ja", "ko", "nospeech"
    use_itn=True,
    batch_size_s=60,
    merge_vad=True,
    merge_length_s=15,
)
text = rich_transcription_postprocess(res[0]["text"])
print(text)
```

+   trust_remote_code
+   +   `True` 表示 model 代码实现从 `remote_code` 处加载，`remote_code` 指定 `model` 具体代码的位置（例如，当前目录下的 `model.py`），支持绝对路径与相对路径，以及网络 url。
    +   `False` 表示，model 代码实现为 [FunASR](https://github.com/modelscope/FunASR) 内部集成版本，此时修改当前目录下的 `model.py` 不会生效，因为加载的是 funasr 内部版本，模型代码 [点击查看](https://github.com/modelscope/FunASR/tree/main/funasr/models/sense_voice)。

-   `vad_model`：表示开启 VAD，VAD 的作用是将长音频切割成短音频，此时推理耗时包括了 VAD 与 SenseVoice 总耗时，为链路耗时，如果需要单独测试 SenseVoice 模型耗时，可以关闭 VAD 模型。
-   `vad_kwargs`：表示 VAD 模型配置，`max_single_segment_time`: 表示 `vad_model` 最大切割音频时长，单位是毫秒 ms。
-   `use_itn`：输出结果中是否包含标点与逆文本正则化。
-   `batch_size_s` 表示采用动态 batch，batch 中总音频时长，单位为秒 s。
-   `merge_vad`：是否将 vad 模型切割的短音频碎片合成，合并后长度为 `merge_length_s`，单位为秒 s。
-   `ban_emo_unk`：禁用 emo_unk 标签，禁用后所有的句子都会被赋与情感标签。默认 `False`

如果输入均为短音频（小于 30s），并且需要批量化推理，为了加快推理效率，可以移除 vad 模型，并设置 `batch_size`

```python
res = model.generate(
    input=f"{model.model_path}/example/en.mp3",
    cache={},
    language="auto", # "zh", "en", "yue", "ja", "ko", "nospeech"
    use_itn=True,
    batch_size=64, 
)
```

