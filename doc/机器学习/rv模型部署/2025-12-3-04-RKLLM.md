# RKLLM

> 相关文档rknn-llm/doc/Rockchip_RKLLM_SDK_CN_1.2.0.pdf

通过该工具提供的Python接口可以便捷地完成以下功能： 

1）模型转换：支持将Hugging Face 和 GGUF 格式的大语言模型（Large Language Model, LLM）转换为 RKLLM 模型

+ 目前支持的模型包括

LLaMA, Qwen, Qwen2, Phi-2, Phi-3, ChatGLM3, Gemma, Gemma2, Gemma3, InternLM2, TeleChat2, MiniCPM-S, MiniCPM 和 MiniCPM3

转换后的RKLLM模型能够在RockchipNPU平台上加载使用。 

2）量化功能：支持将浮点模型量化为定点模型，目前支持的量化类型包括 

a. w4a16； b. w4a16分组量化(支持的分组数为32,64,128)； c. w8a8； d. w8a8分组量化(支持的分组数为128,256,512)；

## 基础步骤

### 模型转换

用户提供的HuggingFace格式的大语言模型将会被转换为RKLLM格式， 以便在RockchipNPU平台上进行高效的推理

+ 获取原始模型：1、开源的HuggingFace格式的大语言模型；2、自行训练得到的大语 言模型，要求模型保存的结构与HuggingFace平台上的模型结构一致；3、GGUF模型，目 前仅支持q4_0和fp16类型模型
+  模型加载：通过rkllm.load_huggingface()函数加载 huggingface 格式模型，通过 rkllm.load_gguf()函数加载 GGUF 模型
+ 模型量化配置：通过 rkllm.build() 函数构建RKLLM模型，在构建过程中可选择是否 进行模型量化来提高模型部署在硬件上的性能，以及选择不同的优化等级和量化类型
+ 模型导出：通过 rkllm.export_rkllm() 函数将 RKLLM模型导出为一个.rkllm格式文件， 用于后续的部署

### 板子部署

+ 模型初始化：加载RKLLM模型到RockchipNPU平台，进行相应的模型参数设置来 定义所需的文本生成方式，并提前定义用于接受实时推理结果的回调函数，进行推理前准备
+ 模型推理：执行推理操作，将输入数据传递给模型并运行模型推理，用户可以通过预 先定义的回调函数不断获取推理结果
+ 模型释放：在完成推理流程后，释放模型资源，以便其他任务继续使用NPU的计算资源

1） 定义回调函数callback()； 

2） 定义RKLLM模型参数结构体RKLLMParam； 

3） rkllm_init()初始化 RKLLM 模型； 

4） rkllm_run()进行模型推理； 

5） 通过回调函数callback()对模型实时传回的推理结果进行处理； 

6） rkllm_destroy()销毁 RKLLM 模型并释放资源；

## 模型量化

**模型量化**是一种模型压缩技术，其核心思想是**用更低精度的数值格式（例如8位整数）来表示和计算一个深度学习模型中原本使用高精度数值格式（例如32位浮点数）的权重和激活值**。

您可以把它想象成：

- **原始模型（FP32）**：像用高保真无损音频文件（如FLAC/WAV）存储音乐，精度高、保真度好，但文件体积巨大。
- **量化后模型（INT8）**：像把音乐转换成MP3格式。虽然损失了一点点音质细节，但文件体积大幅减小，播放更流畅，对设备要求也更低。在绝大多数情况下，这点音质损失是听不出来的。

------

### **为什么要进行模型量化？（动机与好处）**

量化主要为了解决深度学习模型部署时的三大瓶颈：

1. **减少模型体积**
    - 将权重从32位浮点（FP32）转换为8位整数（INT8），模型大小理论上直接减少为原来的 **1/4**。这对于将模型部署到存储空间有限的移动设备、嵌入式系统（如手机、智能摄像头、自动驾驶汽车）至关重要。
2. **提升推理速度**
    - 整数运算比浮点运算快得多，尤其是在没有专用浮点计算单元（如一些边缘计算芯片）的硬件上。
    - 更低精度的数据意味着在相同内存带宽下可以传输更多数据，减少了数据搬运的瓶颈。
    - 许多硬件（如CPU、GPU、NPU）都针对低精度整数运算进行了特殊优化，能大幅提升计算吞吐量。
3. **降低功耗**
    - 更简单的整数运算单元比复杂的浮点运算单元消耗的能量更少。这对于依赖电池的移动设备和物联网设备来说，能显著延长续航时间。

------

### **主要的量化方法**

根据量化发生的时机和是否需要原始训练数据，可以分为以下几类：

1. **训练后量化**
    - **做法**：在一个**已经训练好的FP32模型**上直接进行量化。
    - **优点**：简单快捷，不需要重新训练或原始数据（部分方法需要少量校准数据）。
    - **缺点**：精度损失可能相对较大，尤其是对敏感的模型。
    - **最常见、应用最广**的类型。
2. **量化感知训练**
    - **做法**：在**模型训练过程中**就模拟量化的效果，让模型在训练时“提前适应”低精度的表示。
    - **优点**：能最大程度地保持量化后的模型精度，通常能达到接近原始FP32模型的准确率。
    - **缺点**：过程复杂，需要重新训练或微调模型，计算成本高。

------

### **量化的基本过程（以训练后INT8量化为例）**

量化的关键步骤是找到一个**缩放因子**和一个**零点**，将浮点数范围线性映射到整数范围。

**公式简化表示：**
`量化值 = round(浮点值 / 缩放因子) + 零点`

**步骤：**

1. **统计范围**：分析模型权重或激活值的分布范围（最大值、最小值）。
2. **计算参数**：根据统计的范围，确定最佳的`缩放因子`和`零点`，使得浮点数范围能尽可能无损地映射到有限的整数域（如-128到127）。
3. **转换与存储**：使用上述公式将所有浮点权重转换为整数，并存储在模型中。
4. **推理计算**：在设备上进行推理时，使用高效的整数矩阵乘加运算。输入数据也需要被量化为整数，输出结果再通过反量化转换回浮点数以供后续层使用或最终输出。

------

### **挑战与注意事项**

- **精度损失**：最核心的挑战。过度压缩可能导致模型准确度显著下降。
- **模型敏感性**：不同模型、不同层对量化的容忍度不同。例如，轻量级模型可能比大型模型更敏感。
- **硬件支持**：需要目标部署硬件（如手机芯片、AI加速卡）支持相应的低精度指令集，才能发挥量化优势。

### **总结**

模型量化是**将深度学习模型从“实验室环境”推向“实际生产部署”的关键桥梁**。它通过牺牲微不足道的精度，换来了**模型体积、推理速度和功耗**的显著优化，使得在资源受限的边缘设备上运行强大的AI模型成为可能。如今，它已成为AI模型部署中一项标准且必不可少的技术。

## 模型量化

```python
from rkllm.api import RKLLM
import os
#os.environ['CUDA_VISIBLE_DEVICES']='0'

modelpath = './huggingface/deepseek-r1-1.5b'
llm = RKLLM()

# Load model
# Use 'export CUDA_VISIBLE_DEVICES=0' to specify GPU device
# options ['cpu', 'cuda']
ret = llm.load_huggingface(model=modelpath, model_lora = None, device='cpu')
if ret != 0:
    print('Load model failed!')
    exit(ret)

# Build model
dataset = "./data_quant.json"
qparams = None
# do_quantization: 量化精度优化, quantized_algorithm使用的量化优化算法
# 权重(weights)和激活(activations)都量化为8位整数 → 最高压缩率，速度最快
# optimization_level=1: 优化等级, 
# 0: 基本优化，只做必要转换, 1: 中等优化（常用），包括图优化、算子融合等
# 2: 高级优化，更激进的优化策略 3: 极致优化（可能影响精度）
# 量化校准算法
# 'normal': 标准线性量化（最常用）'minmax': 基于最小-最大值的量化
# 'kl_divergence': 基于KL散度的量化（精度更高）'percentile': 基于百分位的量化（抗异常值）
ret = llm.build(do_quantization=True, optimization_level=1, quantized_dtype='w8a8',
                quantized_algorithm='normal', target_platform='rk3588', num_npu_core=3, extra_qparams=qparams,dataset=dataset)
if ret != 0:
    print('Build model failed!')
    exit(ret)


# Export rkllm model
ret = llm.export_rkllm(f"./deepseek-1.5b-w8a8-rk3588.rkllm")
if ret != 0:
    print('Export model failed!')
    exit(ret)
```

### 量化算法

1. 量化算法的核心作用

量化算法的核心任务是**解决"如何将连续的浮点数映射到离散的整数"**这一数学问题，主要解决：

- **动态范围匹配**：将权重/激活值的浮点分布（如[-3.2, 4.8]）映射到有限的整数范围（如[-128, 127]）
- **精度损失最小化**：找到最优的映射函数，使量化后的信息损失最小
- **异常值处理**：处理极端大/小的值，防止它们影响整体量化效果

2. 不同量化算法的详细区别

a) `'normal'` - 标准线性量化（最常用）

```
# 基本原理：基于均值和标准差的线性映射
scale = (max_value - min_value) / (quant_max - quant_min)
zero_point = -min_value / scale

# 适用场景：
# - 数据分布相对均匀
# - 激活值无明显异常值
# - 大多数CNN、Transformer基础层
# 优点：计算简单，速度快
# 缺点：对异常值敏感
```

#### b) `'minmax'` - 最小-最大值量化

```
# 基本原理：直接使用原始数据的绝对最大/最小值
min_val = min(原始数据)
max_val = max(原始数据)
# 然后线性映射

# 适用场景：
# - 数据范围明确且有限
# - 需要完全保留原始范围
# - 图像处理（像素值范围固定）
# 优点：覆盖全部数据范围
# 缺点：一个异常值就能毁掉整个量化（如max=1000，正常值都在[-1,1]）
```

c) `'kl_divergence'` - KL散度量化(精度最高)

```
# 基本原理：最小化原始分布与量化分布的KL散度
# 1. 将原始数据分成多个bins（直方图）
# 2. 搜索最优的截断阈值，使量化后的分布与原始分布差异最小
# 3. 使用截断后的范围进行线性量化

# 适用场景：
# - 对精度要求极高的应用
# - 数据分布不均匀或有长尾分布
# - LLM中的注意力机制、GeLU激活层
# 优点：精度损失最小，能智能处理长尾分布
# 缺点：计算复杂，需要更多校准数据和时间
```

d) `'percentile'` - 百分位量化

```
# 基本原理：使用百分位数（如99.9%）而非绝对极值
min_val = np.percentile(数据, 0.1)  # 忽略最小的0.1%
max_val = np.percentile(数据, 99.9) # 忽略最大的0.1%

# 适用场景：
# - 数据包含少量异常值
# - 需要鲁棒性强的量化
# - 实时推理系统
# 优点：抗异常值干扰，稳定性好
# 缺点：可能丢失极端但重要的信息
```

3. 算法选择建议

| 算法            | 精度     | 速度 | 内存 | 推荐场景               |
| :-------------- | :------- | :--- | :--- | :--------------------- |
| `normal`        | 中等     | 最快 | 低   | 一般应用，快速部署     |
| `minmax`        | 低-中等  | 快   | 低   | 数据范围明确的简单模型 |
| `kl_divergence` | **最高** | 慢   | 高   | 对话模型、高精度要求   |
| `percentile`    | 高       | 中等 | 中等 | 工业部署，鲁棒性要求高 |

实际经验：

- 对于LLM：通常使用`kl_divergence` + `percentile`组合
- 对于CNN视觉模型：`normal`或`percentile`足够
- 移动端部署：`percentile`（平衡精度和稳定性）

## 模型部署

模型的运行需要使用两个库, 分别是`libgomp`以及`librkllm_api`

### C语言调用

```c
#include <string.h>
#include <unistd.h>
#include <string>
#include "rkllm.h"
#include <fstream>
#include <iostream>
#include <csignal>
#include <vector>

using namespace std;
LLMHandle llmHandle = nullptr;

#define PROMPT_TEXT_PREFIX "<｜begin▁of▁sentence｜><｜User｜>"
#define PROMPT_TEXT_POSTFIX "<｜Assistant｜>"

void exit_handler(int signal)
{
    if (llmHandle != nullptr)
    {
        {
            cout << "程序即将退出" << endl;
            LLMHandle _tmp = llmHandle;
            llmHandle = nullptr;
            rkllm_destroy(_tmp);
        }
    }
    exit(signal);
}
/*
LLM_RUN_NORMAL：表示RKLLM模型当前正在推理中；
LLM_RUN_FINISH：表示RKLLM模型已完成当前输入的全部推理；
LLM_RUN_WAITING：表示当前RKLLM解码出的字符不是完整UTF8编码，需等待与下一次解码拼接；
LLM_RUN_ERROR：表示RKLLM模型推理出现错误；
*/
void callback(RKLLMResult *result, void *userdata, LLMCallState state)
{
    if (state == RKLLM_RUN_FINISH)
    {
        printf("\n");
    } else if (state == RKLLM_RUN_ERROR) {
        printf("\\run error\n");
    } else if (state == RKLLM_RUN_GET_LAST_HIDDEN_LAYER) {
        /* ================================================================================================================
        若使用GET_LAST_HIDDEN_LAYER功能,callback接口会回传内存指针:last_hidden_layer,token数量:num_tokens与隐藏层大小:embd_size
        通过这三个参数可以取得last_hidden_layer中的数据
        注:需要在当前callback中获取,若未及时获取,下一次callback会将该指针释放
        ===============================================================================================================*/
        if (result->last_hidden_layer.embd_size != 0 && result->last_hidden_layer.num_tokens != 0) {
            int data_size = result->last_hidden_layer.embd_size * result->last_hidden_layer.num_tokens * sizeof(float);
            printf("\ndata_size:%d",data_size);
            std::ofstream outFile("last_hidden_layer.bin", std::ios::binary);
            if (outFile.is_open()) {
                outFile.write(reinterpret_cast<const char*>(result->last_hidden_layer.hidden_states), data_size);
                outFile.close();
                std::cout << "Data saved to output.bin successfully!" << std::endl;
            } else {
                std::cerr << "Failed to open the file for writing!" << std::endl;
            }
        }
    } else if (state == RKLLM_RUN_NORMAL) {
        printf("%s", result->text);
    }
}

int main(int argc, char **argv)
{
    if (argc < 4) {
        std::cerr << "Usage: " << argv[0] << " model_path max_new_tokens max_context_len\n";
        return 1;
    }

    signal(SIGINT, exit_handler);
    printf("rkllm init start\n");

    //设置参数及初始化
    RKLLMParam param = rkllm_createDefaultParam();
    param.model_path = argv[1];

    //设置采样参数
    param.top_k = 1;
    param.top_p = 0.95;
    param.temperature = 0.8;
    param.repeat_penalty = 1.1;
    param.frequency_penalty = 0.0;
    param.presence_penalty = 0.0;

    param.max_new_tokens = std::atoi(argv[2]);
    param.max_context_len = std::atoi(argv[3]);
    param.skip_special_token = true;
    param.extend_param.base_domain_id = 0;

    int ret = rkllm_init(&llmHandle, &param, callback);
    if (ret == 0){
        printf("rkllm init success\n");
    } else {
        printf("rkllm init failed\n");
        exit_handler(-1);
    }

    vector<string> pre_input;
    pre_input.push_back("现有一笼子，里面有鸡和兔子若干只，数一数，共有头14个，腿38条，求鸡和兔子各有多少只？");
    pre_input.push_back("有28位小朋友排成一行,从左边开始数第10位是学豆,从右边开始数他是第几位?");
    cout << "\n**********************可输入以下问题对应序号获取回答/或自定义输入********************\n"
         << endl;
    for (int i = 0; i < (int)pre_input.size(); i++)
    {
        cout << "[" << i << "] " << pre_input[i] << endl;
    }
    cout << "\n*************************************************************************\n"
         << endl;

    string text;
    RKLLMInput rkllm_input;

    // 初始化 infer 参数结构体
    RKLLMInferParam rkllm_infer_params;
    memset(&rkllm_infer_params, 0, sizeof(RKLLMInferParam));  // 将所有内容初始化为 0
    rkllm_infer_params.mode = RKLLM_INFER_GENERATE;

    while (true)
    {
        std::string input_str;
        printf("\n");
        printf("user: ");
        std::getline(std::cin, input_str);
        if (input_str == "exit")
        {
            break;
        }
        for (int i = 0; i < (int)pre_input.size(); i++)
        {
            if (input_str == to_string(i))
            {
                input_str = pre_input[i];
                cout << input_str << endl;
            }
        }
        text = PROMPT_TEXT_PREFIX + input_str + PROMPT_TEXT_POSTFIX;
        // text = input_str;
        rkllm_input.input_type = RKLLM_INPUT_PROMPT;
        rkllm_input.prompt_input = (char *)text.c_str();
        printf("robot: ");

        // 若要使用普通推理功能,则配置rkllm_infer_mode为RKLLM_INFER_GENERATE或不配置参数
        rkllm_run(llmHandle, &rkllm_input, &rkllm_infer_params, NULL);
    }
    rkllm_destroy(llmHandle);

    return 0;
}
```

```cmake
cmake_minimum_required(VERSION 3.8)
project(atk_deepseek_rkllm_demo)
set(CMAKE_CXX_STANDARD 11)
set(CMAKE_CXX_STANDARD_REQUIRED ON)

set(TOOLCHAIN_DIR /home/jiao/yh-linux/rk3588/prebuilts/gcc/linux-x86/aarch64/gcc-arm-10.3-2021.07-x86_64-aarch64-none-linux-gnu)
set(CMAKE_CXX_COMPILER ${TOOLCHAIN_DIR}/bin/aarch64-rockchip1031-linux-gnu-g++)
set(CMAKE_C_COMPILER ${TOOLCHAIN_DIR}/bin/aarch64-rockchip1031-linux-gnu-gcc)

include_directories(${CMAKE_SOURCE_DIR}/lib/librkllm_api/include/)
set(RKLLM_RT_LIB ${CMAKE_SOURCE_DIR}/lib/librkllm_api/librkllmrt.so)
set(GOMP_LIB ${CMAKE_SOURCE_DIR}/lib/libgomp/libgomp.so)

add_executable(atk_deepseek_demo main.cc)
target_link_libraries(atk_deepseek_demo ${RKLLM_RT_LIB} ${GOMP_LIB})

set(CMAKE_INSTALL_PREFIX ${CMAKE_SOURCE_DIR}/install/atk_deepseek_rkllm_demo)
install(TARGETS atk_deepseek_demo DESTINATION ./)
install(DIRECTORY rkllm_model DESTINATION ./)
```

```bash
set -e

ROOT_PWD=$( cd "$( dirname $0 )" && cd -P "$( dirname "$SOURCE" )" && pwd )
BUILD_DIR=${ROOT_PWD}/build/build_linux_aarch64

if [[ ! -d "${BUILD_DIR}" ]]; then
  mkdir -p ${BUILD_DIR}
fi

cd ${BUILD_DIR}
cmake ../..
make -j4
make install
cd -
```

运行以后的结果放在`./install/atk_deepseek_rkllm_demo`文件夹里面, 把这个文件复制到开发板

实际运行的命令

```bash
 ./atk_deepseek_demo rkllm_model/deepseek-1.5b-w8a8-rk3588.rkllm 5000 5000
```

> 两个数字指的是, 实际的使用可以在的RKLLM的开源项目手册里面看到, 这里是设置生成token的上限以及推理的token上限