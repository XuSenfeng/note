# 模型部署

## TensorRT

TensorRT是可以在NVIDIA各种GPU硬件平台下运行的一个C++推理框架。我们利用Pytorch、TF或者其他框架训练好的模型，可以转化为TensorRT的格式，然后利用TensorRT推理引擎去运行我们这个模型，从而提升这个模型在英伟达GPU上运行的速度。速度提升的比例是比较可观的。

需要安装CUDA以及CUDNN

TensorRT官方支持Caffe、Tensorflow、Pytorch、ONNX等模型的转换, 对ONNX的支持最好，TensorRT-8最新版ONNX转换器又支持了更多的op操作。而深度学习框架中，TensorRT对Pytorch的支持更为友好

>   MaixCAM使用的也是这个格式的模型

## 模型训练

```python
import warnings
warnings.filterwarnings('ignore')
from ultralytics import YOLO

if __name__ == '__main__':
    model = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)
    model.train(data='my_dataset.yaml',
                cache=False,
                imgsz=416,
                epochs=100,
                single_cls=False,  # 是否是单类别检测
                batch=8,
                close_mosaic=10, # Mosaic 是 YOLO 专属增强（拼接 4 张图），提升小目标检测效果, 训练10以后关闭 Mosaic 效果更佳
                workers=0,
                device='mps',
                optimizer='SGD', # 优化器类型（更新模型参数的算法）：
                amp=True, # True：用 FP16（半精度）+FP32（单精度）混合计算，加速训练
                )
```

测试`yolo detect predict model=../ultralytics/runs/detect/train4/weights/best.pt source=./calling20221009.mp4 show=True`

## 模型导出

可以使用yolo提供的export.py文件进行导出

`pip install onnx`

```python
python export.py --weights yolov5s.pt --include onnx --opset 13
```

指定**较低的 opset 版本**（如 16/13/12，这些版本兼容性更好）