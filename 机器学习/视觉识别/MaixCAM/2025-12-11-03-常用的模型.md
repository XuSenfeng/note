# 常用模型

## MobileNet

Google 针对手机和嵌入式场景提出的一种轻量级的深度神经网络，其主要特点是使用深度可分离卷积（depthwise separable convolution）来替代普通卷积，从而减少计算量，提高网络的计算效率。该网络在 ImageNet 数据集上的分类精度达到了 70.8%，在损失了不多精度的情况下，极大地减少了计算量

> 目标: 按顺序输入若干张图像，输出每张图像属于哪个分类，以及该分类的置信度

### 原理

使用卷积提取图片的特征, 可以在[TensorSpace Playground - MobileNetv1](https://tensorspace.org/html/playground/mobilenetv1.html)这里看到

经过整个由无数个卷积计算和其它计算组成的网络计算，最后输出一个只有 1000 个像素点的图像，不同分类的图输入，在输出层的 1000 个像素点中，其中一个像素点的值会较大，这个像素点对应的分类就是网络的输出结果。比如一只熊猫图像输入，如果输出层的第 388 个像素点的值较大，并且值为 0.8，我们就通过这个网络识别到了这张图像是一只熊猫

> 深度可分离卷积（Depthwise Separable Convolution）是一种把标准卷积分解成两步的卷积操作，旨在降低参数量和计算量。它通常包含两部分：
>
> 1. **Depthwise 卷积（逐通道卷积）**：对每个输入通道独立应用一个空间卷积（如 3x3），不跨通道混合。
> 2. **Pointwise 卷积（1x1 卷积）**：用一个 1x1 的卷积核对每个像素点的通道进行线性组合，从而实现跨通道的信息混合。
>
> 与传统的标准卷积相比，深度可分离卷积把一个 Dk×Dk*D**k*×*D**k* 的卷积核的计算量从 Cin×Cout×Dk2*C**in*×*C**o**u**t*×*D**k*2 降低到：
>
> - Depthwise：Cin×Dk2×H×W*C**in*×*D**k*2×*H*×*W*（每个输入通道一个卷积核）
> - Pointwise：Cin×Cout×H×W*C**in*×*C**o**u**t*×*H*×*W*（1x1 跨通道混合）
>
> 总量约等于标准卷积的一个分数，依赖于通道数和核大小。

## Yolov2

YOLO(You Only Look Once) v2 成功让目标检测达到实时的同时，有着较高检测准确率，正如其名字一样，你只需要看一眼，就能知道结果

YOLO v2 因其检测效率高，可以在性能不是很强的边缘设备运行，不过也有缺点，就是对于小物体的检测能力不够

> 目标: 输入一张图片，输出图片中的目标类别和位置，能同时检测多个物体。

YOLOV2 主干网络也是卷积网络，输出层则使用了 S x S x B(4 + 1 + C) 这样一个三维结构，其中 S 是网格的大小，B 是每个网格预测的边界框数量(Anchor 数量）， 4 个预测框的坐标值（xmin,ymin,xmax,ymax）， 1 个置信度，C 是类别数量

> 在 S x S 个网格中，每个网格预测 B 个框，每个框有 5 个参数，分别是 x, y, w, h, p, 最后 C 个参数分别是对应类别的概率

对于这 B 个框，预先我们手动确定了 B 个 anchor（预选框），每个 anchor 有 w, h 两个参数，这里 B 个预测框的 w, h 是相对于这 B 个 anchor 的缩放系数，这样预测框的 w, h 就可以是任意值了

> 这 B 个 Anchor 是使用 K-Means 算法对训练的数据所有标注框的宽高聚类得到的，聚类的目标是让每个 anchor 的宽高比接近真实框的宽高比，这样预测框的宽高比就会更接近真实框的宽高比，从而提高检测精度。所以代码里面会有一个 anchors 参数，这个参数就是我们训练的时候统计训练数据得到的 B 个 anchor，实际在跑模型时一定要保证这个参数和训练时的一致

在获得所有预测框后，还需要对预测框进行过滤，去除一些置信度低的框，即上面说的 p，这也就是我们在代码里看到的阈值（threshold）; 以及需要进行 nms（non maximum suppression/非极大值抑制） 计算去除预测出相同分类并且有太多重叠部分的框，保留概率大的一个即可，所以代码里面我们看到有一个 nms_threshold 参数，这个参数就是用来过滤 IOU（两个框的相交区域（交集）面积/并集面积）大于这个阈值的框的

## FOMO

FOMO(Faster Objects, More Objects) 是由 Edgeimpulse 工程师提出的一种轻量级的目标检测模型，其主要特点是模型非常小，计算量也很小，表现出来就是速度非常快，精度也不错

> 输入图片，检测目标的位置和大小，并识别出物体的类别，能同时检测多个物体。

和 YOLO v2 类似，目的都是检测到物体，但是 YOLO v2 的后处理还是比较复杂，有大量的框需要处理，想在没有硬件加速的单片机上运行还是很吃力的

FOMO 采用更简单的思路来做检测：

- 使用一个经典网络作为特征提取器，比如 MobileNet v1， 然后从网络中间截断，得到一个特征图，这个特征图的大小是 n x n x c, n 是特征图的宽高，c 是特征图的通道数

这里 n 的取值取决于从网络哪里截断，比如我们输入分辨率是 128 x 128, 想要一个 8 x 8 的特征图输出，相当于把图片分辨率降低了 16 倍，从网络找到该层截断得到 8 x 8 的特征图

- 这个 n x n x c， c 代表了有 c 个分类，每一层用来找一个分类的物体的位置，每层有 n x n 个像素，每个像素代表了在该位置是否有该分类的物体（的置信度）
- 遍历这个 n x n x c 的特征图，找到置信度超过设置的阈值的像素坐标，我们就认为这些地方有物体存在，然后按照缩放比例映射到原图，比如只有一个分类即 c 为 1 时，我们要检测一个杯子，得到如下的结果

![img](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/lenovo-picture/202512112252246.jpeg)

- 在得到了一大堆看起来有效的坐标点后，我们认为这些地方有物体存在，但是有一大堆点，我们可以简单地将挨着的点合并成一个框，这样就得到了一个大框

`mobilenetv1_0.25_8`这样的名字代表使用了`mobilenetv1`网络， alpha 为 0.25, 这个数值越小网络越小，准确率越低，最后面的 8 则代表了输出分辨率是输入分辨率的 1/8 ，比如 输入 128x128，输出就是 16x16，输出的分辨率越大越适合检测小一点的物体，根据你的单片机性能和被检测的物体大小来选择。

> - 定义：alpha 是宽度乘子，用来按比例缩放每层的通道数。若基线 Mobilenetv1 某一层有 C 个输出通道，使用 alpha=0.25 时，该层实际输出通道数为 0.25 × C（通常取整到最近的整数）。
> - 作用：
>     - 模型规模变小：通道数减少，参数量和浮点运算量（FLOPs）显著降低。
>     - 推理速度提高：更少的计算和内存带宽需求通常带来更快的推理，特别是在资源受限的设备上。