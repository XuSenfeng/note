---
layout: post
title: "机器学习" 
date:   2024-8-5 15:39:08 +0800
tags: AI 机器学习
---

# 机器学习

how to make machine learn to do it by itself

依据当前获取的数据, 获取数据的特征值

> 深度学习: 种特殊的机器学习，它通过学习将世界表示为嵌套的概念层次结构来实现强大的功能和灵活性，每个概念都是根据更简单的概念进行定义的，而更抽象的概念则是用不那么抽象（更加具象）的概念计算出来的

> 区别 :深度学习会自动找出对分类很重要的特征，在机器学习中我们必须手动提供这些特征。 重要的区别会随着数据规模的增大而表现出来
>
> 当数据很小时，深度学习算法表现不佳。这是因为深度学习算法需要大量数据才能完美理解它。另一方面，传统的机器学习算法及其手工制作的规则在这种情况下占据优势。
>
> 通常，深度学习算法需要很长时间来训练。这是因为深度学习算法中有很多的参数，所以训练它们需要更长的时间。最先进的深度学习算法ResNet需要大约两周时间才能完全从0开始的训练。相比之下，机器学习的训练时间要短得多，从几秒钟到几小时不等。

机器学习是人工智能的实现方法, 深度学习是机器学习的一部分, 是他的一个方法

## Algorithms

机器学习主要有两种, 监管学习(supervised learning)和无监管学习(unsupervised learning), 第一种使用的比较多, 还有一种强化学习(Reinforcement learning)

### supervised learning监督学习

input to output mappings

learn from being given the right answers 

#### 回归Regression

predict a number from infinitely many possible numbers, 从无数个数字里面预测新的数字, 想办法找到一个最适合的直线, 尽可能的拟合更多的点

![image-20240908122138302](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409081221363.png)

这里的$ \theta0 $ 符号是一个偏置, 在实际计算的时候, 相当于矩阵里面加一列值为1的$\theta_0$这里使用1的原因是 $\theta_0$和1乘的结果是本身的数字

误差: 真实值和预测值之间是存在一个误差的, 使用$\varepsilon$表示, $y^{(i)}=\theta^Tx^{(i)}+\varepsilon^{(i)}$​

这里的误差是独立的具有相同的分布, 服从于均值为0方差为$\theta^2$​的高斯分布, 使用的数据尽可能来自相同的分布(全部来自同一个对象)

这里面的参数被叫做系数(coefficients)或者权重(weights), 实际计算的时候, 把预测值和实际的值进行作差, 平方, 计算所有的差的平方的平均数, 最小的时候即为所求, 在实际使用的时候, 会多除以一个2, 用于使一个比较大的的数字看起来比较整洁  Squared error cost function, 这一个计算的方式在使用线性回归的时候效果最好, 其他的方程也差不多

![image-20240910221432231](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409102214559.png)

#### classification分类算法

依据现有的数据, 对一个新的数据进行分类, 使用这一种算法进行预测的时候, 实际 获取的数字不是连续的

### Unsupervised Learning无监督学习

在使用这一个学习的方式的时候, 我们提供数据, 但是不会对一个数据进行实际的分类

这一个方法可以用于对大量的数据进行分类, 比如说Google的新闻分类, DNA序列的分类, 这一个方式被叫做clustering

这一种学习的方式是只有输入的标签X, 但是没有输出的标签Y, 使用的算法数据, 由算法去查找数据里面的结构

+ Anomaly detection异常检测

可以用于金融领域里面, 用于检测是不是存在诈骗

+ Dimensionality reduction 降维

把一个比较大的数据进行缩小为一个尽可能小的数据集, 同时尽可能的减少数据的丢失, 可以用于一个比较大的数据的压缩

 ## 实际模型

### Linear Regression Model 线性回归模型

 

### 梯度下降算法

是一个用于求$J(W1, w2, ..., w3, b)$这一个式子的最小值的方法, 一直改变w和b的值, 直到获取这一个函数的最小值

如果这一个方程不是一个线性方程以及使用一个平法误差方程, 实际的获取的J函数可能比较复杂, 这一个函数的实际实现方式是计算一下当前位置的所有方向斜率最大的一个, 之后向这一个方向

在起时的时候选择不同的位置, 可能会到达不同的最小值的位置, 这时候找到的最小值是一个局部最小值

$w = w - \alpha\frac{\partial}{\partial w}J(w,b)$, 则这一个式子里面, $\alpha$被叫做学习率, 这一个值一般是一个0-1之间的数字, 使用这一个值表示你的下山的一步的步长, 和$b = b - \alpha\frac{\partial}{\partial b}J(w,b)$式子一起使用

在实际使用的时候, 会使用一个临时值记录一下更新前的数字, 之后进行计算, 计算结束以后再把这两个数字同时写入进去, 这里实际是应用了一下梯度的定义

$\frac{\partial}{\partial w}J(w,b)$这一个式子如果计算的是![image-20240910221432231](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409102214559.png), 求导以后得式子是$\frac{1}{m}\sum_{i=1}^m(f_{w, b}(x^{(i)}) - y^{(i)})x^{(i)}$

$\frac{\partial}{\partial b}J(w,b)$的结果是$\frac{1}{m}\sum_{i=1}^m(f_{w, b}(x^{(i)}) - y^{(i)})$
