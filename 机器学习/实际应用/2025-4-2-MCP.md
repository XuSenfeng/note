# MCP

大模型运行的环境是MCP Client, 实际的函数运行的功能是MCP Server, 这部分调用外部的其它工具

现在开发一个Agent主要需要有四个模块, Planing模块, 规划模型的行动, Tools模块, 负责模型可以使用的外部工具, Memory模块, 负责模型对话的记忆, Action模块, 模型行动的基本流程(交互)

MCP是对于Tools模块的进一步开发

## 开发

### uv管理工具

MCP开发主要依赖的工具, 是一个python的依赖管理工具, 类似于pip和conda, 但是更加的高效同时可以更好的管理Python的虚拟环境依赖项

安装`pip install uv`, 没有pip可以使用`curl -LsSf https://astral.sh/uv/install.sh | sh`

#### 基本语法

`pip venv install requests`安装

`uv venv myenv`建立环境

`source menv/bin/activate` 激活环境, `myenv\Scripts\activate`Win

`uv pip install -r requirements.txt`

`uv run python script.py`如果里面有pyproject.toml这个文件, 这个相当于安装安装requirements之后运行这个文件

#### 建立MCP工程

`uv init mcp-client`

`cd mcp-client`

`uv venv --python 3.12`

`source .venv/bin/activate`

uv可以主动识别当前项目的主目录进行安装创建开发环境

`uv add`添加对应的库

### Client开发

#### 基本流程

```python
import asyncio
from mcp import ClientSession
from contextlib import AsyncExitStack

class MCPClient:
    def __init__(self):
        """初始化客户端"""
        self.session = None
        self.exit_stack = AsyncExitStack() # 上下文管理器栈

    async def connect_to_mock_server(self):
        """模拟 MCP 服务器连接"""
        print("Connecting to mock server...")

    async def chat_loop(self):
        """聊天循环"""
        print("Starting chat loop...")
        while True:
            try:
                query = input("You: ")
                if query.lower() == "exit":
                    print("Exiting chat...")
                    break
                # response = await self.session.chat(query)
                # print(f"Response: {response}")
                print(f"Bot: {query}")
            except Exception as e:
                print(f"Error during chat: {e}")
                break
        print("Chat loop ended.")
    
    async def cleanup(self):
        """清理资源"""
        if self.session:
            await self.session.close()
        await self.exit_stack.aclose()
        print("Resources cleaned up.")

async def main():
    client = MCPClient()
    try: 
        await client.connect_to_mock_server()
        await client.chat_loop()
    finally:
        await client.cleanup() 

if __name__ == "__main__":
    asyncio.run(main())
```

> 可以使用命令`uv run client.py  `来运行服务器

#### 接入大模型

`uv add openai python-dotenv`安装这两个库, 第二个库是读取.env文件使用的库

.env文件的内容如下

```
BASE_URL=https://api.deepseek.com
MODEL=deepseek-chat
OPENAI_API_KEY=sk-16ed1c1c9a934c1393425430a4f7d26c
```

使用下面的方式进行调用

```python
import asyncio
from mcp import ClientSession
from contextlib import AsyncExitStack
import os
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv(override=True) # 加载环境变量

class MCPClient:
    def __init__(self):
        """初始化客户端"""
        self.exit_stack = AsyncExitStack() # 上下文管理器栈
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.model = os.getenv("MODEL", "gpt-3.5-turbo")
        self.base_url = os.getenv("BASE_URL", "https://api.openai.com/v1")

        self.client = OpenAI(api_key=self.openai_api_key, base_url=self.base_url)

    async def process_query(self, query):
        """处理查询"""
        messages = [
            {"role": "system", "content": "你是一个猫娘智能助手, 帮助用户解决问题"},
            {"role": "user", "content": query}
        ]

        try:
            # 函数的主要作用是将一个同步的阻塞函数放入一个线程池中执行，
            # 以便在异步的事件循环中避免阻塞。在异步编程中，通常情况下应避免在事件循环中执行耗时的阻塞操作，
            # 否则会导致整个事件循环被阻塞，影响程序的响应性和性能
            response = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.client.chat.completions.create(
                    model=self.model,
                    messages=messages
                )
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"Error processing query: {e}")
            return None

    async def connect_to_mock_server(self):
        """模拟 MCP 服务器连接"""
        print("Connecting to mock server...")

    async def chat_loop(self):
        """聊天循环"""
        print("Starting chat loop...")
        while True:
            try:
                query = input("You: ")
                if query.lower() == "exit":
                    print("Exiting chat...")
                    break
                response = await self.process_query(query)
                print(f"Bot: {response}")
            except Exception as e:
                print(f"Error during chat: {e}")
                break
        print("Chat loop ended.")
    
    async def cleanup(self):
        """清理资源"""
        if self.session:
            await self.session.close()
        await self.exit_stack.aclose()
        print("Resources cleaned up.")

async def main():
    client = MCPClient()
    try: 
        await client.connect_to_mock_server()
        await client.chat_loop()
    finally:
        await client.cleanup() 

if __name__ == "__main__":
    asyncio.run(main())
```

#### 终极版本

```python
import asyncio
from mcp import ClientSession
from contextlib import AsyncExitStack
import os
from openai import OpenAI
from dotenv import load_dotenv
# MCP 客户端
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

from typing import Optional
import json
import sys
load_dotenv(override=True) # 加载环境变量

class MCPClient:
    def __init__(self):
        """初始化客户端"""
        self.exit_stack = AsyncExitStack() # 上下文管理器栈
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.model = os.getenv("MODEL", "gpt-3.5-turbo")
        self.base_url = os.getenv("BASE_URL", "https://api.openai.com/v1")
        
        self.client = OpenAI(api_key=self.openai_api_key, base_url=self.base_url)
        self.session: Optional[ClientSession] = None


    async def process_query(self, query):
        """处理查询"""
        messages = [
            {"role": "system", "content": "你是一个猫娘智能助手, 帮助用户解决问题"},
            {"role": "user", "content": query}
        ]

        response = await self.session.list_tools()

        available_tools = [{
            "type": "function",
            "function": {
                "name": tool.name,
                "description": tool.description,
                "input_schema": tool.inputSchema
            }
        }for tool in response.tools]

        try:
            # 函数的主要作用是将一个同步的阻塞函数放入一个线程池中执行，
            # 以便在异步的事件循环中避免阻塞。在异步编程中，通常情况下应避免在事件循环中执行耗时的阻塞操作，
            # 否则会导致整个事件循环被阻塞，影响程序的响应性和性能
            response = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    tools=available_tools
                )
            )
            content = response.choices[0]
            if content.finish_reason == "tool_calls":
                # 处理工具调用
                # 这里的content.message是一个包含工具调用信息的对象
                # 通过content.message.tool_calls获取工具调用的列表
                # 这里假设只处理第一个工具调用
                tool_call = content.message.tool_calls[0]
                tool_name = tool_call.function.name
                tool_args = json.loads(tool_call.function.arguments)
                # 调用工具
                # 这里的tool_name是工具的名称，tool_args是工具调用的参数
                result = await self.session.call_tool(
                    tool_name,
                    tool_args
                )
                # 依据工具调用的结果更新消息
                # 这里的content.message是一个包含工具调用信息的对象
                messages.append(content.message.model_dump())
                messages.append({
                    "role": "tool",
                    "content": result.content[0].text,
                    "tool_call_id": tool_call.id,
                })
                # 重新调用模型
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages
                )
                return response.choices[0].message.content
            return content.message.content


        except Exception as e:
            print(f"Error processing query: {e}")
            return None

    async def connect_to_mock_server(self, server_script_path: str):
        """模拟 MCP 服务器连接"""
        print("Connecting to mock server...")
        is_python = server_script_path.endswith(".py")
        is_js = server_script_path.endswith(".js")
        if not (is_python or is_js):
            raise ValueError("Server script must be a .py or .js file.")
        
        command = "python" if is_python else "node"
        server_params = StdioServerParameters(
            command=command,
            args=[server_script_path],
            env=None,
        )
        # AsyncExitStack是一个异步上下文管理器，
        # 它可以自动管理多个异步上下文管理器的退出操作。
        # 这个类允许您将多个异步上下文管理器堆叠在一起，并在退出时按照逆序自动调用它们的退出操作。
        # 它有助于确保在协程结束时清理资源和执行必要的清理操作，而无需手动管理所有上下文管理器的退出操作。

        # stdio_client是一个异步上下文管理器，用于创建与MCP服务器的标准输入/输出连接。
        # 它返回一个包含两个元素的元组，第一个元素是标准输入流，第二个元素是标准输出流。
        # 这两个流可以用于与MCP服务器进行通信。
        stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))
        self.stdio, self.write = stdio_transport
        # ClientSession是一个异步上下文管理器，用于与MCP服务器建立会话。
        # 它提供了与服务器进行交互的方法，例如发送请求和接收响应。
        # 通过将标准输入流和写入流传递给ClientSession，您可以在会话中使用这些流进行通信。
        # 这里的self.stdio是标准输入流，self.write是标准输出流。
        # ClientSession的构造函数接受两个参数：标准输入流和写入流。
        # 这两个流用于与MCP服务器进行通信。
        self.session = await self.exit_stack.enter_async_context(ClientSession(self.stdio, self.write))
        print("Connected to mock server.")
        # 初始化会话
        await self.session.initialize()

        response = await self.session.list_tools()
        tools = response.tools
        print("Available tools:")
        for tool in tools:
            print(f"- {tool.name}: {tool.description}")



    async def chat_loop(self):
        """聊天循环"""
        print("Starting chat loop...")
        while True:
            try:
                query = input("You: ")
                if query.lower() == "exit":
                    print("Exiting chat...")
                    break
                response = await self.process_query(query)
                print(f"Bot: {response}")
            except Exception as e:
                print(f"Error during chat: {e}")
                break
        print("Chat loop ended.")
    
    async def cleanup(self):
        """清理资源"""
        await self.exit_stack.aclose()
        self.client.close()
        print("Resources cleaned up.")

async def main():
    if len(sys.argv) < 2:
        print("Usage: python client.py <server_script_path>")
        sys.exit(1)
    client = MCPClient()
    try: 
        await client.connect_to_mock_server(sys.argv[1])
        await client.chat_loop()
    finally:
        await client.cleanup()

if __name__ == "__main__":
    asyncio.run(main())
```

### Server开发

server可以提供三种类型的标准能力, Resources, Tools, Prompts, 每一个Server可以提供这三种类型能力里面的几种

Resources: 资源, 类似于文件读取, 文件资源或者API响应的返回

Tools: 工具, 第三方的工具服务, 功能函数, 可以控制LLM可以调用的函数

Prompts: 提示词, 用户预先定义好的任务模板

+ 通信机制

1. 远程

HTTP: 使用请求-响应的模式, 客户端发送请求, 服务器返回响应, 每一次的请求都是独立的

SSE: 允许服务器通过单个持久的HTTP连接, 持续向客户端推送数据

2. 同一个机器

可以使用stdio进行通信

> 使用的通信格式都是JSON-RPC 2.0格式

#### 天气示例

```python
import json
import httpx
from typing import Any
from mcp.server.fastmcp import FastMCP

mcp = FastMCP("WeatherServer")

WEATHER_API_BASE = "https://apis.map.qq.com/ws/weather/v1/"
IP_LOCAL_API_BASE = "https://apis.map.qq.com/ws/location/v1/ip"
API_KEY = "IQ7BZ-P7X6W-2I4RK-3BWCR-GN3CZ-PSFQC"

async def get_location():
    params = {
        "key": API_KEY,
    }
    # 使用本机ip获取位置编码
    async with httpx.AsyncClient() as client:
        response = await client.get(IP_LOCAL_API_BASE, params=params)
        result = response.json()
        print(result)
        if result["status"] == 0:
            return result["result"]["ad_info"]["adcode"]
        else:
            print("Error getting location:", result["message"])
            return None


async def get_weather(adcode):
    params = {
        "key": API_KEY,
        "adcode": adcode,
    }
    async with httpx.AsyncClient() as client:
        response = await client.get(WEATHER_API_BASE, params=params)
        result = response.json()
        print(result)
        if result["status"] == 0:
            return result["result"]["realtime"][0]
        else:
            print("Error getting weather:", result["message"])
            return None

def format_weather(data: dict[str, Any]) -> str:
    """格式化天气数据"""
    """
    {
        'province': '北京市', 
        'city': '', 'district': '', 
        'adcode': 110000, 
        'update_time': '2025-04-02 16:25', 
        'infos': {
            'weather': '晴天', 
            'temperature': 18, 
            'wind_direction': '西南风', 
            'wind_power': '4-5级', 
            'humidity': 11
        }
    }
    """
    if not data:
        return "获取天气信息失败"
    info = data.get("infos", {})
    province = data.get("province", "未知")
    city = data.get("city", "未知")
    update_time = data.get("update_time", "未知")
    weather = info.get("weather", "未知")
    temperature = info.get("temperature", "未知")
    wind_direction = info.get("wind_direction", "未知")
    wind_power = info.get("wind_power", "未知")
    humidity = info.get("humidity", "未知")

    return (
        f"当前城市：{province}{city}\n"
        f"天气更新时间：{update_time}\n"
        f"当前天气：{weather}\n"
        f"当前气温：{temperature}°C\n"
        f"风向：{wind_direction}\n"
        f"风力：{wind_power}\n"
        f"湿度：{humidity}%\n"
    )

@mcp.tool()
async def query_weather():
    """
    查询天气
    :return: 格式化以后得天气信息
    """
    adcode = await get_location()
    if not adcode:
        return "获取位置编码失败"
    weather_data = await get_weather(adcode)
    if not weather_data:
        return "获取天气信息失败"
    formatted_weather = format_weather(weather_data)
    return formatted_weather

if __name__ == "__main__":
    # 启动 MCP 服务器
    mcp.run(transport="stdio")
```

## 调试工具

MCP Inspector, 可以使用图形界面调试server, 需要安装nxp

`npx -y @modelcontextprotocol/inspector uv run server.py`

![image-20250402181049486](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/lenovo-picture/202504021810874.png)

![image-20250402181131032](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/lenovo-picture/202504021811125.png)

![image-20250402181212789](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/lenovo-picture/202504021812932.png)

![image-20250402181338887](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/lenovo-picture/202504021813132.png)