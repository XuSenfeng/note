# LoRA微调

下载下来的参数是不变的, 额外加入自己的参数

为了节省资源, 在两个比较大的层之间加入比较小的层, 通常是64或128个参数, 可以使得两个1000W的层参数由1000W * 1000W变为: 1000W * 64 + 64 * 64 + 64 * 1000W

原始的模型是没有改变的, 自己的数据和官方的模型相加获取新的参数矩阵

![image-20250226144728784](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/lenovo-picture/202502261447909.png)

可以单独训练出来不通的配套参数, 从而适配不同的工作

## 工具包

LLaMA-Factory/PEFT

