RunnablePassthroughå…è®¸ä¼ é€’è¾“å…¥æ•°æ®ï¼Œå¯ä»¥ä¿æŒä¸å˜æˆ–æ·»åŠ é¢å¤–çš„é”®ã€‚é€šå¸¸ä¸RunnableParallelä¸€èµ·ä½¿ç”¨ï¼Œå°†æ•°æ®åˆ†é…ç»™æ˜ å°„ä¸­çš„æ–°é”®ã€‚

å®é™…æ˜¯å¯¹ä¸Šä¸€å±‚çš„è¾“å‡ºåšå¤„ç†, ä½œä¸ºå‚æ•°ä¼ å…¥RunnablePassthrough, å¯ä»¥ä½¿ç”¨assignæ¥ç»™å‚æ•°é‡Œé¢æ·»åŠ æ–°çš„é”®å€¼å¯¹

```python
from langchain_core.runnables import RunnableParallel, RunnablePassthrough

runnable = RunnableParallel(
    passed=RunnablePassthrough(),
    extra=RunnablePassthrough.assign(mult=lambda x: x["num"] * 3),
    modified=lambda x: x["num"] + 1,
)

runnable.invoke({"num": 1})
```

> `passed` é”®ä½¿ç”¨ `RunnablePassthrough()` è°ƒç”¨ï¼Œå› æ­¤å®ƒåªæ˜¯ä¼ é€’äº† `{'num': 1}`ã€‚
>
> åœ¨ç¬¬äºŒè¡Œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å¸¦æœ‰å°†æ•°å€¼ä¹˜ä»¥3çš„lambdaçš„ `RunnablePastshrough.assign`ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ`extra` è¢«è®¾ç½®ä¸º `{'num': 1, 'mult': 3}`ï¼Œå³åŸå§‹å€¼åŠ ä¸Š `mult` é”®ã€‚
>
> æœ€åï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨lambdaåœ¨æ˜ å°„ä¸­è®¾ç½®äº†ç¬¬ä¸‰ä¸ªé”® `modified`ï¼Œå°†numåŠ 1ï¼Œç»“æœä¸º `modified` é”®çš„å€¼ä¸º `2`ã€‚

```python
retrieval_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)

retrieval_chain.invoke("where did harrison work?")
```

> åœ¨è¾“å…¥å‚æ•°é‡Œé¢åŠ å…¥ä¸€ä¸ªcontexté”®

### è®°å¿†æ€»ç»“

å®é™…æ˜¯æŠŠå¯¹è¯äº¤ç»™AI, è®©ä»–è¿›è¡Œæ€»ç»“å¤„ç†

```python
def summarize_messages(chain_input):
    stored_messages = history.messages
    if len(stored_messages) == 0:
        return False
    summarization_prompt = ChatPromptTemplate.from_messages([
        MessagesPlaceholder(variable_name="history"),
        ("user", "æŠŠä¸Šé¢çš„æ¶ˆæ¯æµ“ç¼©ä¸€ä¸‹, å°½å¯èƒ½çš„åŒ…å«å¤šä¸ªç»†èŠ‚, å°¤å…¶æ˜¯ç”¨æˆ·çš„ç‰¹å¾")
    ])

    summarization_chain = summarization_prompt | ChatOpenAI(model="gpt-3.5-turbo-1106")
    result = summarization_chain.invoke(input={"history": stored_messages})
    history.clear()
    history.add_message(result)
    return True


chain_with_summarization = (
    RunnablePassthrough.assign(messages_summarized=summarize_messages)
    | with_message_history
)

```

## å¤šæ¨¡æ€è¾“å…¥

æŠŠè¾“å…¥æŒ‰ç…§ä¸€å®šçš„æ ¼å¼è¿›è¡Œè½¬æ¢

### å›¾ç‰‡è¾“å…¥

ç›´æ¥ä¼ é€’å›¾ç‰‡åœ°å€, è¿™ä¸ªæ–¹å¼å¯èƒ½ç”±äºAIæ— æ³•è·å–å›¾ç‰‡å¤±è´¥

```python
import base64 # è¿™æ˜¯ä¸€ä¸ªç¼–ç åº“, ç”¨äºç¼–ç å’Œè§£ç äºŒè¿›åˆ¶æ•°æ®
import httpx # è¿™æ˜¯ä¸€ä¸ªå¼‚æ­¥ HTTP å®¢æˆ·ç«¯åº“
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI
from Siliconflow import CustomLLM_Siliconflow


image_url = "https://th.bing.com/th/id/R.6b5df1bfe0e4778a44dba0753cd169c8?rik=QRQIMqvjWRCO5Q&riu=http%3a%2f%2fpic39.nipic.com%2f20140321%2f8857347_232251363165_2.jpg&ehk=7oAaMo6LCHJc%2bqpQ0IPvcH7v69jGRQhb2vDz%2fOd5720%3d&risl=&pid=ImgRaw&r=0"

# æ–¹å¼1: ç›´æ¥ä¼ å…¥ç½‘å€
model = ChatOpenAI(model="gpt-4o")
message = HumanMessage(
    content=[
        {
            "type": "text", "text": "æè¿°è¿™ä¸ªå›¾ç‰‡é‡Œé¢çš„å†…å®¹"
        },
        {
            "type": "image_url", "image_url": {"url": image_url}
        }
    ]
)
```

ä¹Ÿå¯ä»¥æŠŠå›¾ç‰‡çš„æ•°æ®ç›´æ¥ä¼ è¿‡å»

```python
import base64 # è¿™æ˜¯ä¸€ä¸ªç¼–ç åº“, ç”¨äºç¼–ç å’Œè§£ç äºŒè¿›åˆ¶æ•°æ®
import httpx # è¿™æ˜¯ä¸€ä¸ªå¼‚æ­¥ HTTP å®¢æˆ·ç«¯åº“
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI
from Siliconflow import CustomLLM_Siliconflow


image_url = "https://th.bing.com/th/id/R.6b5df1bfe0e4778a44dba0753cd169c8?rik=QRQIMqvjWRCO5Q&riu=http%3a%2f%2fpic39.nipic.com%2f20140321%2f8857347_232251363165_2.jpg&ehk=7oAaMo6LCHJc%2bqpQ0IPvcH7v69jGRQhb2vDz%2fOd5720%3d&risl=&pid=ImgRaw&r=0"
image_data = base64.b64encode(httpx.get(image_url).content).decode("utf-8") # å°†å›¾ç‰‡è½¬æ¢ä¸º base64 ç¼–ç çš„å­—ç¬¦ä¸²

message_img = HumanMessage(
    content=[
        {
            "type": "text", "text": "æè¿°è¿™ä¸ªå›¾ç‰‡é‡Œé¢çš„å†…å®¹"
        },
        {
            "type": "image_url",
            "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}
        }
    ]
)
```

```python
result = model.invoke([message_img])
print(result)
```

## ä½¿ç”¨å·¥å…·

toolæœ‰æœºéƒ¨åˆ†ç»„æˆ

+ name: åå­—, å¿…é¡»å”¯ä¸€çš„æè¿°
+ description: å·¥å…·çš„æè¿°, LLMä¹‹åä¼šä½¿ç”¨è¿™ä¸ªä½œä¸ºä¸Šä¸‹æ–‡
+ args_schema: å¯é€‰, ä½†æ˜¯å»ºè®®æä¾›æ›´å¤šçš„ä¿¡æ¯, Pydantic BaseModelçš„ç±»å‹
+ return_direct: booleanç±»å‹, ä»£ç†ç›¸å…³çš„, Trueçš„æ—¶å€™ä»£ç†åœæ­¢, ç›´æ¥å°†ç»“æœè¿”å›ç»™ç”¨æˆ·

æœ‰å‡ ç§æ–¹å¼å¯ä»¥å®šä¹‰

### è‡ªå®šä¹‰å·¥å…·

[å®šä¹‰è‡ªå®šä¹‰å·¥å…· | ğŸ¦œï¸ğŸ”— Langchain](https://python.langchain.com.cn/docs/modules/agents/tools/how_to/custom_tools)

#### @toolè£…é¥°å™¨

```python
from langchain_core.tools import tool
@tool
def weather_tool(weather: Literal["æ™´æœ—çš„", "å¤šäº‘çš„", "å¤šé›¨çš„", "ä¸‹é›ªçš„"]) -> None:
    """Discribe the weather"""
    # è¿™ä¸ªå·¥å…·æ¥å—ä¸€ä¸ªå¤©æ°”å‚æ•°, å¹¶è¿”å›ä¸€ä¸ªæè¿°å¤©æ°”çš„å­—ç¬¦ä¸²
    pass

"""
name='weather_tool' description='Discribe the weather' args_schema=<class 'langchain_core.utils.pydantic.weather_tool'> func=<function weather_tool at 0x7f57151d23e0>
"""
```

å»ºç«‹ä¸€ä¸ªå·¥å…·, å·¥å…·çš„æè¿°æ˜¯ä»–çš„æ³¨é‡Š, LangChain ä¸­çš„å·¥å…·æŠ½è±¡å°† Python å‡½æ•°ä¸å®šä¹‰å‡½æ•°**åç§°**ã€**æè¿°**å’Œ**é¢„æœŸå‚æ•°**çš„**æ¶æ„**ç›¸å…³è”ã€‚

> ```python
> print(weather_tool.name)
> print(weather_tool.description)
> print(weather_tool.args)
> """
> weather_tool
> Discribe the weather
> {'weather': {'enum': ['æ™´æœ—çš„', 'å¤šäº‘çš„', 'å¤šé›¨çš„', 'ä¸‹é›ªçš„'], 'title': 'Weather', 'type': 'string'}}
> """
> ```

+ ä½¿ç”¨

```python
from typing import Literal
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI
from Siliconflow import CustomLLM_Siliconflow
from langchain_core.tools import tool

@tool
def weather_tool(weather: Literal["æ™´æœ—çš„", "å¤šäº‘çš„", "å¤šé›¨çš„", "ä¸‹é›ªçš„"]) -> None:
    """Discribe the weather"""
    # è¿™ä¸ªå·¥å…·æ¥å—ä¸€ä¸ªå¤©æ°”å‚æ•°, å¹¶è¿”å›ä¸€ä¸ªæè¿°å¤©æ°”çš„å­—ç¬¦ä¸²
    pass

model = ChatOpenAI(model="gpt-4o")
model_with_tool = model.bind_tools([weather_tool]) # ç»‘å®šå·¥å…·

image_url = "https://th.bing.com/th/id/R.6b5df1bfe0e4778a44dba0753cd169c8?rik=QRQIMqvjWRCO5Q&riu=http%3a%2f%2fpic39.nipic.com%2f20140321%2f8857347_232251363165_2.jpg&ehk=7oAaMo6LCHJc%2bqpQ0IPvcH7v69jGRQhb2vDz%2fOd5720%3d&risl=&pid=ImgRaw&r=0"
# æ–¹å¼1: ç›´æ¥ä¼ å…¥ç½‘å€
message = HumanMessage(
    content=[
        {
            "type": "text", "text": "æè¿°è¿™ä¸ªå›¾ç‰‡é‡Œé¢çš„å†…å®¹"
        },
        {
            "type": "image_url", "image_url": {"url": image_url}
        }
    ]
)
# å¦‚æœæ¨¡å‹æƒ³ä½¿ç”¨å·¥å…·, ä»–åªå¯ä»¥è¾“å‡ºå·¥å…·æ”¯æŒçš„è¾“å…¥, æ‰€ä»¥å¯ä»¥é™å®šæ¨¡å‹çš„è¾“å‡º
result = model.invoke([message])
print(result)
```

> + Create tools using the`@tool`decorator, which simplifies the process of tool creation, supporting the following:
>     ä½¿ç”¨ [@tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) ä¿®é¥°å™¨åˆ›å»ºå·¥å…·ï¼Œè¿™ç®€åŒ–äº†å·¥å…·åˆ›å»ºè¿‡ç¨‹ï¼Œå¹¶æ”¯æŒä»¥ä¸‹åŠŸèƒ½ï¼š
> + + Automatically infer the tool's **name**, **description** and **expected arguments**, while also supporting customization.
>         è‡ªåŠ¨æ¨æ–­å·¥å…·**çš„åç§°**ã€**æè¿°**å’Œ**é¢„æœŸå‚æ•°**ï¼ŒåŒæ—¶è¿˜æ”¯æŒè‡ªå®šä¹‰ã€‚
>     + Defining tools that return **artifacts** (e.g. images, dataframes, etc.)
>         å®šä¹‰è¿”å›**å·¥ä»¶**çš„å·¥å…·ï¼ˆä¾‹å¦‚å›¾åƒã€æ•°æ®å¸§ç­‰ï¼‰
>     + Hiding input arguments from the schema (and hence from the model) using **injected tool arguments**.
>         ä½¿ç”¨**æ³¨å…¥çš„å·¥å…·å‚æ•°**ä» Schema ï¼ˆä»¥åŠæ¨¡å‹ä¸­ï¼‰ éšè—è¾“å…¥å‚æ•°ã€‚

+ å¼‚æ­¥

é»˜è®¤çš„ä½¿ç”¨æ˜¯åŒæ­¥çš„, ä¹Ÿå¯ä»¥ä½¿ç”¨å¼‚æ­¥çš„, ä¹‹åçš„è°ƒç”¨éœ€è¦ä½¿ç”¨å¼‚æ­¥çš„æ–¹å¼

```python
@tool
async def weather_tool(weather: Literal["æ™´æœ—çš„", "å¤šäº‘çš„", "å¤šé›¨çš„", "ä¸‹é›ªçš„"]) -> None:
    """Discribe the weather"""
    # è¿™ä¸ªå·¥å…·æ¥å—ä¸€ä¸ªå¤©æ°”å‚æ•°, å¹¶è¿”å›ä¸€ä¸ªæè¿°å¤©æ°”çš„å­—ç¬¦ä¸²
    pass
```

+ å‚æ•°

ä½¿ç”¨pydanticæŒ‡å®šå‚æ•°

```python
from langchain_core.tools import tool
from pydantic import BaseModel, Field

class CalculatorInput(BaseModel):
    a: int = Field(description="ç¬¬ä¸€ä¸ªæ•°å­—")
    b: int = Field(description="ç¬¬äºŒä¸ªæ•°å­—")

@tool("multiplication-tool", args_schema=CalculatorInput, return_direct=True)
def multiplication_tool(a: int, b: int) -> int:
    """Multiply two numbers"""
    return a * b

print(multiplication_tool.name)
print(multiplication_tool.description)
print(multiplication_tool.args)
print(multiplication_tool.invoke({"a": 2, "b": 3}))
"""
multiplication-tool
Multiply two numbers
{'a': {'description': 'ç¬¬ä¸€ä¸ªæ•°å­—', 'title': 'A', 'type': 'integer'}, 'b': {'description': 'ç¬¬äºŒä¸ªæ•°å­—', 'title': 'B', 'type': 'integer'}}
6
"""
```

#### StructuredTool.from_function

ç±»ä¼¼äºtoolä½†æ˜¯å…è®¸æ›´å¤šé…ç½®ä»¥åŠåŒæ­¥å¼‚æ­¥çš„è§„èŒƒ

```python
from langchain_core.tools import StructuredTool
import asyncio

def multiply(a: int, b:int)->int:
    """Multiply two numbers"""
    return a*b

async def amultiply(a: int, b:int)->int:
    """Multiply two numbers"""
    return a*b

async def main():
    calcuator = StructuredTool.from_function(func=multiply, coroutine=amultiply)
    print(calcuator.invoke({"a": 2, "b": 3}))
    print(await calcuator.ainvoke({"a": 2, "b": 3}))

asyncio.run(main())
"""
6
6
"""
```

#### å¼‚å¸¸å¤„ç†

å·¥å…·çš„å‡ºç°å¼‚å¸¸çš„ä½¿ç”¨çš„å¤„ç†

```python
from langchain_core.tools import StructuredTool
from langchain_core.tools import ToolException
from langchain.tools import Tool
def get_weather(city: str) -> int:
    """Get the weather of a city"""
    return ToolException(f"Can't get the weather of {city}")

get_weather_tool = Tool.from_function(
    func=get_weather,
    # handle_tool_error=True,
    name="get_weather",
    description="Get the weather of a city",
)

response = get_weather_tool.invoke("Beijing")
print(response)
```

> You can set `handle_tool_error` to `True`, set it a unified string value, or set it as a function. If it's set as a function, the function should take a `ToolException` as a parameter and return a `str` value.
>
> ä¼šä½¿ç”¨è¿™ä¸ªå¼‚å¸¸çš„è¾“å‡ºä½œä¸ºè¿”å›å€¼, æ‰‹å†Œé‡Œé¢è¯´è¿™ä¸ªå—åˆ°handle_tool_erroræ§åˆ¶, ä½†æ˜¯è¿™é‡Œå®é™…æµ‹è¯•åå‘ç°handle_tool_errorå‚æ•°æ²¡æœ‰å½±å“
>
> ä½¿ç”¨StructuredToolçš„æ—¶å€™å¯ä»¥ä¼ é€’æ›´å°‘çš„å‚æ•°

#### ä½¿ç”¨BaseToolå»ºç«‹ä¸€ä¸ªå­ç±»

BaseTool ä¼šè‡ªåŠ¨ä» _run æ–¹æ³•çš„ç­¾åæ¨æ–­æ¶æ„ã€‚

### å†…ç½®å·¥å…·

å·¥å…·å®é™…æ˜¯ä»£ç†, é“¾æˆ–è€…èŠå¤©æ¨¡å‹å’Œä¸–ç•Œäº¤äº’çš„æ–¹å¼, ä¸€èˆ¬æœ‰ä»¥ä¸‹çš„éƒ¨åˆ†

1. å·¥å…·çš„åå­—
2. å·¥å…·çš„åŠŸèƒ½æè¿°
3. å·¥å…·çš„è¾“å…¥çš„JSONæ ¼å¼
4. è¦è°ƒç”¨çš„å‡½æ•°
5. å·¥å…·çš„ç»“æœæ˜¯ä¸æ˜¯ç›´æ¥è¿”ç»™ç”¨æˆ·, æŠŠåç§°æè¿°ä¸Šä¸‹æ–‡å‘é€ç»™å¤§æ¨¡å‹, è®©æ¨¡å‹è¿›è¡Œå·¥å…·çš„è°ƒç”¨, ä¸€èˆ¬ç»è¿‡å¾®è°ƒçš„æ¨¡å‹æ•ˆæœæ¯”è¾ƒå¥½, æ²¡æœ‰å¾®è°ƒçš„æ¨¡å‹å¯èƒ½å¤±è´¥

#### ç»´åŸºç™¾ç§‘å·¥å…·

[WikipediaAPIWrapper â€” ğŸ¦œğŸ”— LangChain documentation](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.wikipedia.WikipediaAPIWrapper.html)

```python
from langchain_community.tools import WikipediaQueryRun
from langchain_community.utilities import WikipediaAPIWrapper

api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=100)
tool = WikipediaQueryRun(api_wrapper=api_wrapper)
print(tool.invoke({"query": "Langchain"}))

print(tool.name)
print(tool.description)
print(tool.args)
"""
Wiki-tools
A tool to query Wikipedia
{'query': {'description': 'æŸ¥è¯¢å…³é”®è¯, é•¿åº¦ä¸è¶…è¿‡10ä¸ªå­—', 'title': 'Query', 'type': 'string'}}
"""
```

å¯ä»¥åšä¸€ä¸‹å‚æ•°çš„å°è£…

```python
from langchain_community.tools import WikipediaQueryRun
from langchain_community.utilities import WikipediaAPIWrapper
from pydantic import BaseModel, Field

class WikiInputs(BaseModel):
    """ç»´åŸºç™¾ç§‘å·¥å…·è¾“å…¥"""
    query: str = Field(description="æŸ¥è¯¢å…³é”®è¯, é•¿åº¦ä¸è¶…è¿‡10ä¸ªå­—")


api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=100)
tool = WikipediaQueryRun(
    api_wrapper=api_wrapper,
    name="Wiki-tools",
    description="A tool to query Wikipedia",
    args_schema=WikiInputs,
    return_direct=True
    )

print(tool.run("Langchain"))

print(tool.name)
print(tool.description)
print(tool.args)
"""
Wiki-tools
A tool to query Wikipedia
{'query': {'description': 'æŸ¥è¯¢å…³é”®è¯, é•¿åº¦ä¸è¶…è¿‡10ä¸ªå­—', 'title': 'Query', 'type': 'string'}}
"""
```

#### SQLå¤„ç†

[LangChainä¸­æ–‡ç½‘](https://www.langchain.com.cn/docs/integrations/tools/sql_database/)

```python
from langchain_openai import ChatOpenAI
from langchain_community.agent_toolkits.sql.base import create_sql_agent
from langchain.agents.agent_types import AgentType
from langchain_community.utilities import SQLDatabase
from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit

import sqlite3
import requests
from sqlalchemy import create_engine
from sqlalchemy.pool import StaticPool

# è¿™æ®µä»£ç çš„ä½œç”¨æ˜¯ä»ä¸€ä¸ªè¿œç¨‹çš„SQLè„šæœ¬ä¸­åˆ›å»ºä¸€ä¸ªåœ¨å†…å­˜ä¸­çš„SQLiteæ•°æ®åº“ï¼Œå¹¶è¿”å›ä¸€ä¸ªæ•°æ®åº“å¼•æ“å¯¹è±¡ã€‚
def get_engine_for_chinook_db():
    """ä»SQLè„šæœ¬åˆ›å»ºå†…å­˜æ•°æ®åº“å¹¶è¿”å›å¼•æ“å¯¹è±¡ã€‚"""
    url = "https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql"
    response = requests.get(url)
    sql_script = response.text

    connection = sqlite3.connect(":memory:", check_same_thread=False)
    connection.executescript(sql_script)
    return create_engine(
        "sqlite://",
        creator=lambda: connection,
        poolclass=StaticPool,
        connect_args={"check_same_thread": False},
    )

engine = get_engine_for_chinook_db()

db = SQLDatabase(engine)
# db = SQLDatabase.from_uri("sqlite:///langchain.db")

# å»ºç«‹ä¸€ä¸ªSQLå·¥å…·ç®±å¯¹è±¡, ç”¨äºæ‰§è¡ŒSQLæŸ¥è¯¢, å¯ä»¥ç»‘å®šLLMæ¨¡å‹
toolkit = SQLDatabaseToolkit(db=db, llm=ChatOpenAI(temperature=0))
# print(toolkit.get_tools())

agent_exector = create_sql_agent(
    llm=ChatOpenAI(temperature=0),
    toolkit=toolkit,
    verbose=False,
    agent_type=AgentType.OPENAI_FUNCTIONS
)

result = agent_exector.invoke("Create a full_llm_cache table")
print(result)
```



## æŒ‡å®šæ ¼å¼è¾“å‡º

å¦‚æœè¾“å‡ºçš„æ ¼å¼æ˜¯ä¸€ä¸ªjson, xmlæˆ–è€…yaml, ä½†æ˜¯å¤§æ¨¡å‹çš„è¾“å‡ºå¯ä»¥èƒ½æ˜¯ä¸€ä¸ªæ··ä¹±çš„æ ¼å¼, æ‰€ä»¥ä½¿ç”¨çš„æ¨¡å‹å¿…é¡»è¶³å¤Ÿå¤§

å¯ä»¥ä½¿ç”¨JsonOutputParserè¿›è¡Œæç¤ºå¹¶ä¸”è§£æå¤§æ¨¡å‹çš„è¾“å‡º, ä¹Ÿå¯ä»¥ä½¿ç”¨PydanticOutputParser, Jsonçš„è¾“å‡ºæ˜¯å†…ç½®çš„, æ”¯æŒæµå¼è¿”å›

> Pydanticç¤ºä¾‹, [Pythonç¬”è®°ï¼šPydanticåº“ç®€ä»‹-CSDNåšå®¢](https://blog.csdn.net/codename_cys/article/details/107675748)
>
> ```python
> from pydantic import BaseModel
> 
> class Person(BaseModel):
>     name: str
> 
> p = Person(name="Tom")
> print(p.json()) # {"name": "Tom"}
> ```

```python
from langchain_core.output_parsers import JsonOutputParser
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field



model = ChatOpenAI(model="gpt-4o")
# æè¿°è¾“å‡ºçš„å…·ä½“æ ¼å¼
class Joke(BaseModel):
    setup: str = Field(description="è®¾ç½®ç¬‘è¯çš„é—®é¢˜")
    punchline: str = Field(description="è§£å†³ç¬‘è¯çš„ç­”æ¡ˆ")

joke_query = "å‘Šè¯‰æˆ‘ä¸€ä¸ªç¬‘è¯"

parser = JsonOutputParser(pydantic_object=Joke)
prompt = PromptTemplate(
    template="å›ç­”ç”¨æˆ·çš„é—®é¢˜, \n{format_instructions}\n{query}\n",
    input_variables=["query"],
    partial_variables={"format_instructions": parser.get_format_instructions()}
)

chain = prompt |  model | parser
message = chain.invoke({"query": joke_query})
print(message)
"""
{'setup': 'ä¸ºä»€ä¹ˆç¨‹åºå‘˜ä¸é¥¿ï¼Ÿ', 'punchline': 'å› ä¸ºä»–ä»¬æœ‰å¾ˆå¤šçš„ç¼“å­˜ã€‚'}
"""
```

å¯ä»¥çœ‹ä¸€ä¸‹è¾“å…¥çš„æ¨¡æ¿, å¦‚æœä¸ä½¿ç”¨è¿™ä¸ªæ¨¡ç‰ˆ, å®é™…çš„è¾“å‡ºä¼šæœ‰éšæœºæ€§

````python
print(parser.get_format_instructions())
"""
The output should be formatted as a JSON instance that conforms to the JSON schema below.
As an example, for the schema 
{
	"properties": {
		"foo": {
			"title": "Foo", 
			"description": "a list of strings", 
			"type": "array", 
			"items": {"type": "string"}
		}
	}, 
	"required": ["foo"]
}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{
	"properties": {
		"setup": {
			"title": "Setup",
             "description": "è®¾ç½®ç¬‘è¯çš„é—®é¢˜", 
             "type": "string"
         }, 
         "punchline": {
             "title": "Punchline", 
             "description": 
             "è§£å†³ç¬‘è¯çš„ç­”æ¡ˆ", 
             "type": "string"
          }
     }, 
     "required": ["setup", "punchline"]
}
```
"""
````

å¯ä»¥ä½¿ç”¨æµå¼è¾“å‡º

```python
for s in chain.stream({"query": joke_query}):
    print(s)
"""
{}
{'setup': ''}
{'setup': 'ä¸ºä»€ä¹ˆ'}
{'setup': 'ä¸ºä»€ä¹ˆè¶³çƒ'}
{'setup': 'ä¸ºä»€ä¹ˆè¶³çƒæ¯”èµ›'}
{'setup': 'ä¸ºä»€ä¹ˆè¶³çƒæ¯”èµ›æ€»'}
{'setup': 'ä¸ºä»€ä¹ˆè¶³çƒæ¯”èµ›æ€»æ˜¯åœ¨'}
{'setup': 'ä¸ºä»€ä¹ˆè¶³çƒæ¯”èµ›æ€»æ˜¯åœ¨ç¯'}
{'setup': 'ä¸ºä»€ä¹ˆè¶³çƒæ¯”èµ›æ€»æ˜¯åœ¨ç¯å…‰'}
{'setup': 'ä¸ºä»€ä¹ˆè¶³çƒæ¯”èµ›æ€»æ˜¯åœ¨ç¯å…‰ä¸‹'}
{'setup': 'ä¸ºä»€ä¹ˆè¶³çƒæ¯”èµ›æ€»æ˜¯åœ¨ç¯å…‰ä¸‹ï¼Ÿ'}
{'setup': 'ä¸ºä»€ä¹ˆè¶³çƒæ¯”èµ›æ€»æ˜¯åœ¨ç¯å…‰ä¸‹ï¼Ÿ', 'punchline': ''}
{'setup': 'ä¸ºä»€ä¹ˆè¶³çƒæ¯”èµ›æ€»æ˜¯åœ¨ç¯å…‰ä¸‹ï¼Ÿ', 'punchline': 'å› ä¸º'}
{'setup': 'ä¸ºä»€ä¹ˆè¶³çƒæ¯”èµ›æ€»æ˜¯åœ¨ç¯å…‰ä¸‹ï¼Ÿ', 'punchline': 'å› ä¸ºè§‚'}
{'setup': 'ä¸ºä»€ä¹ˆè¶³çƒæ¯”èµ›æ€»æ˜¯åœ¨ç¯å…‰ä¸‹ï¼Ÿ', 'punchline': 'å› ä¸ºè§‚ä¼—'}
{'setup': 'ä¸ºä»€ä¹ˆè¶³çƒæ¯”èµ›æ€»æ˜¯åœ¨ç¯å…‰ä¸‹ï¼Ÿ', 'punchline': 'å› ä¸ºè§‚ä¼—ä»¬'}
{'setup': 'ä¸ºä»€ä¹ˆè¶³çƒæ¯”èµ›æ€»æ˜¯åœ¨ç¯å…‰ä¸‹ï¼Ÿ', 'punchline': 'å› ä¸ºè§‚ä¼—ä»¬éœ€è¦'}
{'setup': 'ä¸ºä»€ä¹ˆè¶³çƒæ¯”èµ›æ€»æ˜¯åœ¨ç¯å…‰ä¸‹ï¼Ÿ', 'punchline': 'å› ä¸ºè§‚ä¼—ä»¬éœ€è¦çœ‹åˆ°'}
{'setup': 'ä¸ºä»€ä¹ˆè¶³çƒæ¯”èµ›æ€»æ˜¯åœ¨ç¯å…‰ä¸‹ï¼Ÿ', 'punchline': 'å› ä¸ºè§‚ä¼—ä»¬éœ€è¦çœ‹åˆ°è¿›'}
{'setup': 'ä¸ºä»€ä¹ˆè¶³çƒæ¯”èµ›æ€»æ˜¯åœ¨ç¯å…‰ä¸‹ï¼Ÿ', 'punchline': 'å› ä¸ºè§‚ä¼—ä»¬éœ€è¦çœ‹åˆ°è¿›çƒ'}
{'setup': 'ä¸ºä»€ä¹ˆè¶³çƒæ¯”èµ›æ€»æ˜¯åœ¨ç¯å…‰ä¸‹ï¼Ÿ', 'punchline': 'å› ä¸ºè§‚ä¼—ä»¬éœ€è¦çœ‹åˆ°è¿›çƒï¼'}
"""
```

### YAMLè¾“å‡º

è¿™ä¸ªæ ¼å¼çš„å®é™…ä½¿ç”¨å’ŒJSONçš„ä½¿ç”¨æ˜¯ä¸€æ ·çš„

### XMLæ ¼å¼

å¯ä»¥ä½¿ç”¨XMLOutputParserè¿›è¡Œè¾“å‡º, ä½¿ç”¨`tags=["", ""]`å¯ä»¥æŒ‡å®šå®é™…è¾“å‡ºçš„æ ‡ç­¾, ä¹‹åçš„è¾“å‡ºä¼šæŒ‰ç…§è¿™ä¸ªçš„å±‚çº§è¿›è¡Œè¾“å‡º, å¦‚æœä½¿ç”¨çš„æ˜¯æµå¼è¾“å‡º, å®é™…çš„ç»“æœè¿˜æ˜¯json

```python
from langchain_core.output_parsers import XMLOutputParser
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate

model = ChatOpenAI(model="gpt-3.5-turbo-1106")

joke_query = "ç”Ÿæˆå‘¨æ˜Ÿé©°çš„ç”µå½±ä½œå“, å®‰è£…æ—¶é—´é¡ºåºè¿›è¡Œæ’åº"

parser = XMLOutputParser(tags=["movie", "actor", "film", "genre","time"])
prompt = PromptTemplate(
    template="å›ç­”ç”¨æˆ·çš„é—®é¢˜, \n{format_instructions}\n{query}\n",
    input_variables=["query"],
    partial_variables={"format_instructions": parser.get_format_instructions()}
)

chain = prompt |  model | parser
message = chain.invoke({"query": joke_query})
print(message)
```

## è‡ªå®šä¹‰æ¨¡å‹

[Custom LLM | ğŸ¦œï¸ğŸ”— LangChain](https://python.langchain.com/v0.1/docs/modules/model_io/llms/custom_llm/)

éœ€è¦å®ç°ä¸¤ä¸ªå‡½æ•°, 

[ä½¿ç”¨LangChainè‡ªå®šä¹‰å¤§æ¨¡å‹ | å®Œç¾è°ƒç”¨ç¬¬ä¸‰æ–¹ API | å¦‚OneAPI/ç¡…åŸºæµåŠ¨-è…¾è®¯äº‘å¼€å‘è€…ç¤¾åŒº-è…¾è®¯äº‘](https://cloud.tencent.com/developer/article/2467458)