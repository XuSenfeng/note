# LangChain

## æ¶æ„

+ LangSmith: ç›‘æ§
+ LangServe: æœåŠ¡å™¨å¤„ç†
+ Templates: æ¨¡æ¿
+ LangChain: æ™ºèƒ½è°ƒç”¨, Agentså¼€å‘ä»¥åŠæ£€ç´¢ç­–ç•¥
+ LangChainCommunity: ç¤¾åŒº, æ”¯æŒå„ç§å¤§æ¨¡å‹, æç¤ºè¯ç­‰çš„å¤„ç†, è¾“å…¥å†…å®¹çš„æ ¼å¼åŒ–, å·¥å…·è°ƒç”¨çš„æ”¯æŒ
+ LangChainCore: LCELè¡¨è¾¾å¼è¯­è¨€çš„æ‰§è¡Œ

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/blog_migrate/faaf313af63c5029c7983192c1f43bd2.png)

ä¸»è¦çš„ç»„æˆ

+ LangChainåº“, æœ‰Pythonå’Œjava, é‡Œé¢æœ‰å„ç§ç»„ä»¶çš„æ¥å£ä»¥åŠè¿è¡Œçš„åŸºç¡€
+ LangChainæ¨¡æ¿: æä¾›çš„AIæ¨¡æ¿
+ LangServer: FastAPIæŠŠLangChainçš„é“¾(Chain)å‘å¸ƒä¸ºREST API
+ LangSmith: å¼€å‘å¹³å°äº‘æœåŠ¡

åº“

+ langchain-core: åŸºç¡€çš„æŠ½è±¡ä»¥åŠLangChainçš„è¡¨è¾¾è¯­è¨€
+ langchain-community: ç¬¬ä¸‰å‘é›†æˆ
+ langchain: é“¾ä»¥åŠä»£ç†å’Œagentæ£€ç´¢ç­–ç•¥

![image-20250218132339058](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502181323177.png)

> è¾“å…¥å’Œæ¨¡æ¿ç»“åˆä»¥åè¾“å…¥åˆ°LLMé‡Œé¢, è¿›è¡Œå¤„ç†è·å¾—è¾“å‡º, æŒ‰ç…§å®¢æˆ·éœ€æ±‚çš„æ ¼å¼è¿›è¡Œç»„è£…
>
> LLM: é—®ç­”æ¨¡å‹, è¾“å…¥ä¸€ä¸ªæ–‡æœ¬, è¿”å›ä¸€ä¸ªæ–‡æœ¬
>
> Chat Model: å¯¹è¯æ¨¡å‹, æ¥æ”¶ä¸€ç»„å¯¹è¯, è¿”å›å¯¹è¯æ¶ˆæ¯, å’ŒèŠå¤©ç±»ä¼¼

æ ¸å¿ƒæ¦‚å¿µ

+ LLMs

å°è£…çš„åŸºç¡€æ¨¡å‹, æ¥å—ä¸€ä¸ªæ–‡æœ¬çš„è¾“å…¥, è¿”å›ä¸€ä¸ªç»“æœ

+ ChatModels

èŠå¤©æ¨¡å‹, å’ŒLLMsä¸åŒ, å‘³è•¾å¯¹è¯è®¾è®¡, å¯ä»¥å¤„ç†é•¿ä¸‹æ–‡

+ æ¶ˆæ¯Message

èŠå¤©æ¨¡å‹çš„æ¶ˆæ¯å†…å®¹, æœ‰å¤šç§HumanMessage, AIMessage, SystemMessage, FunctionMessage, ToolMessageç­‰

+ æç¤ºprompts

æ ¼å¼åŒ–æç¤ºè¯

+ è¾“å‡ºè§£é‡Šå™¨

llmè¿”å›ä»¥åä½¿ç”¨ä¸“é—¨çš„è§£é‡Šå™¨è¿›è¡Œæ ¼å¼åŒ–ä¸ºjsonä¹‹ç±»çš„

+ Retrievers

ç§æœ‰æ•°æ®å¯¼å…¥å¤§æ¨¡å‹, æé«˜é—®é¢˜çš„è´¨é‡, LangChainå°è£…äº†æ£€ç´¢çš„æ¡†æ¶Retrieveers, å¯ä»¥åŠ å…¥æ–‡æ¡£, åˆ‡å‰², å­˜å‚¨æœç´¢

+ å‘é‡å­˜å‚¨Vector stores

ç§æœ‰æ•°æ®çš„è¯­ä¹‰ç›¸ä¼¼æ£€ç´¢, æ”¯æŒå¤šç§å‘é‡æ•°æ®åº“

+ Agent

æ™ºèƒ½ä½“, ä»¥LLMä¸ºå†³ç­–å¼•æ“, æ ¹æ®ç”¨æˆ·çš„è¾“å…¥, è‡ªåŠ¨è°ƒç”¨å¤–éƒ¨ç³»ç»Ÿå’Œè®¾å¤‡å®Œæˆä»»åŠ¡



## ç®€å•ç¤ºä¾‹

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
import os
os.environ["OPENAI_BASE_URL"] = "https://api.chatanywhere.tech"
llm = ChatOpenAI()

prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªå›¾ä¹¦ç®¡ç†å‘˜ã€‚"),
    ("user", "{input}"),
])
# é€šè¿‡LangChainçš„é“¾å¼è°ƒç”¨ç”Ÿæˆä¸€ä¸ªchainå¯¹è±¡
chain  = prompt | llm
# æ‰“å°ç”Ÿæˆçš„å¯¹è¯
result = chain.invoke({"input": "ä½ å¥½ï¼Œç»™æˆ‘æ¨èä¸€ä¸ªæ•…äº‹ä¹¦ã€‚"})
print(result)

"""
content='ä½ å¥½ï¼Œæˆ‘æ¨èç»™ä½ ä¸€æœ¬ç»å…¸çš„æ•…äº‹ä¹¦ã€Šå°ç‹å­ã€‹ã€‚è¿™æœ¬ä¹¦ç”±æ³•å›½ä½œå®¶å®‰æ‰˜ä¸‡Â·å¾·Â·åœ£-åŸƒå…‹çµ®ä½åˆ›ä½œï¼Œè®²è¿°äº†ä¸€ä¸ªå°ç‹å­åœ¨å®‡å®™ä¸­çš„å†’é™©æ•…äº‹ï¼Œé€šè¿‡ä»–å’Œå„ç§å¥‡ç‰¹è§’è‰²çš„ç›¸é‡ï¼Œæ­ç¤ºäº†äººç”Ÿã€å‹æƒ…ã€çˆ±æƒ…ã€è´£ä»»ç­‰æ·±åˆ»çš„ä¸»é¢˜ï¼Œæ˜¯ä¸€éƒ¨å¯Œæœ‰å“²ç†çš„ä½œå“ã€‚å¸Œæœ›ä½ ä¼šå–œæ¬¢è¿™æœ¬ä¹¦ï¼' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 149, 'prompt_tokens': 32, 'total_tokens': 181, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': 'fp_0165350fbb', 'finish_reason': 'stop', 'logprobs': None} id='run-b9a821df-af44-429d-906c-8f5ddb6a46e7-0' usage_metadata={'input_tokens': 32, 'output_tokens': 149, 'total_tokens': 181, 'input_token_details': {}, 'output_token_details': {}}
"""
# ä½¿ç”¨ä¸‹é¢çš„æ–¹å¼å¯ä»¥æ”¹å˜è¾“å‡ºçš„æ ¼å¼
from langchain_core.output_parsers import StrOutputParser
output_parser = StrOutputParser()
chain  = prompt | llm | output_parser
"""
ä½ å¥½ï¼å½“ç„¶å¯ä»¥ã€‚æˆ‘æ¨èä½ é˜…è¯»ã€Šå°ç‹å­ã€‹è¿™æœ¬ä¹¦ã€‚è¿™æ˜¯ä¸€æœ¬ç»å…¸çš„ç«¥è¯æ•…äº‹ï¼Œè®²è¿°äº†ä¸€ä½ç‹å­åœ¨ä¸åŒæ˜Ÿçƒä¸Šçš„å†’é™©æ•…äº‹ï¼Œä»¥åŠä»–ä¸ä¸€åªç‹ç‹¸å’Œä¸€æœµç«ç‘°çš„æ„Ÿäººæƒ…æ„Ÿçº è‘›ã€‚è¿™æœ¬ä¹¦æ·±åˆ»åœ°æ¢è®¨äº†å‹è°Šã€çˆ±æƒ…å’Œäººç”Ÿæ„ä¹‰çš„ä¸»é¢˜ï¼Œæ˜¯ä¸€éƒ¨æ„Ÿäººè‡³æ·±çš„æ–‡å­¦ä½œå“ã€‚å¸Œæœ›ä½ ä¼šå–œæ¬¢ï¼
"""
```

## æç¤ºè¯å·¥ç¨‹

å’ŒAIè¿›è¡Œæ²Ÿé€šçš„æ—¶å€™éœ€è¦ä½¿ç”¨æç¤ºè¯å’ŒAIè¿›è¡Œå¯¹è¯, åŒæ—¶AIå¯èƒ½å‡ºç°å¹»è§‰, å¼€å‘çš„æ—¶å€™ä¸å¯ä»¥ç›´æ¥è¿›è¡Œç¡¬ç¼–, ä¸åˆ©äºæç¤ºè¯ç®¡ç†, é€šè¿‡æç¤ºè¯çš„æ¨¡æ¿è¿›è¡Œç»´æŠ¤

å®é™…ç”¨æˆ·çš„è¾“å…¥åªæ˜¯æç¤ºè¯é‡Œé¢ä¸€ä¸ªå‚æ•°

+ å‘ç»™å¤§æ¨¡å‹çš„æŒ‡ä»¤
+ ä¸€ç»„é—®ç­”ç¤ºä¾‹
+ å‘ç»™æ¨¡å‹çš„é—®é¢˜

### æ„æˆ

+ PromptValue: è¡¨ç¤ºæ¨¡å‹è¾“å…¥çš„ç±»ã€‚

+ Prompt Templates: è´Ÿè´£æ„å»º PromptValue çš„ç±»ã€‚

+ ç¤ºä¾‹é€‰æ‹©å™¨ Example Selectors: åœ¨æç¤ºä¸­åŒ…å«ç¤ºä¾‹é€šå¸¸æ˜¯æœ‰ç”¨çš„ã€‚è¿™äº›ç¤ºä¾‹å¯ä»¥ç¡¬ç¼–ç ï¼Œä½†å¦‚æœå®ƒä»¬æ˜¯åŠ¨æ€é€‰æ‹©çš„ï¼Œåˆ™é€šå¸¸æ›´æœ‰ç”¨ã€‚

+ è¾“å‡ºè§£æå™¨ Output Parsers: è¯­è¨€æ¨¡å‹ï¼ˆå’ŒèŠå¤©æ¨¡å‹ï¼‰è¾“å‡ºæ–‡æœ¬ã€‚ä½†æ˜¯è®¸å¤šæ—¶å€™ï¼Œæ‚¨å¯èƒ½æƒ³è·å¾—æ¯”ä»…æ–‡æœ¬æ›´æœ‰ç»“æ„åŒ–çš„ä¿¡æ¯ã€‚è¿™å°±æ˜¯è¾“å‡ºè§£æå™¨å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚è¾“å‡ºè§£æå™¨è´Ÿè´£ï¼ˆ1ï¼‰æŒ‡ç¤ºæ¨¡å‹å¦‚ä½•æ ¼å¼åŒ–è¾“å‡ºï¼Œï¼ˆ2ï¼‰å°†è¾“å‡ºè§£æä¸ºæ‰€éœ€æ ¼å¼ï¼ˆåŒ…æ‹¬å¿…è¦æ—¶è¿›è¡Œé‡è¯•ï¼‰ã€‚

æœ‰ä¸¤ä¸ªæ–¹æ³•ä¸€å®šæœ‰å®ç°

`get_format_instructions() -> str`ï¼šä¸€ä¸ªè¿”å›åŒ…å«è¯­è¨€æ¨¡å‹è¾“å‡ºæ ¼å¼åŒ–æŒ‡ä»¤çš„å­—ç¬¦ä¸²çš„æ–¹æ³•ã€‚ parse(str) -> Anyï¼šä¸€ä¸ªæ¥å—å­—ç¬¦ä¸²ï¼ˆå‡è®¾ä¸ºè¯­è¨€æ¨¡å‹çš„å“åº”ï¼‰å¹¶å°†å…¶è§£æä¸ºæŸç§ç»“æ„çš„æ–¹æ³•ã€‚ è¿˜æœ‰ä¸€ä¸ªå¯é€‰çš„æ–¹æ³•ï¼š

`parse_with_prompt(str) -> Any`ï¼šä¸€ä¸ªæ¥å—å­—ç¬¦ä¸²ï¼ˆå‡è®¾ä¸ºè¯­è¨€æ¨¡å‹çš„å“åº”ï¼‰å’Œæç¤ºï¼ˆå‡è®¾ä¸ºç”Ÿæˆæ­¤å“åº”çš„æç¤ºï¼‰çš„æ–¹æ³•ï¼Œå¹¶å°†å…¶è§£æä¸ºæŸç§ç»“æ„ã€‚åœ¨è¾“å‡ºè§£æå™¨å¸Œæœ›ä»¥æŸç§æ–¹å¼é‡è¯•æˆ–ä¿®å¤è¾“å‡ºï¼Œå¹¶ä¸”éœ€è¦æ¥è‡ªæç¤ºçš„ä¿¡æ¯æ—¶ï¼Œæä¾›æç¤ºéå¸¸æœ‰ç”¨ã€‚

### å­—ç¬¦ä¸²æ ¼å¼çš„æ¨¡æ¿

```python
from langchain_core.prompts import ChatPromptTemplate
# è¿™ç§çš„ä¼šç”Ÿæˆæ¶ˆæ¯, å¯ä»¥æœ‰ä¸Šä¸‹æ–‡
chat_template = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªçŒ«å¨˜ï¼Œä½ çš„åå­—æ˜¯{name}ã€‚"),
    ("system", "ä½ æ­£åœ¨ä¸€å®¶å® ç‰©åº—é‡Œç­‰å¾…è¢«é¢†å…»ã€‚"),
    ("human", "ä½ å¥½ï¼Œå°çŒ«ã€‚"),
    ("ai", "ä½ å¥½, å–µ~~"),
    ("human", "{input}")
])

message = chat_template.format_messages(name = "å°èŠ±", input = "ä½ çš„åå­—æ˜¯ä»€ä¹ˆ?")
print(message)
"""
[
SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªçŒ«å¨˜ï¼Œä½ çš„åå­—æ˜¯å°èŠ±ã€‚', additional_kwargs={}, response_metadata={}), 
SystemMessage(content='ä½ æ­£åœ¨ä¸€å®¶å® ç‰©åº—é‡Œç­‰å¾…è¢«é¢†å…»ã€‚', additional_kwargs={}, response_metadata={}), 
HumanMessage(content='ä½ å¥½ï¼Œå°çŒ«ã€‚', additional_kwargs={}, response_metadata={}), AIMessage(content='ä½ å¥½, å–µ~~', additional_kwargs={}, response_metadata={}), HumanMessage(content='ä½ çš„åå­—æ˜¯ä»€ä¹ˆ?', additional_kwargs={}, response_metadata={})
]
"""
# ç”Ÿæˆä¸€ä¸ªå­—ç¬¦ä¸²
from langchain_core.prompts import PromptTemplate
prompt_template = PromptTemplate.from_template(
    "ä½ çš„åå­—æ˜¯{name}, ä½ æ­£åœ¨ä¸€å®¶å® ç‰©åº—é‡Œç­‰å¾…è¢«é¢†å…»ã€‚ä½ å¥½ï¼Œå°çŒ«ã€‚{input}"
)
message = prompt_template.format(name = "å°èŠ±", input = "ä½ çš„åå­—æ˜¯ä»€ä¹ˆ?")
print(message)
"""
ä½ çš„åå­—æ˜¯å°èŠ±, ä½ æ­£åœ¨ä¸€å®¶å® ç‰©åº—é‡Œç­‰å¾…è¢«é¢†å…»ã€‚ä½ å¥½ï¼Œå°çŒ«ã€‚ä½ çš„åå­—æ˜¯ä»€ä¹ˆ?
"""
from langchain_core.messages import SystemMessage
from langchain.prompts import HumanMessagePromptTemplate

chat_template = ChatPromptTemplate.from_messages([
    SystemMessage(
        content = ("ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚")
    ),
    HumanMessagePromptTemplate.from_template("{text}")
])

message = chat_template.format_messages(text = "ä½ å¥½ï¼Œæˆ‘æƒ³äº†è§£ä¸€ä¸‹ä½ çš„äº§å“ã€‚")
print(message)
"""
[
SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚', additional_kwargs={}, response_metadata={}), 
HumanMessage(content='ä½ å¥½ï¼Œæˆ‘æƒ³äº†è§£ä¸€ä¸‹ä½ çš„äº§å“ã€‚', additional_kwargs={}, response_metadata={})
]
"""

```

æ¶ˆæ¯æç¤ºè¯é‡Œé¢æœ‰ä¸‰ç§è§’è‰²

+ åŠ©æ‰‹Assistant: AIçš„å›ç­”
+ äººç±»User: ä½ å‘é€çš„æ¶ˆæ¯
+ ç³»ç»ŸSystem: è¿›è¡ŒAIèº«ä»½çš„æè¿°

### MessagesPlaceholder

ç‰¹å®šçš„ä½ç½®æ·»åŠ æ¶ˆæ¯åˆ—è¡¨, ä¸Šé¢å¤„ç†çš„æ˜¯ä¸¤æ¡æ¶ˆæ¯, æ¯ä¸€ä¸ªæ¶ˆæ¯éƒ½æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸², å¦‚æœè¾“å…¥çš„æ˜¯ä¸€ä¸ªæ¶ˆæ¯åˆ—è¡¨å¯ä»¥ä½¿ç”¨è¿™ä¸ª

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage
prompt_template = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹"),
    MessagesPlaceholder("msgs")
])

prompt_template.invoke({"msgs": [HumanMessage(content="ä½ å¥½")]})
"""
ChatPromptValue(
messages=[
SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹', additional_kwargs={}, response_metadata={}), 
HumanMessage(content='ä½ å¥½', additional_kwargs={}, response_metadata={})
]
)
"""
```

> å¯ä»¥çœ‹ä¸ºä¸€ä¸ªå ä½ç¬¦, è¿™é‡Œå¯ä»¥ç©¿è¿›å»ä¸€ç³»åˆ—çš„Message

### Few-shot prompt template

è¿½åŠ æç¤ºè¯ç¤ºä¾‹

å¸®åŠ©äº¤äº’æ ·æœ¬æ›´å¥½çš„äº†è§£ç”¨æˆ·çš„æ„å›¾, ä»è€Œæ›´å¥½çš„å›ç­”é—®é¢˜ä»¥åŠå¤„ç†ä»»åŠ¡, ä½¿ç”¨å°‘é‡çš„ç¤ºä¾‹æŒ‡å¯¼æ¨¡å‹è¾“å…¥, å¯ä»¥ä½¿ç”¨ç¤ºä¾‹é›†è¿›è¡Œ

```python
from langchain.prompts.few_shot import FewShotPromptTemplate
from langchain.prompts.prompt import PromptTemplate

examples = [
    {
        "question": "è°çš„å¯¿å‘½æ›´é•¿, æ¯›æ³½ä¸œè¿˜æ˜¯é‚“å°å¹³?",
        "answer": 
                    """
                    è¿™é‡Œè¦å›ç­”çš„æ˜¯è°çš„å¯¿å‘½æ›´é•¿, æ¯›æ³½ä¸œè¿˜æ˜¯é‚“å°å¹³? æ¯›æ³½ä¸œçš„å¯¿å‘½æ˜¯1893å¹´12æœˆ26æ—¥å‡ºç”Ÿ, 1976å¹´9æœˆ9æ—¥å»ä¸–, äº«å¹´82å². 
                    é‚“å°å¹³çš„å¯¿å‘½æ˜¯1904å¹´8æœˆ22æ—¥å‡ºç”Ÿ, 1997å¹´2æœˆ19æ—¥å»ä¸–, äº«å¹´92å². 
                    å› æ­¤, é‚“å°å¹³çš„å¯¿å‘½æ›´é•¿.
                    æ‰€ä»¥æœ€ç»ˆçš„ç­”æ¡ˆæ˜¯é‚“å°å¹³çš„å¯¿å‘½æ›´é•¿.        
                    """
    },
    {
        "question": "å“ªå’ä¹‹é­”ç«¥é™ä¸–çš„å¯¼æ¼”æ˜¯å“ªå›½äºº?",
        "answer": 
                    """
                    è¿™é‡Œè¦å›ç­”çš„æ˜¯è·Ÿè¿›é—®é¢˜å—? æ˜¯çš„
                    è·Ÿè¿›: å“ªå’ä¹‹é­”ç«¥é™ä¸–çš„å¯¼æ¼”æ˜¯è°?
                    å“ªå’ä¹‹é­”ç«¥é™ä¸–çš„å¯¼æ¼”æ˜¯é¥ºå­.
                    è·Ÿè¿›: é¥ºå­æ˜¯å“ªå›½äºº?
                    é¥ºå­æ˜¯ä¸­å›½äºº.
                    æ‰€ä»¥æœ€ç»ˆçš„ç­”æ¡ˆæ˜¯é¥ºå­æ˜¯ä¸­å›½äºº.
                    """
    }
]
# ä½¿ç”¨è¿™ä¸ªæ¨¡æ¿æ¥ç”Ÿæˆé—®é¢˜å’Œç­”æ¡ˆ
example_prompt = PromptTemplate(input_variables=["question", "answer"], template="é—®é¢˜: {question}\\n{answer}")
"""
StringPromptValue
"""
prompt = FewShotPromptTemplate(
    example_prompt=example_prompt,
    examples=examples,
    suffix="é—®é¢˜:{input}", # åç¼€
    input_variables=["input"]
)
print(prompt.format(input="å²å‡¯æ­Œçˆ¸çˆ¸æ˜¯è°?"))
"""
é—®é¢˜: è°çš„å¯¿å‘½æ›´é•¿, æ¯›æ³½ä¸œè¿˜æ˜¯é‚“å°å¹³?\n
                    è¿™é‡Œè¦å›ç­”çš„æ˜¯è°çš„å¯¿å‘½æ›´é•¿, æ¯›æ³½ä¸œè¿˜æ˜¯é‚“å°å¹³? æ¯›æ³½ä¸œçš„å¯¿å‘½æ˜¯1893å¹´12æœˆ26æ—¥å‡ºç”Ÿ, 1976å¹´9æœˆ9æ—¥å»ä¸–, äº«å¹´82å². 
                    é‚“å°å¹³çš„å¯¿å‘½æ˜¯1904å¹´8æœˆ22æ—¥å‡ºç”Ÿ, 1997å¹´2æœˆ19æ—¥å»ä¸–, äº«å¹´92å². 
                    å› æ­¤, é‚“å°å¹³çš„å¯¿å‘½æ›´é•¿.
                    æ‰€ä»¥æœ€ç»ˆçš„ç­”æ¡ˆæ˜¯é‚“å°å¹³çš„å¯¿å‘½æ›´é•¿.        
                    

é—®é¢˜: å“ªå’ä¹‹é­”ç«¥é™ä¸–çš„å¯¼æ¼”æ˜¯å“ªå›½äºº?\n
                    è¿™é‡Œè¦å›ç­”çš„æ˜¯è·Ÿè¿›é—®é¢˜å—? æ˜¯çš„
                    è·Ÿè¿›: å“ªå’ä¹‹é­”ç«¥é™ä¸–çš„å¯¼æ¼”æ˜¯è°?
                    å“ªå’ä¹‹é­”ç«¥é™ä¸–çš„å¯¼æ¼”æ˜¯é¥ºå­.
                    è·Ÿè¿›: é¥ºå­æ˜¯å“ªå›½äºº?
                    é¥ºå­æ˜¯ä¸­å›½äºº.
                    æ‰€ä»¥æœ€ç»ˆçš„ç­”æ¡ˆæ˜¯é¥ºå­æ˜¯ä¸­å›½äºº.
                    

é—®é¢˜:å²å‡¯æ­Œçˆ¸çˆ¸æ˜¯è°?
"""
```

> è¿™é‡Œçš„example_promptç›´æ¥ä½¿ç”¨examplesçš„æ—¶å€™éœ€è¦ä½¿ç”¨**è§£å¼•ç”¨
>
> ```python
> example_prompt = PromptTemplate(input_variables=["question", "answer"], template="é—®é¢˜: {question}\\n{answer}")
> print(example_prompt.format(**examples[0]))
> ```

### ç¤ºä¾‹é€‰æ‹©å™¨

å®é™…ä½¿ç”¨çš„æ—¶å€™ä¸å¯ä»¥å¸¦å¤ªé•¿çš„ç¤ºä¾‹, æ‰€ä»¥å¯ä»¥ä½¿ç”¨ç¤ºä¾‹é€‰æ‹©å™¨

ExampleSelector, åœ¨å®é™…ä½¿ç”¨çš„æ—¶å€™æŠŠé—®é¢˜å’Œç¤ºä¾‹è¿›è¡Œä¸€ä¸ªåŒ¹é…, ä½¿ç”¨å‘é‡æ•°æ®åº“è¿›è¡Œæœç´¢

```python
from langchain.prompts.example_selector import SemanticSimilarityExampleSelector # è¯­ä¹‰ç›¸ä¼¼åº¦é€‰æ‹©å™¨
from langchain_community.vectorstores import Chroma # å¼€æºçš„å‘é‡åº“
from langchain_openai import OpenAIEmbeddings # OpenAIçš„Embeddings

example_selector = SemanticSimilarityExampleSelector.from_examples(
    examples=examples,
    vectorstore_cls=Chroma,
    embeddings=OpenAIEmbeddings(),
    k = 1,
)

question = "çˆ±å› æ–¯å¦å’Œéœé‡‘è°æ´»å¾—é•¿?"
selected_examples = example_selector.select_examples({"question":question})
print(selected_examples)
for example in selected_examples:
    print(example_prompt.format(**example))
    
"""
[{'answer': '\n è¿™é‡Œè¦å›ç­”çš„æ˜¯è°çš„å¯¿å‘½æ›´é•¿, æ¯›æ³½ä¸œè¿˜æ˜¯é‚“å°å¹³? æ¯›æ³½ä¸œçš„å¯¿å‘½æ˜¯1893å¹´12æœˆ26æ—¥å‡ºç”Ÿ, 1976å¹´9æœˆ9æ—¥å»ä¸–, äº«å¹´82å². \né‚“å°å¹³çš„å¯¿å‘½æ˜¯1904å¹´8æœˆ22æ—¥å‡ºç”Ÿ, 1997å¹´2æœˆ19æ—¥å»ä¸–, äº«å¹´92å². \n     å› æ­¤, é‚“å°å¹³çš„å¯¿å‘½æ›´é•¿.\næ‰€ä»¥æœ€ç»ˆçš„ç­”æ¡ˆæ˜¯é‚“å°å¹³çš„å¯¿å‘½æ›´é•¿.  \n ', 'question': 'è°çš„å¯¿å‘½æ›´é•¿, æ¯›æ³½ä¸œè¿˜æ˜¯é‚“å°å¹³?'}]
é—®é¢˜: è°çš„å¯¿å‘½æ›´é•¿, æ¯›æ³½ä¸œè¿˜æ˜¯é‚“å°å¹³?\n
                    è¿™é‡Œè¦å›ç­”çš„æ˜¯è°çš„å¯¿å‘½æ›´é•¿, æ¯›æ³½ä¸œè¿˜æ˜¯é‚“å°å¹³? æ¯›æ³½ä¸œçš„å¯¿å‘½æ˜¯1893å¹´12æœˆ26æ—¥å‡ºç”Ÿ, 1976å¹´9æœˆ9æ—¥å»ä¸–, äº«å¹´82å². 
                    é‚“å°å¹³çš„å¯¿å‘½æ˜¯1904å¹´8æœˆ22æ—¥å‡ºç”Ÿ, 1997å¹´2æœˆ19æ—¥å»ä¸–, äº«å¹´92å². 
                    å› æ­¤, é‚“å°å¹³çš„å¯¿å‘½æ›´é•¿.
                    æ‰€ä»¥æœ€ç»ˆçš„ç­”æ¡ˆæ˜¯é‚“å°å¹³çš„å¯¿å‘½æ›´é•¿.
"""
```

ç›´æ¥è¿”å›çš„æ•°æ®è¿˜æ˜¯ä¸€ä¸ªå­—å…¸çš„

## å·¥ä½œæµ

é“¾ï¼ˆ Chains ï¼‰æ˜¯ä¸€ä¸ªéå¸¸é€šç”¨çš„æ¦‚å¿µï¼Œå®ƒæŒ‡çš„æ˜¯å°†ä¸€ç³»åˆ—æ¨¡å—åŒ–ç»„ä»¶ï¼ˆæˆ–å…¶ä»–é“¾ï¼‰ä»¥ç‰¹å®šæ–¹å¼ç»„åˆèµ·æ¥ï¼Œä»¥å®ç°å…±åŒçš„ç”¨ä¾‹ã€‚

æœ€å¸¸ç”¨çš„é“¾ç±»å‹æ˜¯LLMChainï¼ˆLLMé“¾ï¼‰ï¼Œå®ƒç»“åˆäº†PromptTemplateï¼ˆæç¤ºæ¨¡æ¿ï¼‰ã€Modelï¼ˆæ¨¡å‹ï¼‰å’ŒGuardrailsï¼ˆå®ˆå«ï¼‰æ¥æ¥æ”¶ç”¨æˆ·è¾“å…¥ï¼Œè¿›è¡Œç›¸åº”çš„æ ¼å¼åŒ–ï¼Œå°†å…¶ä¼ é€’ç»™æ¨¡å‹å¹¶è·å–å“åº”ï¼Œç„¶åéªŒè¯å’Œä¿®æ­£ï¼ˆå¦‚æœéœ€è¦ï¼‰æ¨¡å‹çš„è¾“å‡º

å¯ä»¥ä½¿ç”¨åŒæ­¥å’Œå¼‚æ­¥çš„API, å¯ä»¥åœ¨ä»»æ„ä½ç½®è¿›è¡Œé‡è¯•ä»¥åŠå›é€€, è®¿é—®ä¸­é—´çš„ç»“æœ

### Runable interface

ä¸€ä¸ªé€šç”¨çš„æ¥å£, æ‰€æœ‰çš„éƒ¨ä»¶éƒ½æ”¯æŒè¿™ä¸ªæ¥å£, åŒ…æ‹¬ä»¥ä¸‹çš„å†…å®¹:

+ stream: è¿”å›å“åº”çš„æ•°æ®å—, æ•°æ®å—ä¼ è¾“, è¾“å‡ºçš„æ—¶å€™æ˜¯ä¸€ä¸ªå­—ä¸€ä¸ªå­—çš„, å‡å°‘ç­‰å¾…çš„æ—¶é—´
+ invoke: å¯¹è¾“å…¥çš„è°ƒç”¨é“¾, åŒæ­¥è°ƒç”¨, æ²¡æœ‰ç»“æœä¸€ç›´ç­‰å¾…
+ batch: è¾“å…¥åˆ—è¡¨çš„è°ƒç”¨é“¾, æ‰¹é‡è°ƒç”¨, åŒæ—¶è°ƒç”¨å¤šæ¬¡

è¿˜å¯ä»¥ä½¿ç”¨å¼‚æ­¥çš„æ–¹æ³•, å’Œasyncio+awaitä¸€èµ·ä½¿ç”¨

+ astream: å¼‚æ­¥è¿”å›ç›¸åº”çš„æ•°æ®é“¾
+ ainvoke: å¼‚æ­¥å¯¹è¾“å…¥çš„è°ƒç”¨é“¾
+ abatch: å¼‚æ­¥å¯¹è¾“å…¥åˆ—è¡¨çš„è°ƒç”¨
+ astream_log: å¼‚æ­¥è¿”å›ä¸­é—´æ­¥éª¤, ä»¥åŠæœ€ç»ˆçš„ç›¸åº”
+ astream_events: å¤„ç†æ—¶é—´

é€šå¸¸æƒ…å†µä¸‹ï¼ŒLangChainä¸­çš„é“¾åŒ…å«: å¤§æ¨¡å‹(LLMs)ã€æç¤ºè¯æ¨¡ç‰ˆ(Prompt Template)ã€å·¥å…·(Tools)å’Œè¾“å‡ºè§£æå™¨(Output Parsers)ã€‚è¿™å‡ éƒ¨åˆ†éƒ½ç»§æ‰¿å¹¶å®ç°äº†`Runnable`æ¥å£ï¼Œæ‰€ä»¥ä»–ä»¬éƒ½æ˜¯`Runnable`çš„å®ä¾‹ã€‚

åœ¨LangChainä¸­ï¼Œå¯ä»¥ä½¿ç”¨LangChain Expression Language(LCEL)å°†å¤šä¸ª`Runnable`ç»„åˆæˆé“¾ã€‚å…¶å…·ä½“çš„`Runnable`ç»„åˆæ–¹å¼ä¸»è¦æœ‰ä¸¤ç§ï¼š

+ `RunnableSequence`:æŒ‰é¡ºåºè°ƒç”¨ä¸€ç³»åˆ—å¯è¿è¡Œæ–‡ä»¶ï¼Œå…¶ä¸­ä¸€ä¸ª`Runnable`çš„è¾“å‡ºä½œä¸ºä¸‹ä¸€ä¸ªçš„è¾“å…¥ã€‚ä¸€èˆ¬é€šè¿‡ä½¿ç”¨`|`è¿ç®—ç¬¦æˆ–å¯è¿è¡Œé¡¹åˆ—è¡¨æ¥æ„é€ ã€‚

```python
from langchain_core.runnables import RunnableSequence
#æ–¹æ³•1
chain=prompt|llm
#æ–¹æ³•2
chain = RunnableSequence([prompt,llm])
```

+ `RunnableParallell`:åŒæ—¶è°ƒç”¨å¤š`Runnable`ã€‚åœ¨åºåˆ—ä¸­ä½¿ç”¨dictæˆ–é€šè¿‡å°†dictä¼ é€’ç»™`RunnableParallel`æ¥æ„é€ å®ƒã€‚

```python
from langchain_core.runnables import RunnableLambda

def add_one(x: int) -> int:
    return x + 1

def mul_two(x: int) -> int:
    return x * 2

def mul_three(x: int) -> int:
    return x * 3

runnable_1 = RunnableLambda(add_one)
runnable_2 = RunnableLambda(mul_two)
runnable_3 = RunnableLambda(mul_three)
#æ–¹æ³•1
sequence = runnable_1 | { 
    "mul_two": runnable_2,
    "mul_three": runnable_3,
}
#æ–¹æ³•2
sequence = runnable_1 | RunnableParallel(
     {"mul_two": runnable_2, "mul_three": runnable_3})
```

#### å¸¸ç”¨ç±»

RunnableLambdaå’ŒRunnableGeneratorè¿™ä¸¤ä¸ªç±»é€šå¸¸ç”¨æ¥è‡ªå®šä¹‰Runnableã€‚è¿™ä¸¤è€…çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼š

+ `RunnableLambda`:æ˜¯å°†Pythonä¸­çš„å¯è°ƒç”¨å¯¹è±¡åŒ…è£…æˆRunnable, è¿™ç§ç±»å‹çš„Runnableæ— æ³•å¤„ç†æµå¼æ•°æ®ã€‚
+ `RunnableGenerator`:å°†Pythonä¸­çš„ç”Ÿæˆå™¨åŒ…è£…æˆRunnable,å¯ä»¥å¤„ç†æµå¼æ•°æ®ã€‚

```python
from langchain_core.runnables import RunnableLambda,RunnableGenerator
from dotenv import load_dotenv,find_dotenv
_=load_dotenv(find_dotenv())
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import CommaSeparatedListOutputParser

#ç®€å•çš„ä¾‹å­ï¼Œè¾“å‡º1åˆ°10ä¹‹é—´çš„æ‰€æœ‰æ•´æ•°
prompt=PromptTemplate.from_template("è¾“å‡º1åˆ°{max_value}ä¹‹é—´çš„æ‰€æœ‰æ•´æ•°ã€‚æ¯ä¸ªæ•°å­—ä¹‹é—´ç”¨é€—å·,åˆ†éš”, æ— ç»“å°¾ç¬¦ã€‚")
def add_one(x):
    return ' '.join([str((int(i)+1)) for i in x])
runnable=RunnableLambda(add_one) # ä¸€ä¸ªç®€å•çš„lambdaå‡½æ•°ï¼Œå°†è¾“å…¥çš„æ•°å­—åŠ 1
#éæµå¼å¤„ç†
llm=ChatOpenAI()
# CommaSeparatedListOutputParser() ç”¨äºè§£æé€—å·åˆ†éš”çš„æ•°å­—, è¾“å‡ºä¸ºä¸€ä¸ªåˆ—è¡¨, æ¯ä¸€é¡¹ä¸ºä¸€ä¸ªæ•°å­—
chain=prompt | llm | CommaSeparatedListOutputParser() | runnable
print(chain.invoke({"max_value":"10"}))

#æµå¼å¤„ç†
stream_llm=ChatOpenAI(model_kwargs={"stream":True})
def stream_add_one(x):
    print(x)
    for i in x:
        if i.content.isdigit():
            yield str((int(i.content)+1))
stream_chain=prompt | stream_llm | RunnableGenerator(stream_add_one)
for chunk in stream_chain.stream({"max_value":"10"}):
    print(chunk)
```

#### RunableBinding

RunnableBindingå¯ä»¥çœ‹ä½œæ˜¯Runnableçš„è£…é¥°å™¨ï¼Œå®ƒå…è®¸åœ¨ä¸æ”¹å˜åŸå‡½æ•°ä»£ç çš„å‰æä¸‹ï¼ŒåŠ¨æ€åœ°æ·»åŠ æˆ–ä¿®æ”¹å‡½æ•°çš„åŠŸèƒ½ã€‚Runnableä¸­å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹æ³•åˆ›å»ºRunnableBindingç±»æˆ–å…¶å­ç±»ã€‚å…·ä½“å¦‚ä¸‹ï¼š

+ bind: ç»‘å®šè¿è¡Œå‚æ•°kwargsã€‚æ¯”å¦‚ï¼Œå¯ä»¥å°†å¸¸ç”¨çš„æ–¹æ³•(æ¯”å¦‚invoke, batch, transform, streamåŠå…¶ä»–)ä¸­çš„å¯é€‰å‚æ•°åˆ°Runnableä¸Šã€‚
+ with_configï¼šç»‘å®šconfigã€‚
+ with_listenersï¼šç»‘å®šç”Ÿå‘½å‘¨æœŸç›‘å¬å™¨ã€‚Runnableå¯ä»¥è®¾ç½®ä¸‰ç±»ç›‘å¬å™¨ï¼šon_startã€on_endå’Œon_errorã€‚é€šè¿‡ç›‘è§†å™¨å¯ä»¥è·å–ç›¸å…³è¿è¡Œä¿¡æ¯ï¼ŒåŒ…æ‹¬å…¶idã€ç±»å‹ã€è¾“å…¥ã€è¾“å‡ºã€é”™è¯¯ã€start_timeã€end_timeä»¥åŠå…¶ä»–æ ‡è®°å’Œå…ƒæ•°æ®ã€‚å…·ä½“ä¸¾ä¾‹å¦‚ä¸‹ï¼š

```python
from langchain_core.runnables import RunnableLambda
from langchain_core.tracers.schemas import Run
import time

def add_one(a):
    try:
        return a+1
    except Exception as e:
        print("Error: æ•°æ®ç±»å‹é”™è¯¯ï¼Œæ— æ³•è¿›è¡ŒåŠ æ³•è¿ç®—",e)

def fn_start(run_obj: Run):
    print("Runnableå¼€å§‹è¿è¡Œæ—¶é—´:",run_obj.start_time)

def fn_end(run_obj: Run):
    print("Runnableç»“æŸè¿è¡Œæ—¶é—´:",run_obj.end_time)
    
def fn_error(run_obj: Run):
    print(run_obj.error)

runnable=RunnableLambda(add_one).with_listeners(on_start=fn_start,
                                                on_end=fn_end,
                                                on_error=fn_error)
runnable.invoke(2)
runnable.invoke("2")

"""
Runnableå¼€å§‹è¿è¡Œæ—¶é—´: 2025-02-21 10:42:16.286062+00:00
Runnableç»“æŸè¿è¡Œæ—¶é—´: 2025-02-21 10:42:16.287158+00:00
Runnableå¼€å§‹è¿è¡Œæ—¶é—´: 2025-02-21 10:42:16.287158+00:00
Error: æ•°æ®ç±»å‹é”™è¯¯ï¼Œæ— æ³•è¿›è¡ŒåŠ æ³•è¿ç®— can only concatenate str (not "int") to str
Runnableç»“æŸè¿è¡Œæ—¶é—´: 2025-02-21 10:42:16.287158+00:00
"""
```

- with_typesï¼šè¦†ç›–è¾“å…¥å’Œè¾“å‡ºç±»å‹ã€‚
- with_fallbacksï¼šç»‘å®šå›é€€ç­–ç•¥ã€‚
- with_retryï¼šç»‘å®šé‡è¯•ç­–ç•¥ã€‚

#### RunnableEach

`RunnableEach`æ˜¯ä¸€ä¸ªç”¨äºæ‰¹é‡å¤„ç†ä»»åŠ¡çš„ç»„ä»¶ï¼Œå®ƒå¯ä»¥å¯¹ä¸€ç»„è¾“å…¥æ•°æ®åˆ†åˆ«åº”ç”¨æŒ‡å®šçš„`Runnable`ç»„ä»¶

```python
from langchain_core.runnables import RunnableLambda
from langchain_core.runnables.base import RunnableEach

def add_one(a):
    try:
        return a+1
    except Exception as e:
        print("Error: æ•°æ®ç±»å‹é”™è¯¯ï¼Œæ— æ³•è¿›è¡ŒåŠ æ³•è¿ç®—",e)
runnable=RunnableEach(bound=RunnableLambda(add_one))
print(runnable.invoke([1,2,4]))
```

#### RunnableBranch

```python
from langchain_core.runnables import RunnableBranch,RunnableLambda

def add_int_one(a):
    try:
        return a+1
    except Exception as e:
        print("Error:",e)
def add_str_one(a):
    try:
        return chr(ord(a)+1)
    except Exception as e:
        print("Error:",e)

runnable=RunnableBranch(
    (lambda x:type(x)==str,RunnableLambda(add_str_one)),
    (lambda x:type(x)==int,RunnableLambda(add_int_one)),
    lambda x:x,
)
print(runnable.invoke('a'))
print(runnable.invoke(1))
```



### è¾“å…¥è¾“å‡ºç±»å‹

![image-20250220135854967](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502201358867.png)

> æ‰€æœ‰çš„è¾“å…¥å’Œè¾“å‡ºéƒ½æ˜¯å…¬å¼€çš„æ‰€ä»¥å¯ä»¥ä½¿ç”¨Pydanticæ¨¡å‹è¿›è¡Œæ£€æŸ¥, input_schema, output_schema

### stream

æ‰€æœ‰çš„runableå¯¹è±¡éƒ½å¯ä»¥ä½¿ç”¨streamå’Œastreamçš„æ–¹æ³•, ä»¥æµå¼çš„æ–¹æ³•è¿›è¡Œè¾“å‡ºä»¥åŠå¤„ç†è¾“å…¥æµ

```python
from langchain_openai import ChatOpenAI
model = ChatOpenAI(model="gpt-3.5-turbo-1106")
chunks = []
for chunk in model.stream("ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"):
    chunks.append(chunk)
    print(chunk.content, end="|", flush=True)
    
"""
|ä½ |å¥½|ï¼Œ|æˆ‘|æ˜¯|ä¸€ä¸ª|è¯­|è¨€|æ¨¡|å‹|äºº|å·¥|æ™º|èƒ½|åŠ©|æ‰‹|ï¼Œ|å¯ä»¥|å›|ç­”|å„|ç§|é—®é¢˜|ã€|æ|ä¾›|ä¿¡æ¯|å’Œ|å¸®|åŠ©|è§£|å†³|é—®é¢˜|ã€‚|æˆ‘|æ²¡æœ‰|å…·|ä½“|çš„|ä¸ª|äºº|èº«|ä»½|å’Œ|ç»|å†|ï¼Œ|ä½†|æˆ‘|è¢«|è®¾è®¡|æˆ|å¯ä»¥|å’Œ|ç”¨æˆ·|è¿›è¡Œ|è‡ª|ç„¶|ã€|å¯Œ|æœ‰|æ„|ä¹‰|çš„|å¯¹|è¯|ã€‚|å¸Œ|æœ›|æˆ‘|èƒ½|å¤Ÿ|å¸®|åŠ©|åˆ°|ä½ |ï¼|æœ‰|ä»€|ä¹ˆ|é—®é¢˜|å¯ä»¥|é—®|æˆ‘çš„|å—|ï¼Ÿ||
"""
```

è¿”å›çš„æ•°æ®æ˜¯å¾ˆä¸ªAIMessageChunk

```python
chunks[0]
"""
AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-7f89c698-1a31-403a-96b8-105b4d3bc9ea')
"""
```

å®é™…æ˜¯æŠŠLLMçš„äº‹ä»¶æŠ¥æ–‡è¿›è¡Œè½¬æ¢, è¿™ä¸€ä¸ªæ•°æ®ç±»å‹å¯ä»¥ä½¿ç”¨`+`è¿›è¡Œè¿æ¥

```python
chunks[0] + chunks[1] + chunks[2]
"""
AIMessageChunk(content='ä½ å¥½', additional_kwargs={}, response_metadata={}, id='run-7f89c698-1a31-403a-96b8-105b4d3bc9ea')
"""
```

### LCELè¯­è¨€

LangChainçš„è¡¨è¾¾å¼è¯­è¨€, å¯ä»¥ä½¿ç”¨è¿™ä¸ªè¯­è¨€æŠŠåŸºæœ¬çš„å—è¿›è¡Œç»„åˆ, å¯ä»¥è‡ªåŠ¨çš„å®ç°streamå’Œastreamçš„è§†çº¿, å®ç°å¯¹æœ€ç»ˆç»“æœçš„æµå¼ä¼ è¾“

```python
import asyncio
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
prompt = ChatPromptTemplate.from_template("ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹, ç»™æˆ‘ä¸€ä¸ªæœ‰å…³{input}çš„ç¬‘è¯")
parser = StrOutputParser()  # æŠŠè¾“å‡ºçš„å¯¹è±¡è½¬æ¢ä¸ºä¸€ä¸ªå­—ç¬¦ä¸²
chain = prompt | model | parser
async for chunk in chain.astream({"input":"çŒ«"}):
    print(chunk, end="|", flush=True)
"""
|ä¸º|ä»€|ä¹ˆ|çŒ«|æ— |æ³•|å‚|åŠ |æ¯”|èµ›|ï¼Ÿ|å› |ä¸º|ä»–|ä»¬|æ€»|æ˜¯|åœ¨|æŠ“|è€³|æŒ |è…®|ï¼|å“ˆ|å“ˆ|å“ˆ|ï¼||
"""
```



```python
import asyncio
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import JsonOutputParser

model = ChatOpenAI(model="gpt-3.5-turbo-1106")
parser = JsonOutputParser()
chain = model | parser
async def async_stream():
    async for text in chain.astream("ä»¥jsonçš„æ ¼å¼è¾“å‡ºè‹±å›½, ä¸­å›½, æ—¥æœ¬çš„äººå£åˆ—è¡¨"
                                    "ä½¿ç”¨ä¸€ä¸ªæœ‰countryå­—æ®µçš„jsonæ ¼å¼æ¥è¡¨ç¤ºå›½å®¶, ä¸€ä¸ªæœ‰population"
                                    "å­—æ®µçš„jsonæ ¼å¼æ¥è¡¨ç¤ºäººå£"
                                    ):
        print(text)


asyncio.run(async_stream())

"""
{}
{'countries': []}
{'countries': [{}]}
{'countries': [{'country': ''}]}
{'countries': [{'country': 'è‹±'}]}
{'countries': [{'country': 'è‹±å›½'}]}
{'countries': [{'country': 'è‹±å›½', 'population': ''}]}
{'countries': [{'country': 'è‹±å›½', 'population': '660'}]}
{'countries': [{'country': 'è‹±å›½', 'population': '660400'}]}
{'countries': [{'country': 'è‹±å›½', 'population': '66040000'}]}
{'countries': [{'country': 'è‹±å›½', 'population': '66040000'}, {}]}
{'countries': [{'country': 'è‹±å›½', 'population': '66040000'}, {'country': ''}]}
{'countries': [{'country': 'è‹±å›½', 'population': '66040000'}, {'country': 'ä¸­å›½'}]}
{'countries': [{'country': 'è‹±å›½', 'population': '66040000'}, {'country': 'ä¸­å›½', 'population': ''}]}
{'countries': [{'country': 'è‹±å›½', 'population': '66040000'}, {'country': 'ä¸­å›½', 'population': '143'}]}
{'countries': [{'country': 'è‹±å›½', 'population': '66040000'}, {'country': 'ä¸­å›½', 'population': '143932'}]}
{'countries': [{'country': 'è‹±å›½', 'population': '66040000'}, {'country': 'ä¸­å›½', 'population': '143932377'}]}
{'countries': [{'country': 'è‹±å›½', 'population': '66040000'}, {'country': 'ä¸­å›½', 'population': '1439323776'}]}
{'countries': [{'country': 'è‹±å›½', 'population': '66040000'}, {'country': 'ä¸­å›½', 'population': '1439323776'}, {}]}
{'countries': [{'country': 'è‹±å›½', 'population': '66040000'}, {'country': 'ä¸­å›½', 'population': '1439323776'}, {'country': ''}]}
{'countries': [{'country': 'è‹±å›½', 'population': '66040000'}, {'country': 'ä¸­å›½', 'population': '1439323776'}, {'country': 'æ—¥'}]}
{'countries': [{'country': 'è‹±å›½', 'population': '66040000'}, {'country': 'ä¸­å›½', 'population': '1439323776'}, {'country': 'æ—¥æœ¬'}]}
{'countries': [{'country': 'è‹±å›½', 'population': '66040000'}, {'country': 'ä¸­å›½', 'population': '1439323776'}, {'country': 'æ—¥æœ¬', 'population': ''}]}
{'countries': [{'country': 'è‹±å›½', 'population': '66040000'}, {'country': 'ä¸­å›½', 'population': '1439323776'}, {'country': 'æ—¥æœ¬', 'population': '125'}]}
{'countries': [{'country': 'è‹±å›½', 'population': '66040000'}, {'country': 'ä¸­å›½', 'population': '1439323776'}, {'country': 'æ—¥æœ¬', 'population': '125360'}]}
{'countries': [{'country': 'è‹±å›½', 'population': '66040000'}, {'country': 'ä¸­å›½', 'population': '1439323776'}, {'country': 'æ—¥æœ¬', 'population': '125360000'}]}
"""
```

### Stream events(äº‹ä»¶æµ)

| event                | name             | chunk                           | input                                         | output                                          |
| -------------------- | ---------------- | ------------------------------- | --------------------------------------------- | ----------------------------------------------- |
| on_chat_model_start  | [model name]     |                                 | {"messages": [[SystemMessage, HumanMessage]]} |                                                 |
| on_chat_model_stream | [model name]     | AIMessageChunk(content="hello") |                                               |                                                 |
| on_chat_model_end    | [model name]     |                                 | {"messages": [[SystemMessage, HumanMessage]]} | AIMessageChunk(content="hello world")           |
| on_llm_start         | [model name]     |                                 | {'input': 'hello'}                            |                                                 |
| on_llm_stream        | [model name]     | 'Hello'                         |                                               |                                                 |
| on_llm_end           | [model name]     |                                 | 'Hello human!'                                |                                                 |
| on_chain_start       | format_docs      |                                 |                                               |                                                 |
| on_chain_stream      | format_docs      | "hello world!, goodbye world!"  |                                               |                                                 |
| on_chain_end         | format_docs      |                                 | [Document(...)]                               | "hello world!, goodbye world!"                  |
| on_tool_start        | some_tool        |                                 | {"x": 1, "y": "2"}                            |                                                 |
| on_tool_end          | some_tool        |                                 |                                               | {"x": 1, "y": "2"}                              |
| on_retriever_start   | [retriever name] |                                 | {"query": "hello"}                            |                                                 |
| on_retriever_end     | [retriever name] |                                 | {"query": "hello"}                            | [Document(...), ..]                             |
| on_prompt_start      | [template_name]  |                                 | {"question": "hello"}                         |                                                 |
| on_prompt_end        | [template_name]  |                                 | {"question": "hello"}                         | ChatPromptValue(messages: [SystemMessage, ...]) |

```python
async def async_stream():
    events = []
    async for event in model.astream_events("ä»¥jsonçš„æ ¼å¼è¾“å‡ºè‹±å›½, ä¸­å›½, æ—¥æœ¬çš„äººå£åˆ—è¡¨"
                                            "ä½¿ç”¨ä¸€ä¸ªæœ‰countryå­—æ®µçš„jsonæ ¼å¼æ¥è¡¨ç¤ºå›½å®¶,"
                                            "ä¸€ä¸ªæœ‰populationå­—æ®µçš„jsonæ ¼å¼æ¥è¡¨ç¤ºäººå£", 
                                            version="v2"):
        events.append(event)
    print(events)

asyncio.run(async_stream())
"""
[{
'event': 'on_chat_model_start', 
'data': 
	{
        'input': 'ä»¥jsonçš„æ ¼å¼è¾“å‡ºè‹± å›½, ä¸­å›½, æ—¥æœ¬çš„äººå£åˆ—è¡¨ä½¿ç”¨ä¸€ä¸ªæœ‰countryå­—æ®µçš„jsonæ ¼å¼æ¥è¡¨ç¤ºå›½å®¶,ä¸€ä¸ªæœ‰populationå­—æ®µçš„jsonæ ¼å¼æ¥è¡¨ç¤ºäººå£'}, 
        'name': 'ChatOpenAI', 
        'tags': [], 
        'run_id': '4b4bb3fb-d9ef-489e-8168-59e25185ade3', 
        'metadata': {
            'ls_provider': 'openai', 
            'ls_model_name': 'gpt-3.5-turbo-1106', 
            'ls_model_type': 'chat', 
            'ls_temperature': None
        }, 
        'parent_ids': []
	}, 
{
    'event': 'on_chat_model_stream', 
    'run_id': '4b4bb3fb-d9ef-489e-8168-59e25185ade3', 
    'name': 'ChatOpenAI', 
    'tags': [], 
    'metadata': {
        'ls_provider': 'openai', 
        'ls_model_name': 'gpt-3.5-turbo-1106', 
        'ls_model_type': 'chat', 
        'ls_temperature': None
	}, 
	'data': {
		'chunk': AIMessageChunk(content='', 
		additional_kwargs={}, 
		response_metadata={}, 
		id='run-4b4bb3fb-d9ef-489e-8168-59e25185ade3')
	}, 
	'parent_ids': []
}, 
....
"""
```

###  å¼‚æ­¥æ‰§è¡Œ

```python
async def task1():
    model = ChatOpenAI(model="gpt-3.5-turbo-1106")
    chunks = []
    async for chunk in model.astream("ä»‹ç»ä¸€ä¸‹è¶Šå—"):
        chunks.append(chunk)
        print(chunk.content, end="|", flush=True)


async def task2():
    model = ChatOpenAI(model="gpt-3.5-turbo-1106")
    chunks = []
    async for chunk in model.astream("ä»‹ç»ä¸€ä¸‹è€æŒ"):
        chunks.append(chunk)
        print(chunk.content, end="|", flush=True)


async def main():
    await asyncio.gather(task1(), task2())

asyncio.run(main())
"""
|è¶Š|å—|ï¼Œ|å…¨|å|ä¸º||è€|æŒ|ï¼Œ|å…¨|è¶Š|ç§°|ä¸º|å—|ç¤¾|è€|æŒ|äºº|æ°‘|æ°‘|ä¸»|å…±|å’Œ| å›½|ä¼š|ä¸»|ä¹‰|å…±|å’Œ|å›½|ï¼Œ|æ˜¯|ä¸œ|å—|äºš|çš„|ä¸€ä¸ª|ï¼Œ|æ˜¯|ä¸œ|å—|å›½|äºš|çš„|å®¶|ï¼Œ| ä¸œ|ä¸€ä¸ª|å†…|é™†|å›½|ä¸´|å®¶|å—|ï¼Œ|ä¸­å›½|ä½|æµ·|ï¼Œ|äº|ä¸­å›½|ã€|è¶Š|å—|ä¸|å—|è€|æŒ|ã€|å’Œ|æ³°|æŸ¬|åŸ”|å›½|å¯¨|ã€|ç›¸|é‚»|ç¼…|ç”¸|ï¼Œ|è¥¿|ä¸|å’Œ|æŸ¬|åŸ”|æŸ¬|å¯¨|ä¹‹|åŸ”|é—´|å¯¨|å’Œ|ã€‚|æ³°|è€|æŒ|å›½|çš„|æ¥|é¦–|éƒ½|å£¤|å’Œ|ï¼Œ|æœ€|åŒ—|å¤§|ç•Œ|åŸ|ä¸|å¸‚|ä¸­å›½|æ˜¯|äº¤| ç•Œ|ä¸‡|ã€‚|è¶Š|è±¡|ã€‚

|å—|è€|æ˜¯|æŒ|æ˜¯|ä¸€ä¸ª|ä¸€ä¸ª|å¤š|å±±|åœ°|æ‹¥|æœ‰|å½¢|çš„|æ‚ |å›½|ä¹…|å®¶|ï¼Œ|å†|æ‹¥|å²| æœ‰|å’Œ|ä¸°|ä¸°|å¯Œ|å¯Œ|çš„|è‡ª|ç„¶|æ–‡|èµ„æº|ï¼Œ|åŒ…|æ‹¬|æ°´|åŒ–|é—|äº§|çš„|å›½|åŠ›|å®¶|èµ„æº|ï¼Œ|ã€|è‡ª|æ£®|æ—|å¤|ä»¥|æ¥|å°±|èµ„æº|æ˜¯|å’Œ|ä¸€ä¸ª|çŸ¿|é‡|è¦|äº§|çš„|èµ„æº|æ–‡|ã€‚|åŒ–|å†œ|å’Œ|ä¸š|æ˜¯|å•†|è€|ä¸š|æŒ|æ¢|ç»|çº½|ã€‚

|æµ|è¶Š|å—|çš„|çš„|æ”¯|æŸ±|ï¼Œ|é¦–|éƒ½|ä¸»|æ˜¯|æ²³|å†…|è¦|ç§|ï¼Œ|æ¤|ç¨»|æ˜¯|ç±³|ã€|ä¸€ä¸ª|ç‰|å†|ç±³|ã€|å²|æ‚ |å’–|å•¡|ä¹…|ä¸”|å’Œ|æ©¡|å……|æ»¡|èƒ¶|æ´»|ç­‰|ä½œ|åŠ›|ç‰©|çš„|ã€‚

|è€|åŸ|æŒ|å¸‚|æ˜¯|ã€‚|è¶Š|ä¸€ä¸ª|ä¸–|ä¿—|å—|å›½|ä»¥|å…¶|å®¶|ï¼Œ|ç¾|ä¿¡|ä¸½|ä»°|çš„|è‡ª|ç„¶|é£|å…‰|ã€|æ‚ |ä¹…|çš„|å†|å²|å’Œ|æ–‡|åŒ–|ã€|ä¸°|å¯Œ|çš„|ç¾|é£Ÿ|å’Œ|ç‹¬|ç‰¹|çš„|æ°‘|ä¿—|é£|æƒ…|è€Œ|é—»|å|äº|ä¸–|ã€‚|è¥¿|å±±|ç¾¤|å³°|ã€|ä¸‹|é¾™|æ¹¾|ã€|æ²³|å†…|è€|åŸ|ã€|å²˜|æ¸¯|ã€|èƒ¡|å¿—|æ˜|å¸‚|ç­‰|åœ°|éƒ½|æ˜¯|è¶Š|å—|è‘—|å|çš„|æ—…|æ¸¸|èƒœ|åœ°|ã€‚

|è¶Š|å—|çš„|ç»|æµ|ä¸»|è¦|ä»¥|å†œ|ä¸š|ã€|å·¥|ä¸š|å’Œ|æœåŠ¡|ä¸š|ä¸º|ä¸»|ï¼Œ|å…¶ä¸­|å†œ|ä¸š| æ˜¯|å›½|æ°‘|ç»|æµ|çš„|é‡|è¦|æ”¯|æŸ±|ã€‚|è¶Š|å—|æ˜¯|ä¸–|ç•Œ|ä¸Š|æœ€|å¤§|çš„|ç¨»|ç±³|å‡º|å£|å›½|ä¸Š|ä»¥|ä¹‹|ä¸€|ï¼Œ|ä½›|åŒæ—¶|æ•™|ä¹Ÿ|ä¸º|æœ‰|ä¸°|ä¸»|å¯Œ|ï¼Œ|çš„|å¤©|ç„¶|ä½›|æ•™|èµ„æº|ï¼Œ|åŒ…|æ‹¬|æ–‡|åŒ–|åœ¨|è€|æŒ|å¾—|çŸ³|åˆ°|æ²¹|äº†|ã€|å¹¿|å¤©|æ³›|ç„¶|ä¼ |æ’­|æ°”|å’Œ|å’Œ|å‘|ç¨€|å±•|åœŸ|ã€‚|è€|ç­‰|ã€‚|æŒ|è¿‘|çš„|æ–‡|åŒ–|å’Œ|å¹´|ä¼ |æ¥|ç»Ÿ|ï¼Œ|è¶Š|ä¹Ÿ|å—|çš„|å—|åˆ°|ä½›|ç»|æµ|æ•™|çš„|å¾—|å½±|åˆ°|å“|äº†|ï¼Œ|äºº|ä»¬|å¿«|é€Ÿ|å°Š|é‡|å‘|å±•|é•¿|è¾ˆ|ï¼Œ|ï¼Œ|å¸|å¼•|äº†|æ³¨|å¤§|é‡|ç¤¼|é‡|ä»ª|å’Œ|å¤–|å›½|ä¼ |ç»Ÿ|æŠ•|èŠ‚|èµ„|æ—¥|ã€‚

|è¶Š|ã€‚

|è€|å—|äºº|æŒ|æ°‘|çƒ­|çš„|æ—…|æƒ…|æ¸¸|ä¸š|å¥½|å‘|å±•|å®¢|è¿…|ï¼Œ|é€Ÿ|å–œ|ï¼Œ|æ¬¢|å¸|éŸ³|å¼•|äº†|ä¹|è¶Š|ã€|èˆ|è¹ˆ|æ¥|è¶Š|å¤š|çš„|å›½|å†…|å’Œ|å¤–|ç¾|é£Ÿ|æ¸¸|å®¢|ã€‚|ã€‚|è¶Š|è‘—|å|çš„|æ—…|å—|èœ|æ¸¸|æ™¯|ä»¥|ç‚¹|æ±¤|æ–™|åŒ…|ä¸º|æ‹¬|ä¸»|ç…|å‹ƒ|ï¼Œ|æ‹‰|é‚¦|å£|ã€|å‘³|ä¸‡|æ¸…|è£|ã€|æ·¡|æ°¸|ï¼Œ|ä½†|ç|ã€|å¹³|åˆ|ä¸|å£¤|ä¹|å’Œ|ç‹¬|ç‰¹|çš„|å—|èµ›|æ¾|æ²¹|å‘³|ã€‚

|è€|é“|æŒ|ï¼Œ|äºº|è¢«|èª‰|æ°‘|å‹|ä¸º|å¥½|çƒ­|ä¸–|ç•Œ|æƒ…|ï¼Œ|ç”Ÿ|ä¸Š|æœ€|ç¾|æ´»|å‘³|èŠ‚|å¥|çš„|ç¾|é£Ÿ|ä¹‹|æ‚ |ä¸€|é—²|ã€‚|ï¼Œ|è¢«|èª‰|ä¸º|åœ¨|è¶Š|å—|ï¼Œ|â€œ|ä¸œ|æ–¹|äºº|ä¹‹|å›½|ä»¬|â€ã€‚|å¦‚æœ|ä½ |è¿˜|æƒ³|ä½“|ä¿|éªŒ|ç•™|ä¸|ç€|è®¸|å¤š|ä¸€|æ ·|çš„|ä¸œ|ä¼ |å—|äºš|ç»Ÿ|é£|çš„|æƒ…|èŠ‚|ï¼Œ|è€|æ—¥|æŒ|æ˜¯|å’Œ|ä¸€ä¸ª|å¾ˆ|ä¹ |å¥½|çš„|é€‰æ‹©|ã€‚||ä¿—|ï¼Œ|å¦‚|æ³¼|æ°´|èŠ‚|ã€|ä¸­| ç§‹|èŠ‚|ã€|ç«|æŠŠ|èŠ‚|ç­‰|ï¼Œ|è¿™|äº›|èŠ‚|æ—¥|éƒ½|ä½“|ç°|äº†|è¶Š|å—|äºº|æ°‘|çš„|å‹¤|åŠ³|å’Œ|çƒ­|æƒ…|ã€‚

|æ€»|çš„|æ¥|è¯´|ï¼Œ|è¶Š|å—|æ˜¯|ä¸€ä¸ª|æ‹¥|æœ‰|ä¸°|å¯Œ|æ–‡|åŒ–|å’Œ|é£|åœŸ|äºº|æƒ…|çš„|å›½|å®¶|ï¼Œ|æ˜¯|ä¸€ä¸ª|å€¼|å¾—|ä¸€|æ¸¸|çš„|æ—…|æ¸¸|èƒœ|åœ°|ï¼Œ|ä¹Ÿ|æ˜¯|ä¸€ä¸ª|æ­£åœ¨|è“¬|å‹ƒ|å‘|å±•|çš„|ç»|æµ|ä½“|ã€‚||
"""
```

## æœåŠ¡éƒ¨ç½²

[ğŸ¦œï¸ğŸ“ æœ—æ–¯ |ğŸ¦œï¸ğŸ”— LangChain è¯­è¨€é“¾ --- ğŸ¦œï¸ğŸ“ LangServe | ğŸ¦œï¸ğŸ”— LangChain](https://python.langchain.com/docs/langserve/)

LangServerå¯ä»¥æŠŠLangServeréƒ¨ç½²ä¸ºä¸€ä¸ªAPIçš„å½¢å¼, é›†æˆFastAPI, ä½¿ç”¨Pydanticè¿›è¡ŒéªŒè¯, åºåˆ—åŒ–, ç”Ÿæˆjsonæ¶æ„ä»¥åŠè‡ªåŠ¨ç”Ÿæˆæ–‡æ¡£ç­‰, æ­¤å¤–è¿˜æœ‰ä¸€ä¸ªå®¢æˆ·ç«¯

+ å®‰è£…

```bash
pip install --upgrade "langserve[all]"
# ä¹Ÿå¯ä»¥åˆ†å¼€
pip install --upgrade "langserve[server]"
pip install --upgrade "langserve[client]"
```

+ CLIå·¥å…·å¿«é€Ÿåˆ›å»ºå·¥ç¨‹

```bash
pip install -U langchain-cli
```

ä¹‹åå¯ä»¥ä½¿ç”¨å‘½ä»¤`langchain app new é¡¹ç›®åå­—`

![image-20250220172546614](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502201725775.png)

ä¹‹ååœ¨add_routesé‡Œé¢å®šä¹‰å¯ä»¥è¿è¡Œçš„å¯¹è±¡, åœ¨server.pyé‡Œé¢ç¼–è¾‘

+ ä½¿ç”¨poetryæ·»åŠ ç¬¬ä¸‰æ–¹çš„åŒ…(langchain-openaiç­‰)

```python
pip install pipx
pipx install poetry
poetry add langchain
poetry add langchain-openai
```



### ç‰¹æ€§

+ APIè°ƒç”¨çš„æ—¶å€™è‡ªåŠ¨ç”Ÿæˆé”™è¯¯çš„ä¿¡æ¯
+ ä¸€ä¸ªå¸¦æœ‰JSONchemaå’ŒSwaggerçš„APIæ–‡æ¡£é¡µé¢, æ’å…¥ç¤ºä¾‹é“¾æ¥
+ é«˜æ•ˆçš„`/stream`, `/invoke`, `/batch`ç«¯ç‚¹, å•æœåŠ¡å™¨å¤šä¸ªå¹¶å‘è¯·æ±‚
+ `/stream_log`ç«¯ç‚¹, ç”¨äºæµå¼ä¼ è¾“ä»£ç†çš„æ‰€æœ‰ä¸­é—´æ­¥éª¤
+ `/stream_events`é«˜ç‰ˆæœ¬çš„æ—¶å€™ä½¿ç”¨æµå¼ä¼ è¾“
+ å®¢æˆ·ç«¯çš„SDKè°ƒç”¨çš„å®é™…æ•ˆæœå’Œæœ¬åœ°å¯è¿è¡Œå¯¹è±¡ä¸€æ ·condaconda

### å®é™…ä½¿ç”¨

æœåŠ¡å™¨ç«¯

```python
from fastapi import FastAPI
from fastapi.responses import RedirectResponse
from langserve import add_routes
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

# å»ºç«‹ä¸€ä¸ªFastAPIåº”ç”¨
app = FastAPI(
    title="LangChain æœåŠ¡å™¨",
    description="è¿™æ˜¯ä¸€ä¸ªåŸºäºFastAPIçš„LangChainæœåŠ¡å™¨",
    version="0.1.0",
)

# å°†æ ¹è·¯å¾„é‡å®šå‘åˆ°/docs
@app.get("/")
async def redirect_root_to_docs():
    return RedirectResponse("/docs")

# åœ¨è¿™é‡Œæ·»åŠ LangChainçš„è·¯ç”±
add_routes(app, 
           ChatOpenAI(model="gpt-3.5-turbo-1106")| StrOutputParser(),
           path="/openai")

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8000)
```

å¯ä»¥ä½¿ç”¨`poetry run langchain serve --port=8000`æ‰“å¼€æœåŠ¡

å®¢æˆ·ç«¯ä½¿ç”¨, langchainæ¨¡å¼

```python
from langchain.schema.runnable import RunnableMap
from langchain_core.prompts import ChatPromptTemplate
from langserve import RemoteRunnable

openai = RemoteRunnable("http://127.0.0.1:8000/openai")
prompt = ChatPromptTemplate.from_messages(
    [("system",  "ä½ æ˜¯ä¸€ä¸ªçŒ«å¨˜"), ("user", "{input}")]
)

chain = prompt | RunnableMap({
    "openai": openai
}) # ç”¨äºå°†è¾“å…¥æ˜ å°„åˆ°ä¸åŒçš„Runnableä¸Š, å¯ä»¥ä½¿ç”¨chainè¿›è¡Œè¿œç¨‹è°ƒç”¨

print("åŒæ­¥è°ƒç”¨/openai/invoke")
response = chain.invoke({"input": "ä½ æ˜¯è°"})
print(response)
"""
{'openai': 'å—¯ï¼Œæˆ‘æ˜¯ä¸€åªçŒ«å¨˜ï¼Œå¯ä»¥é™ªä½ èŠå¤©å’Œå›ç­”é—®é¢˜å“¦ã€‚æœ‰ä»€ä¹ˆéœ€è¦å¸®åŠ©çš„å—ï¼Ÿ'}
"""
```

å®¢æˆ·ç«¯ä½¿ç”¨requestsæ¨¡å¼

```python
import requests
import json

respond = requests.post(
    url="http://127.0.0.1:8000/openai/invoke",
    json={
        "input": "ä½ æ˜¯è°"
    }
)
print(respond.json())
"""
{'output': 'æˆ‘æ˜¯ä¸€ä¸ªç”¨äººå·¥æ™ºèƒ½æŠ€æœ¯è®¾è®¡çš„è™šæ‹ŸåŠ©æ‰‹ï¼Œå¯ä»¥å›ç­”ä½ çš„é—®é¢˜å¹¶ä¸ä½ è¿›è¡Œå¯¹è¯ã€‚æˆ‘ä¸æ˜¯ä¸€ä¸ªå…·æœ‰å®ä½“å½¢æ€çš„äººæˆ–ç”Ÿç‰©ï¼Œè€Œæ˜¯ä¸€ä¸ªç¨‹åºåœ¨è®¡ç®—æœºä¸­è¿è¡Œçš„è™šæ‹Ÿå®ä½“ã€‚æœ‰ä»€ä¹ˆå¯ä»¥å¸®åˆ°ä½ çš„å—ï¼Ÿ', 'metadata': {'run_id': '8955a316-8620-4082-ae73-7a68cfe91290', 'feedback_tokens': []}}
"""
```

å¦‚æœæ„å»ºçš„æ—¶å€™ä½¿ç”¨å‚æ•°

```python
prompt = ChatPromptTemplate.from_template(
    "ä½ æ˜¯ä¸€ä¸ªçŒ«å¨˜, ç”¨æˆ·å‘æ¥{message}, è¯·ä½ å›ç­”"
) # ç”¨äºç”Ÿæˆä¸€ä¸ªèŠå¤©æ¨¡æ¿, å®é™…è°ƒç”¨çš„æ—¶å€™ä¼šå°†ç”¨æˆ·çš„è¾“å…¥å¡«å…¥{message}ä¸­
add_routes(app,
            prompt | ChatOpenAI(model="gpt-3.5-turbo-1106") | StrOutputParser(),
            path="/catgirl")
```

è®¿é—®çš„æ—¶å€™ä¹Ÿéœ€è¦è¿™ä¸ªå‚æ•°

```python
print("åŒæ­¥è°ƒç”¨/catgirl/invoke")
response = chain.invoke({"input": {"message": "ä½ æ˜¯è°"}})
print(response)
```

```python
respond = requests.post(
    url="http://127.0.0.1:8000/catgirl/invoke",
    json={
        "input": {"message": "ä½ æ˜¯è°"}
    }
)
print(respond.json())
```

+ ä½¿ç”¨æµå¼è°ƒç”¨

```python
print("å¼‚æ­¥è°ƒç”¨/openai/stream")
for chunk in chain.stream({"input": "ä½ æ˜¯è°"}):
    print(chunk, end="", flush=True)

print("å¼‚æ­¥è°ƒç”¨/catgirl/stream")
for chunk in chain.stream({"input": {"message": "ä½ æ˜¯è°"}}):
    print(chunk, end="", flush=True)
"""
å¼‚æ­¥è°ƒç”¨/openai/stream
{'openai': ''}{'openai': 'æˆ‘'}{'openai': 'æ˜¯'}{'openai': 'ä¸€ä¸ª'}{'openai': 'AI'}{'openai': 'åŠ©'}{'openai': 'æ‰‹'}{'openai': 'ï¼Œ'}{'openai': 'å¯ä»¥'}{'openai': 'å›'}{'openai': 'ç­”'}{'openai': 'ä½ '}{'openai': 'çš„'}{'openai': 'é—®é¢˜'}{'openai': 'å’Œ'}{'openai': 'ä¸'}{'openai': 'ä½ '}{'openai': 'èŠ'}{'openai': 'å¤©'}{'openai': 'ã€‚'}{'openai': 'ä½ '}{'openai': 'å¯ä»¥'}{'openai': 'å«'}{'openai': 'æˆ‘'}{'openai': 'çŒ«'}{'openai': 'å¨˜'}{'openai': 'å“¦'}{'openai': '~'}{'openai': 'æœ‰'}{'openai': 'ä»€'}{'openai': 'ä¹ˆ'}{'openai': 'é—®é¢˜'}{'openai': 'æƒ³'}{'openai': 'é—®'}{'openai': 'æˆ‘'}{'openai': 'å—'}{'openai': 'ï¼Ÿ'}{'openai': ''}
å¼‚æ­¥è°ƒç”¨/catgirl/stream
{'openai': ''}{'openai': 'å—¨'}{'openai': 'ï¼'}{'openai': 'æˆ‘'}{'openai': 'æ˜¯'}{'openai': 'ä¸€'}{'openai': 'åª'}{'openai': 'çŒ«'}{'openai': 'å¨˜'}{'openai': 'ï¼Œ'}{'openai': 'å¾ˆ'}{'openai': 'é«˜'}{'openai': 'å…´'}{'openai': 'è®¤'}{'openai': 'è¯†'}{'openai': 'ä½ '}{'openai': 'ï¼'}{'openai': 'æœ‰'}{'openai': 'ä»€'}{'openai': 'ä¹ˆ'}{'openai': 'é—®é¢˜'}{'openai': 'æƒ³'}{'openai': 'è¦'}{'openai': 'é—®'}{'openai': 'æˆ‘'}{'openai': 'å—'}{'openai': 'ï¼Ÿ'}{'openai': 'ğŸ±'}{'openai': ''}
"""
```

```python
respond = requests.post(
    url="http://127.0.0.1:8000/openai/stream",
    json={
        "input": "ä½ æ˜¯è°"
    }
)
for line in respond.iter_lines():
    print(line.decode("utf-8"))
"""
event: metadata
data: {"run_id": "82b92785-d68a-4485-8ba1-634a35b22cba"}

event: data
data: ""

event: data
data: "æˆ‘"

...

event: data
data: ""

event: end
"""
```

## æœåŠ¡ç›‘æ§

å¯ä»¥ä½¿ç”¨LangSmith, 

ä½¿ç”¨è¿™ä¸ªéœ€è¦ä½¿ç”¨ç¯å¢ƒå˜é‡

```bash
setx LANGCHAIN_TRACING_V2 "True"
setx LANGCHAIN_API_KEY "..."
setx TAVILY_API_KEY	"..."
```

è¿˜å¯ä»¥ä½¿ç”¨verboseè¯¦ç»†æ‰“å°æ—¥å¿—

```python
from langchain.globals import set_verbose
set_verbose(True)
```

## èŠå¤©ç®¡ç†

### å†…å­˜

å¯ä»¥é€šè¿‡å­—å…¸å®ç°

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a assistant helping a user with their homework. The user asks you to help them with their math homework."),
        MessagesPlaceholder(variable_name="history"),# å†å²æ¶ˆæ¯å ä½ç¬¦
        ("human", "{input}"),
    ]
)

model = ChatOpenAI(model = "gpt-3.5-turbo-1106")
runable = prompt | model

store = {}

def get_chat_message_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

# é€šè¿‡RunnableWithMessageHistoryåŒ…è£…runableï¼Œä½¿å…¶èƒ½å¤Ÿè·å–å†å²æ¶ˆæ¯
with_message_history = RunnableWithMessageHistory(
    runable, 
    get_chat_message_history,
    input_messages_key="input",
    history_messages_key="history",
)
# RunnableWithMessageHistory must always be called with a config that contains
# the appropriate parameters for the chat message history factory.
response = with_message_history.invoke(
    input={
        "input": "ä»‹ç»ä¸€ä¸‹çº¿æ€§ä»£æ•°."
    },
    config={
        "configurable": {"session_id": "session_1"}
    }
)

print(response)

response = with_message_history.invoke(
    input={
        "input": "å†è¯¦ç»†ä¸€ç‚¹."
    },
    config={
        "configurable": {"session_id": "session_1"}
    }
)
print(response)
```

+ å¦‚æœä½¿ç”¨æ›´å¤šçš„æŸ¥æ‰¾å‚æ•°, éœ€è¦è‡ªå®šä¹‰

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

from langchain_core.runnables import ConfigurableFieldSpec

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a assistant helping a user with their homework. The user asks you to help them with their math homework."),
        MessagesPlaceholder(variable_name="history"),# å†å²æ¶ˆæ¯å ä½ç¬¦
        ("human", "{input}"),
    ]
)

model = ChatOpenAI(model = "gpt-3.5-turbo-1106")
runable = prompt | model
# è®°å½•ä½¿ç”¨çš„å­—å…¸
store = {}

# ä½¿ç”¨ç”¨æˆ·IDå’Œå¯¹è¯IDä½œä¸ºé”®
def get_chat_message_history(user_id: str, conversation_id: str) -> BaseChatMessageHistory:
    if (user_id, conversation_id) not in store:
        store[(user_id, conversation_id)] = ChatMessageHistory()
    return store[(user_id, conversation_id)]

# é€šè¿‡RunnableWithMessageHistoryåŒ…è£…runableï¼Œä½¿å…¶èƒ½å¤Ÿè·å–å†å²æ¶ˆæ¯
with_message_history = RunnableWithMessageHistory(
    runable, 
    get_chat_message_history,
    input_messages_key="input",
    history_messages_key="history",
    # é»˜è®¤åªç”¨ä¸€ä¸ªsession_id, è¿™é‡Œä½¿ç”¨user_idå’Œconversation_idä½œä¸ºé”®
    history_factory_config=[
        ConfigurableFieldSpec(
            id="user_id",
            annotation=str, # æ³¨è§£
            name="User ID",
            description="ç”¨æˆ·æ ‡è¯†.",
            is_shared=True,
            default="",
        ),
        ConfigurableFieldSpec(
            id="conversation_id",
            annotation=str, # æ³¨è§£
            name="Conversation ID",
            description="å¯¹è¯æ ‡è¯†.",
            is_shared=True, # å…±äº«, ç”¨äºåŒºåˆ†ä¸åŒç”¨æˆ·çš„å¯¹è¯
            default="",
        )
    ]
)
response = with_message_history.invoke(
    input={
        "input": "ä»‹ç»ä¸€ä¸‹çº¿æ€§ä»£æ•°."
    },
    config={
        "configurable": {"user_id": "123", "conversation_id": "1"}
    }
)
print(response)

response = with_message_history.invoke(
    input={
        "input": "å†è¯¦ç»†ä¸€ç‚¹."
    },
    config={
        "configurable": {"user_id": "123", "conversation_id": "1"}
    }
)
print(response)
```

### Rediså­˜å‚¨

```python
from langchain_community.chat_message_histories import RedisChatMessageHistory
# å®é™…æ”¹çš„ä½ç½®æ˜¯æŠŠè·å–å†å²è®°å½•çš„æ–¹å¼è¿›è¡Œæ”¹å˜
def get_message_history(session_id: str) -> RedisChatMessageHistory:
    # è¿æ¥åˆ°æœ¬åœ°çš„Redisæ•°æ®åº“, ä½¿ç”¨1å·æ•°æ®åº“, session_idä½œä¸ºé”®
    return RedisChatMessageHistory(session_id, url=REDIS_URL)

REDIS_URL = "redis://localhost:6379/1"
```

![image-20250222163748007](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502221637786.png)

![image-20250222164014405](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502221640666.png)

#### å¤„ç†å†å²è®°å½•

æ¶ˆæ¯å¦‚æœæ¯”è¾ƒé•¿, ä¼šæ¶ˆè€—token

```python
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser


temp_chat_history = ChatMessageHistory()
temp_chat_history.add_user_message("ä½ å¥½, æˆ‘åˆšæ‰åœ¨æ‰“ç¯®çƒ")
temp_chat_history.add_ai_message("ä½ å¥½")
temp_chat_history.add_user_message("ä½ å¥½, æˆ‘å«å°æ˜")
temp_chat_history.add_ai_message("ä½ å¥½")
temp_chat_history.add_user_message("å†è§")
temp_chat_history.add_ai_message("å†è§")
# print(temp_chat_history.messages) # æ¶ˆæ¯åˆ—è¡¨


prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a assistant helping a user"),
        MessagesPlaceholder(variable_name="history"),# å†å²æ¶ˆæ¯å ä½ç¬¦
        ("human", "{input}"),
    ]
)

model = ChatOpenAI(model = "gpt-3.5-turbo-1106")
runable = prompt | model


# æŠŠè¿å¤©è®°å½•é™åˆ¶åœ¨3æ¡æ¶ˆæ¯ä»¥å†…
def trim_message(chat_input):
    stored_messages = temp_chat_history.messages
    if len(stored_messages) <= 4:
        return False
    temp_chat_history.clear()
    for message in stored_messages[-4:]:
        temp_chat_history.add_message(message)
    return True



with_message_history = RunnableWithMessageHistory(
    runable, 
    lambda session_id: temp_chat_history,
    input_messages_key="input",
    history_messages_key="history",
)

chain_with_trim = (
    # ä½¿ç”¨trim_messageå‡½æ•°å¯¹è¾“å…¥æ¶ˆæ¯è¿›è¡Œå¤„ç†
    # trim_messageä¼ å…¥çš„å‚æ•°å®é™…æ˜¯è¾“å…¥æ¶ˆæ¯
    RunnablePassthrough.assign(messages_trimmed=trim_message) 
    | with_message_history
    | StrOutputParser()
)

response = chain_with_trim.invoke(
    input={
        "input": "æˆ‘çš„åå­—æ˜¯å•¥?"
    },
    config={
        "configurable": {"session_id": "session_1"}
    }
)
print(response)

response = chain_with_trim.invoke(
    input={
        "input": "æˆ‘åˆšæ‰åœ¨å¹²å•¥?"
    },
    config={
        "configurable": {"session_id": "session_1"}
    }
)
print(response)
"""
ä½ è¯´ä½ å«å°æ˜
ä½ åˆšæ‰åœ¨å’Œæˆ‘å¯¹è¯
"""
```

