## 思路

[图解大模型计算加速系列：vLLM源码解析1，整体架构 - 知乎](https://zhuanlan.zhihu.com/p/691045737)

![img](https://pic4.zhimg.com/v2-e902e6b12166aaebd2d9b50b0370ae8d_1440w.jpg)

- **`add_request()`**：该方法将每一个请求包装成vLLM能处理的数据类型(SequenceGroup，后面我们会详细解释)，并将其加入调度器（Scheduler）的waiting队列中。**在LLMEngine中，这个函数是按照“同步”的方式设计的**，也就是它被设计为“遍历batch中的每条数据，然后做相应处理”。所以这个函数本身只适合批处理场景。在异步的online serving中将会把它重写成异步的形式。
- **`abort_request`**：在推理过程中，并不是所有的请求都能有返回结果。比如客户端断开连接时，这个请求的推理就可以终止了（abort），这个函数就被用来做这个操作。
- **`step()`：负责执行1次推理过程（1个prefill算1个次推理，每个decode各算1次推理）**。在这个函数中，vLLM的调度器会决定要送那些数据去执行本次推理，并负责给这些数据分配好物理块（这些信息都被作为metadata放在要送给模型做推理的数据中）。模型会根据这些信息，采用PagedAttention方法，实际完成推理。

> 在文件E:\a学习\22-机器学习\vllm-main\vllm\engine\llm_engine.py里面
>
> ![img](https://pic1.zhimg.com/v2-eee036cd8edbc2b94d8758721b9809e8_1440w.jpg)

**Centralized Controller，也就是前文我们所说的调度器(Scheduler)**。它和LLMEngine所在的进程是同一个，且两者都是在CPU上的。

- **调度器的主要作用就是，在每1个推理阶段，决定要把哪些数据送给模型做推理，同时负责给这些模型分配[KV Cache](https://zhida.zhihu.com/search?content_id=241696234&content_type=Article&match_order=1&q=KV+Cache&zhida_source=entity)物理块**。但要注意，它只是分配了物理块的id，而不是物理块本身。物理块的实际分配是模型在推理过程中根据物理块id来操作的，也就是[CacheEngine](https://zhida.zhihu.com/search?content_id=241696234&content_type=Article&match_order=1&q=CacheEngine&zhida_source=entity)做的事情。
- **调度器下维护着BlockSpaceManager。它负责管理BlockAllocator（实际参与分配物理块的类）。BlockAllocator又分成gpu和cpu两种类型，分别管理这两类设备上的物理块**。**你可能会问，cpu上的物理块是什么呢**？你还记得调度器有一个swap策略吗？当gpu上显存不足时，它会把后来的请求抢占，并将其相关的KV cache物理块全部都先swap（置换、卸载）在cpu上，等后续gpu显存充足时，再把它们加载回gpu上继续做相关请求的推理。所以在cpu上我们也需要一个管控物理块的BlockAllocator。**实际代码实现时，Block相关的部分可不止这两个class，还有一些更复杂的逻辑细节。这个我们放在本系列后面的文章中讲解**。

Distributed Workers，也就是分布式系统，你可以将每个worker理解成一块gpu。它的作用是将我们要使用的模型load到各块卡上（目前对单卡装不下的模型，vLLM支持tp/pp推理），然后对Controller传来的数据做1次推理，返回相关结果。我们来细看下这块：

- **Distributed Workers**：图中绘制为Distributed Workers这个绿色块，**其实按vLLM的源码内容，写成Executor会更合适一些**。**它就是所有Workers的管控中心**，它指定了用什么方法管控这些Workers，负责分布式环境的初始化，目前支持的方法有：
  - cpu_executor：（较少用），使用cpu做推理时可考虑
  - gpu_executor：单卡（world_size = 1）的情况下可用
  - ray_gpu_executor：使用ray这个分布式计算框架实现的executor，适用于多卡环境

- **Worker**：**在硬件上，它指gpu；在代码上，它指的是Worker实例（每个gpu上的进程维护自己的Worker实例）**。在每个Worker实例中又管控着如下两个重要实例：
  - **CacheEngine：**负责管控gpu/cpu上的KV cache物理块（调度器的block manager只负责物理块id的分配，CacheEngine则是根据这个id分配结果实打实地在管理物理块中的数据）
  - **Worker.model**：根据vLLM代码，这里写成**model_runner**会更合适一些。**它负责加载模型，并执行推理**。PagedAttention的相关逻辑，就维护这个实例关联的代码下。

## 文件构成

![image-20241231170219993](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202412311702190.png)

![image-20241231170824801](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202412311708851.png)

## Debug

使用下面的脚本

![image-20241231171219425](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202412311712460.png)

```python
from vllm import LLM, SamplingParams

# Sample prompts.
prompts = [
    "Hello, my name is",
    "The president of the United States is",
    "The capital of France is",
    "The future of AI is",
]
# Create a sampling params object.
sampling_params = SamplingParams(temperature=0.8, top_p=0.95)

# Create an LLM.
llm = LLM(model="facebook/opt-125m")
# Generate texts from the prompts. The output is a list of RequestOutput objects
# that contain the prompt, generated text, and other information.
outputs = llm.generate(prompts, sampling_params)
# Print the outputs.
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
```

这里在初始化的时候加载了一个模型, 这个模型的类型是一个LLM类型的, 在文件`llm.py`里面

![image-20241231173220166](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202412311732206.png)

LLM=>EngineArgs=>LLMEngine

使用EngineArgs进行参数的初始化, 之后使用标准化的参数进行初始化一个Engine

在Engine里面主要是初始化Tokenizer, model_executer, init_kv_cache以及schedular, 这里面最主要的是执行器model_executer

![image-20250102132307961](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501021323221.png)

在这一个文件夹里面是各种各样的执行器, 所有的执行器使用统一的父类ExecutorBase, 在executor_base文件里面, gpu里面最后使用的初始化函数如下

```python
def _init_executor(self) -> None:
    """Initialize the worker and load the model.
        """
    assert self.parallel_config.world_size == 1, (
        "GPUExecutor only supports single GPU.")

    self.driver_worker = self._create_worker()
    self.driver_worker.init_device()
    self.driver_worker.load_model()
```

在建立worker的时候使用的类是WorkerWrapperBase, 这一层最后操作的为ModelRunner这一个层面, 到了这一层以后实际加载的模型不同使用不同的加载方式

### 推理

在实际调用的时候使用的是把数据调用llm的generate方法进行生成

这一个函数对传进来的参数进行处理以后使用函数_add_request函数进行添加参数, 最后使用run_engine函数

实际是把数据传递给engine

![image-20250102141010599](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501021410675.png)

_add_processed_request这一个函数里面把数据加入SequenceGroup, 使用add_seq_group给调度器

实际的执行是在llm的run_engine里面

![image-20250102141532186](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501021415245.png)

每一步实际是在executer_model

## 调度器

再开始的时候初始化一个块管理器

```c
class Circle{
// 公共权限
public:
    int radius;
    double calculateArea(){
        return 3.14159 * radius * radius;
    }

};

int main(void){
    Circle circle;
    circle.radius = 10;
    std::cout << "The area of the circle is: " << circle.calculateArea() << std::endl;
    return 0;
}
```



