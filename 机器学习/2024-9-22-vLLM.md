---
layout: post
title: "vLLM" 
date:   2024-8-5 15:39:08 +0800
tags: AI 机器学习
---

# vLLM

[vllm-project/vllm: A high-throughput and memory-efficient inference and serving engine for LLMs (github.com)](https://github.com/vllm-project/vllm)

`vLLM`是伯克利大学LMSYS组织开源的[大语言模型](https://zhida.zhihu.com/search?q=大语言模型&zhida_source=entity&is_preview=1)高速推理框架，旨在极大地提升实时场景下的语言模型服务的吞吐与内存使用效率。`vLLM`是一个快速且易于使用的库，用于 LLM 推理和服务，可以和HuggingFace [无缝集成](https://zhida.zhihu.com/search?q=无缝集成&zhida_source=entity&is_preview=1)。vLLM利用了全新的注意力算法「PagedAttention」，有效地管理注意力键和值。

在实际算的时候, 每一个新获取的token需要和之前的token进行计算, 这一个部分计算在获取一个新的token的使用是重复的, 所以可以存储起来, 用空间换时间

在实际使用的时候由于不知道需要申请的数量, 只能申请一个大数组

[arxiv.org/pdf/2309.06180](https://arxiv.org/pdf/2309.06180)

在模型运行的时候, 会使用大量的内存, 这时候由于不能预测实际会使用的内存的大小, 所以会使用预设的最大值进行申请, 同时会出现申请的内存之间存在碎片, 不足已被使用, 这导致实际的内存使用率只有20%到40%

## Transform和Key-Value Cache

Transformer是一种用于自然语言处理（NLP）和其他序列到序列（sequence-to-sequence）任务的深度学习模型架构，它在2017年由Vaswani等人首次提出。Transformer架构引入了自注意力机制（self-attention mechanism），这是一个关键的创新，使其在处理序列数据时表现出色。
以下是Transformer的一些重要组成部分和特点：

+ 自注意力机制（Self-Attention）：这是Transformer的核心概念之一，它使模型能够同时考虑输入序列中的所有位置，而不是像循环神经网络（RNN）或卷积神经网络（CNN）一样逐步处理。自注意力机制允许模型根据输入序列中的不同部分来赋予不同的注意权重，从而更好地捕捉语义关系。
+ 多头注意力（Multi-Head Attention）：Transformer中的自注意力机制被扩展为多个注意力头，每个头可以学习不同的注意权重，以更好地捕捉不同类型的关系。多头注意力允许模型并行处理不同的信息子空间。
+ 堆叠层（Stacked Layers）：Transformer通常由多个相同的编码器和解码器层堆叠而成。这些堆叠的层有助于模型学习复杂的特征表示和语义。
+ 位置编码（Positional Encoding）：由于Transformer没有内置的序列位置信息，它需要额外的位置编码来表达输入序列中单词的位置顺序。
+ 残差连接和层归一化（Residual Connections and Layer Normalization）：这些技术有助于减轻训练过程中的梯度消失和爆炸问题，使模型更容易训练。
+ 编码器和解码器：Transformer通常包括一个编码器用于处理输入序列和一个解码器用于生成输出序列，这使其适用于序列到序列的任务，如机器翻译。

在实际使用的时候可以使用这一个模型生成音频, 文本等

### 原理

输入的数据会被分割为简单的token, 这一个分割可以不是按照单词进行的, 每一个token实际会对应一个向量也就是一组数据, 可以看做高位坐标里面的一个位置, 通过计算, 比较接近的数据坐标靠近

### 实际的流程

对一个输入的数据进行encoder之后把数据输入到decoder, 这两个结构可以是多个相同的结构, 但是参数不是相同的

![image-20240924105336210](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409241053267.png)

#### encoder

这一个部分从下到上可以分为输入部分, 注意力机制, 前馈神经网络

![image-20240924110907862](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409241109898.png)

+ 输入部分

embedding, 把每一个字初始化为一个512字节的向量

## 部署

尝试部署一个GLM-4的模型[THUDM/GLM-4: GLM-4 series: Open Multilingual Multimodal Chat LMs | 开源多语言多模态对话模型 (github.com)](https://github.com/THUDM/GLM-4)

### 资料记录

[Quickstart — vLLM官方文档](https://docs.vllm.ai/en/latest/getting_started/quickstart.html)

[B站教程](https://github.com/echonoshy/cgft-llm/tree/master/vllm)

### 下载大模型

1. [huggingface-cli下载数据（含国内镜像源方法）_huggingface-cli download-CSDN博客](https://blog.csdn.net/lanlinjnc/article/details/136709225)

2. 这里使用ModelScope进行下载

```bash
pip install modelscope
modelscope download --model ZhipuAI/glm-4-9b-chat --local_dir /root/autodl-tmp/models/glm-4-9b-chat
```

3. 使用vllm进行下载

By default, vLLM downloads model from [HuggingFace](https://huggingface.co/). If you would like to use models from [ModelScope](https://www.modelscope.cn/) in the following examples, please set the environment variable:

```bash
export VLLM_USE_MODELSCOPE=True
```

### 安装VLLM

```bash
pip install vllm
```

> 使用这一个命令可以进行安装, 但是安装以后出现pytorch不能使用, 需要再次安装pytorch

### 使用

[使用vllm部署自己的大模型_vllm部署大模型-CSDN博客](https://blog.csdn.net/qq_35082030/article/details/138225284)

+ 用代码调用

```python
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams
import torch 
print(torch.cuda.is_available())
def demo1():

    prompts = [
        "Hello, my name is",
        "The president of the United States is",
        "The capital of France is",
        "The future of AI is",
    ]
    # Create a sampling params object.
    sampling_params = SamplingParams(temperature=0.8, top_p=0.95)

    # Create an LLM.
    model_path = "./vllm_model/model"  # 使用的模型
    # model_path = "./vllm_model/models/qwen2-1.5b"
    
    llm = LLM(model=model_path,
          trust_remote_code=True,
          tensor_parallel_size=1, 
          dtype=torch.float16)

    outputs = llm.generate(prompts, sampling_params)
    # Print the outputs.
    for output in outputs:
        prompt = output.prompt
        generated_text = output.outputs[0].text
        print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")


def demo2():

    # 如果遇见 OOM 现象，建议减少max_model_len，或者增加tp_size
    max_model_len, tp_size = 32768, 2
    model = "/root/autodl-tmp/models/glm-4-9b-chat"
    prompt = [{"role": "user", "content": "你好"}]

    tokenizer = AutoTokenizer.from_pretrained(model, trust_remote_code=True) 
    
    llm = LLM(
        model=model,
        tensor_parallel_size=tp_size,
        max_model_len=max_model_len,
        trust_remote_code=True,
        enforce_eager=True,
    )
    stop_token_ids = [151329, 151336, 151338]
    sampling_params = SamplingParams(temperature=0.95, max_tokens=1024, stop_token_ids=stop_token_ids)

    inputs = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)
    outputs = llm.generate(prompts=inputs, sampling_params=sampling_params)

    print(outputs[0].outputs[0].text)


if __name__ == "__main__":
    demo1()
```

+ 命令函调用

```bash
vllm serve facebook/opt-125m
```

By default, the server uses a predefined chat template stored in the tokenizer. You can override this template by using the `--chat-template` argument:

```python
vllm serve facebook/opt-125m --chat-template ./examples/template_chatml.jinja
```

## 思路

[大模型推理框架 vLLM 源码解析 PagedAttention原理详解 continueBatching策略详解-卢菁博士授课-怎么加快大模型推理_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1YfW4eDE6V/?spm_id_from=333.337.search-card.all.click&vd_source=3771cc8df803eed7244034a762706c24)

### 整理流程以及问题

 ![image-20241015093233859](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410150932135.png)

![image-20241015093324423](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410150933481.png)

![image-20241015093331603](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410150933661.png)

![image-20241015093426145](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410150934360.png)

 ![image-20241015093732837](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410150937895.png)

> 造成的根本原因是需要连续申请一个连续的内存

### 解决

1. 虚拟化内存
2. 对相同的数据进行内存共享
3. 及时清除不需要的内存
4. 动态Continue Batching, 在处理多条数据的时候batch只需要加载一次内存, 提高吞吐量

> ![image-20241015094806652](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410150948722.png)
>
> 对导致一个问题, 数据的长度是不同的, 长一点的数据需要更长的时间进行处理

![image-20241015095044335](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410150950426.png)

1. 起始位置
2. 生成一个字符, 留出下一个字符
3. 有两个到达结束位置
4. 打印两条数据

把已经END的内存直接释放

![image-20241015095252157](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410150952246.png)

加入S5, S5->prefill(可以并行但是S1, S3已经结束), 之后decode

> 新版本已经解决

在上面的图里面白色的地方还是有浪费

解决: 拼起来计算

![image-20241015095627802](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410150956886.png)

虚拟显存可以随意插入数据, 如果剩余的内存在下一次生成的时候需要的内存不够, 先暂时移出去

### 文件构成

[[vllm\]vllm架构分析 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/654659042#:~:text=vllm架构分析 1 文件目录结构 benchmark%3A 测试延迟和吞吐的脚本 ... 2 关键源码分析,Scheduler ... 6 vllm%2Fworker ... 7 vllm%2Fengine )

[vLLM (2) - 架构总览_vllm官方文档-CSDN博客](https://blog.csdn.net/daihaoguang/article/details/141284561)

[图解大模型计算加速系列：vLLM源码解析1，整体架构-CSDN博客](https://blog.csdn.net/stephen147/article/details/141193770)

[Efficient Memory Management for Large Language Model Serving with PagedAttention (arxiv.org)](https://arxiv.org/pdf/2309.06180)

### 文件目录结构

![image-20241015102942978](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410151029043.png)

+ benchmark: 测试延迟和吞吐的脚本
+ csrc: torch下的cuda扩展，一些关键kernels的cpp源码，包含了attention、[激活函数](https://zhida.zhihu.com/search?content_id=233608460&content_type=Article&match_order=1&q=激活函数&zhida_source=entity)、cache等核函数 
+ vllm/core: 关键[调度算法](https://zhida.zhihu.com/search?content_id=233608460&content_type=Article&match_order=1&q=调度算法&zhida_source=entity)，调度策略以及维护cpu和gpu映射的关系表

> 调度器的主要作用就是，在每1个推理阶段，决定要把哪些数据送给模型做推理，同时负责给这些模型分配KV Cache物理块。但要注意，它只是分配了物理块的id，而不是物理块本身。物理块的实际分配是模型在推理过程中根据物理块id来操作的，也就是CacheEngine做的事情。
> 调度器下维护着BlockSpaceManager。它负责管理BlockAllocator（实际参与分配物理块的类）。BlockAllocator又分成gpu和cpu两种类型，分别管理这两类设备上的物理块。你可能会问，cpu上的物理块是什么呢？你还记得调度器有一个swap策略吗？当gpu上显存不足时，它会把后来的请求抢占，并将其相关的KV cache物理块全部都先swap（置换、卸载）在cpu上，等后续gpu显存充足时，再把它们加载回gpu上继续做相关请求的推理。所以在cpu上我们也需要一个管控物理块的BlockAllocator。实际代码实现时，Block相关的部分可不止这两个class，还有一些更复杂的逻辑细节。

+ vllm/engine: llm的engine，包含模型配置，启动模型，请求的前后处理等, 直接处理用户的请求

> **使用的主要API**
>
> add_request()：该方法将每一个请求包装成vLLM能处理的数据类型(SequenceGroup，后面我们会详细解释)，并将其加入调度器（Scheduler）的waiting队列中。在LLMEngine中，这个函数是按照“同步”的方式设计的，也就是它被设计为“遍历batch中的每条数据，然后做相应处理”。所以这个函数本身只适合批处理场景。在异步的online serving中将会把它重写成异步的形式。
> abort_request：在推理过程中，并不是所有的请求都能有返回结果。比如客户端断开连接时，这个请求的推理就可以终止了（abort），这个函数就被用来做这个操作。
> step()：负责执行1次推理过程（1个prefill算1个次推理，每个decode各算1次推理）。在这个函数中，vLLM的调度器会决定要送那些数据去执行本次推理，并负责给这些数据分配好物理块（这些信息都被作为metadata放在要送给模型做推理的数据中）。模型会根据这些信息，采用PagedAttention方法，实际完成推理。

**启动模型**：把你的base model加载到worker上。如果你是online加载的，vLLM默认使用HuggingFace，你也可以在环境变量中把相关配置改成ModelScope。

+ vllm/entrypoints: 纯模型生成部分，只包含模型的prompt到token的这部分
+ vllm/model_executor: 模型op到layer到model组成部分以及包含了各模型的配置 vllm/transformers_utils: tokenizer的一些配置
+ vllm/worker: 负责分布式调度以及cache的分配 bloclk: 逻辑块和物理块的定义以及基本操作

```bash
vllm/
├── attention/                 # 注意力
│   ├── backends/              # 注意力各种后端实现，比如flash attention
│   ├── ops/
│   ├── layer.py
│   ├── selector.py   
│   └── __init__.py
├── core/                      # 核心，vllm最关键的部分
│   ├── block/                 # 块，为指定的序列管理物理块
│   ├── block_manager_v1.py    # 块管理器v1，管理逻辑块和物理块之间的映射关系等
│   ├── block_manager_v2.py    # 块管理器v2
│   ├── embedding_model_block_manager.py     # 针对embedding模型的块管理器
│   ├── evictor_v1.py          # 驱逐器v1，驱逐长时间未使用的物理块缓存，腾出空间
│   ├── evictor_v2.py          # 驱逐器v2
│   ├── interfaces.py
│   ├── policy.py              # 调度策略，比如fcfs（first come first serve）
│   ├── scheduler.py           # 调度器，当多个请求到来时，需要调度以高效的方式完成推理，给到用户响应
│   └── __init__.py
├── distributed/               # 分布式设备相关内容（暂不涉及）
│   ├── device_communicators/
│   ├── communication_op.py
│   ├── parallel_state.py 
│   ├── utils.py
│   └── __init__.py
├── engine/                    # 推理引擎
│   ├── output_processor/      # 输出处理器，后处理
│   ├── arg_utils.py           # 管理输入参数
│   ├── async_llm_engine.py    # 异步llm_engine，用于部署，不支持batch推理
│   ├── llm_engine.py          # llm_engine，线下推理，可以batch
│   ├── metrics.py             # 指标，记录kv_cache的使用，延迟等
│   └── __init__.py
├── entrypoints/               # 部署server相关（暂不涉及）
│   ├── openai/
│   ├── api_server.py
│   ├── llm.py 
│   └── __init__.py
├── executor/                         # 执行器
│   ├── cpu_executor.py               
│   ├── distributed_gpu_executor.py
│   ├── executor_base.py              # 执行器基类
│   ├── gpu_executor.py               # gpu执行器，比如我们使用的Nvidia单卡gpu
│   ├── multiproc_gpu_executor.py
│   ├── multiproc_worker_utils.py
│   ├── neuron_executor.py
│   ├── ray_gpu_executor.py 
│   ├── ray_utils.py
│   ├── tpu_executor.py
│   └── __init__.py
├── logging/                          # 日志
│   ├── formatter.py
│   └── __init__.py
├── lora/                             # lora相关（暂不涉及）
│   ├── fully_sharded_layers.py
│   ├── layers.py
│   ├── lora.py
│   ├── models.py 
│   ├── punica.py
│   ├── request.py
│   ├── utils.py
│   ├── worker_manager.py 
│   └── __init__.py
├── model_executor/                   # 模型执行器，主要是管理模型相关部分的          
│   ├── guided_decoding.py 
│   ├── layers.py
│   ├── models.py
│   ├── custom_op.py
│   ├── pooling_metadata.py 
│   ├── sampling_metadata.py          # 采样元数据
│   ├── utils.py 
│   └── __init__.py
├── multimodal/                       # 多模态部分（暂不涉及）
│   ├── base.py
│   ├── image.py
│   ├── registry.py 
│   ├── utils.py 
│   └── __init__.py
├── sepc_decode/                      # 投机采样（暂不涉及）
│   ├── batch_expansion.py
│   ├── interfaces.py
│   ├── metrics.py
│   ├── multi_step_worker.py 
│   ├── ngram_worker.py
│   ├── proposer_worker_base.py
│   ├── spec_decode_worker.py
│   ├── top1_proposer.py 
│   ├── utils.py 
│   └── __init__.py
├── transformers_utils/               # transformers相关的工具
│   ├── configs/ 
│   ├── tokenizers/
│   ├── tokenizer_group/
│   ├── config.py
│   ├── detokenizer.py 
│   ├── image_processor.py 
│   ├── tokenizer.py 
│   └── __init__.py
├── usage/
│   ├── usage_lib.py 
│   └── __init__.py
├── worker/                           # worker，是executor的重要组成部分
│   ├── cache_engine.py 
│   ├── cpu_model_runner.py
│   ├── cpu_worker.py
│   ├── embedding_model_runner.py
│   ├── model_runner.py               # 负责加载和执行模型，准备输入张量等
│   ├── neuron_model_runner.py 
│   ├── neuron_worker.py 
│   ├── tpu_model_runner.py 
│   ├── tpu_worker.py 
│   ├── worker.py                     # worker，使用的是gpu
│   ├── worker_base.py                # worker基类
│   └── __init__.py
├── block.py             # 块（逻辑块，物理块）定义
├── config.py            # 配置，输入参数按照功能区分构成多个配置
├── envs.py              # 环境变量相关
├── inputs.py            # 输入类定义
├── logger.py            # 日志
├── outputs.py           # 输出类定义
├── pooling_params.py     
├── py.typed
├── sampling_params.py   # 采样参数类定义
├── sequence.py          # 序列Sequence和序列组SequenceGroup等的定义
├── utils.py
├── version.py           # vllm版本
├── _C.abi3.so
├── _custom_ops.py
├── _moe_C.abi3.so
├── _punica_C.abi.so
└── __init__.py

```

## vllm和pytorch对接

