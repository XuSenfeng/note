---
layout: post
title: "vLLM" 
date:   2024-8-5 15:39:08 +0800
tags: AI 机器学习
---

# vLLM

[vllm-project/vllm: A high-throughput and memory-efficient inference and serving engine for LLMs (github.com)](https://github.com/vllm-project/vllm)

`vLLM`是伯克利大学LMSYS组织开源的[大语言模型](https://zhida.zhihu.com/search?q=大语言模型&zhida_source=entity&is_preview=1)高速推理框架，旨在极大地提升实时场景下的语言模型服务的吞吐与内存使用效率。`vLLM`是一个快速且易于使用的库，用于 LLM 推理和服务，可以和HuggingFace [无缝集成](https://zhida.zhihu.com/search?q=无缝集成&zhida_source=entity&is_preview=1)。vLLM利用了全新的注意力算法「PagedAttention」，有效地管理注意力键和值。

在实际算的时候, 每一个新获取的token需要和之前的token进行计算, 这一个部分计算在获取一个新的token的使用是重复的, 所以可以存储起来, 用空间换时间

在实际使用的时候由于不知道需要申请的数量, 只能申请一个大数组

[arxiv.org/pdf/2309.06180](https://arxiv.org/pdf/2309.06180)

在模型运行的时候, 会使用大量的内存, 这时候由于不能预测实际会使用的内存的大小, 所以会使用预设的最大值进行申请, 同时会出现申请的内存之间存在碎片, 不足已被使用, 这导致实际的内存使用率只有20%到40%

## Transform和Key-Value Cache

Transformer是一种用于自然语言处理（NLP）和其他序列到序列（sequence-to-sequence）任务的深度学习模型架构，它在2017年由Vaswani等人首次提出。Transformer架构引入了自注意力机制（self-attention mechanism），这是一个关键的创新，使其在处理序列数据时表现出色。
以下是Transformer的一些重要组成部分和特点：

+ 自注意力机制（Self-Attention）：这是Transformer的核心概念之一，它使模型能够同时考虑输入序列中的所有位置，而不是像循环神经网络（RNN）或卷积神经网络（CNN）一样逐步处理。自注意力机制允许模型根据输入序列中的不同部分来赋予不同的注意权重，从而更好地捕捉语义关系。
+ 多头注意力（Multi-Head Attention）：Transformer中的自注意力机制被扩展为多个注意力头，每个头可以学习不同的注意权重，以更好地捕捉不同类型的关系。多头注意力允许模型并行处理不同的信息子空间。
+ 堆叠层（Stacked Layers）：Transformer通常由多个相同的编码器和解码器层堆叠而成。这些堆叠的层有助于模型学习复杂的特征表示和语义。
+ 位置编码（Positional Encoding）：由于Transformer没有内置的序列位置信息，它需要额外的位置编码来表达输入序列中单词的位置顺序。
+ 残差连接和层归一化（Residual Connections and Layer Normalization）：这些技术有助于减轻训练过程中的梯度消失和爆炸问题，使模型更容易训练。
+ 编码器和解码器：Transformer通常包括一个编码器用于处理输入序列和一个解码器用于生成输出序列，这使其适用于序列到序列的任务，如机器翻译。

在实际使用的时候可以使用这一个模型生成音频, 文本等

### 原理

输入的数据会被分割为简单的token, 这一个分割可以不是按照单词进行的, 每一个token实际会对应一个向量也就是一组数据, 可以看做高位坐标里面的一个位置, 通过计算, 比较接近的数据坐标靠近

### 实际的流程

对一个输入的数据进行encoder之后把数据输入到decoder, 这两个结构可以是多个相同的结构, 但是参数不是相同的

![image-20240924105336210](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409241053267.png)

#### encoder

这一个部分从下到上可以分为输入部分, 注意力机制, 前馈神经网络

![image-20240924110907862](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409241109898.png)

+ 输入部分

embedding, 把每一个字初始化为一个512字节的向量

## 部署

尝试部署一个GLM-4的模型[THUDM/GLM-4: GLM-4 series: Open Multilingual Multimodal Chat LMs | 开源多语言多模态对话模型 (github.com)](https://github.com/THUDM/GLM-4)

### 资料记录

[Quickstart — vLLM官方文档](https://docs.vllm.ai/en/latest/getting_started/quickstart.html)

[B站教程](https://github.com/echonoshy/cgft-llm/tree/master/vllm)

### 下载大模型

1. [huggingface-cli下载数据（含国内镜像源方法）_huggingface-cli download-CSDN博客](https://blog.csdn.net/lanlinjnc/article/details/136709225)

2. 这里使用ModelScope进行下载

```bash
pip install modelscope
modelscope download --model ZhipuAI/glm-4-9b-chat --local_dir /root/autodl-tmp/models/glm-4-9b-chat
```

3. 使用vllm进行下载

By default, vLLM downloads model from [HuggingFace](https://huggingface.co/). If you would like to use models from [ModelScope](https://www.modelscope.cn/) in the following examples, please set the environment variable:

```bash
export VLLM_USE_MODELSCOPE=True
```

### 安装VLLM

```bash
pip install vllm
```

> 使用这一个命令可以进行安装, 但是安装以后出现pytorch不能使用, 需要再次安装pytorch

### 使用

[使用vllm部署自己的大模型_vllm部署大模型-CSDN博客](https://blog.csdn.net/qq_35082030/article/details/138225284)

+ 用代码调用

```python
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams
import torch 
print(torch.cuda.is_available())
def demo1():

    prompts = [
        "Hello, my name is",
        "The president of the United States is",
        "The capital of France is",
        "The future of AI is",
    ]
    # Create a sampling params object.
    sampling_params = SamplingParams(temperature=0.8, top_p=0.95)

    # Create an LLM.
    model_path = "./vllm_model/model"  # 使用的模型
    # model_path = "./vllm_model/models/qwen2-1.5b"
    
    llm = LLM(model=model_path,
          trust_remote_code=True,
          tensor_parallel_size=1, 
          dtype=torch.float16)

    outputs = llm.generate(prompts, sampling_params)
    # Print the outputs.
    for output in outputs:
        prompt = output.prompt
        generated_text = output.outputs[0].text
        print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")


def demo2():

    # 如果遇见 OOM 现象，建议减少max_model_len，或者增加tp_size
    max_model_len, tp_size = 32768, 2
    model = "/root/autodl-tmp/models/glm-4-9b-chat"
    prompt = [{"role": "user", "content": "你好"}]

    tokenizer = AutoTokenizer.from_pretrained(model, trust_remote_code=True) 
    
    llm = LLM(
        model=model,
        tensor_parallel_size=tp_size,
        max_model_len=max_model_len,
        trust_remote_code=True,
        enforce_eager=True,
    )
    stop_token_ids = [151329, 151336, 151338]
    sampling_params = SamplingParams(temperature=0.95, max_tokens=1024, stop_token_ids=stop_token_ids)

    inputs = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)
    outputs = llm.generate(prompts=inputs, sampling_params=sampling_params)

    print(outputs[0].outputs[0].text)


if __name__ == "__main__":
    demo1()
```

+ 命令函调用

```bash
vllm serve facebook/opt-125m
```

By default, the server uses a predefined chat template stored in the tokenizer. You can override this template by using the `--chat-template` argument:

```python
vllm serve facebook/opt-125m --chat-template ./examples/template_chatml.jinja
```

