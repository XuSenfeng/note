# NiN网络中的网络

**实际使用比较少**

卷积层使用的参数比较少c~i~ x c~o~ x k^2^, 但是卷积层后面的第一个全连接层参数是全部的通道乘长和宽在乘一个输出的维度, 这一个数值非常大

+ LetNet = 48K
+ AlexNet 256\*5\*5\*4096 = 26M
+ VGG 512\*7\*7\*4096 = 102M

所以需要想办法替代一下

## NiN块

一个卷积后面有两个全连接层使用步幅为1, 无填充, 输出的形状和卷积是相同的1x1卷积

起到一个全连接的作用

![image-20250114154247207](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501141542318.png)

> 这两个1×1卷积层充当带有ReLU激活函数的逐像素全连接层, 第一层的卷积窗口形状通常由用户设置。 随后的卷积窗口形状固定为1×1。

![image-20250114154327323](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501141543371.png)

实际使用的时候交替使用NiN块以及步幅为2的最大池化层逐步减小高和宽以及增大通道数, 最后使用全局平均池化得到输出

![image-20250114154707656](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501141547719.png)

- NiN使用由一个卷积层和多个1×1卷积层组成的块。该块可以在卷积神经网络中使用，以允许更多的每像素非线性。
- NiN去除了容易造成过拟合的全连接层，将它们替换为全局平均汇聚层（即在所有位置上进行求和）。该汇聚层通道数量为所需的输出数量（例如，Fashion-MNIST的输出为10）。
- 移除全连接层可减少过拟合，同时显著减少NiN的参数。
- NiN的设计影响了许多后续卷积神经网络的设计。

## 实际实现

```python
import torch
from torch import nn
from d2l import torch as d2l

def nin_block(in_channels, out_channels, kernel_size, strides, padding):
    return nn.Sequential(
        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),
        nn.ReLU(),
        nn.Conv2d(out_channels, out_channels, kernel_size=1),
        nn.ReLU(),
        nn.Conv2d(out_channels, out_channels, kernel_size=1),
        nn.ReLU()
    )
```

```python
net = nn.Sequential(
    nin_block(1, 96, kernel_size=11, strides=4, padding=0),
    nn.MaxPool2d(3, stride=2),
    nin_block(96, 256, kernel_size=5, strides=1, padding=2),
    nn.MaxPool2d(3, stride=2),
    nin_block(256, 384, kernel_size=3, strides=1, padding=1),
    nn.MaxPool2d(3, stride=2),
    nn.Dropout(0.5),
    nin_block(384, 10, kernel_size=3, strides=1, padding=1),
    nn.AdaptiveAvgPool2d((1, 1)), # 一个指定输出大小的全局平均池化层
    nn.Flatten()
)
```

+ 看一下形状

```python
X = torch.rand(size=(1, 1, 224, 224))
for layer in net:
    X = layer(X)
    print(layer.__class__.__name__, 'output shape:\t', X.shape)
"""
Sequential output shape:	 torch.Size([1, 96, 54, 54])
MaxPool2d output shape:	 torch.Size([1, 96, 26, 26])
Sequential output shape:	 torch.Size([1, 256, 26, 26])
MaxPool2d output shape:	 torch.Size([1, 256, 12, 12])
Sequential output shape:	 torch.Size([1, 384, 12, 12])
MaxPool2d output shape:	 torch.Size([1, 384, 5, 5])
Dropout output shape:	 torch.Size([1, 384, 5, 5])
Sequential output shape:	 torch.Size([1, 10, 5, 5])
AdaptiveAvgPool2d output shape:	 torch.Size([1, 10, 1, 1])
Flatten output shape:	 torch.Size([1, 10])
"""
```

