# 层和块

## 自定义模型

之前使用的是nn.Sequential定义的一种特殊的Module

```python
net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))

X = torch.rand(2, 20)
net(X)
```

任意的一个层以及神经网络都是Module的一个子类

使用torch提供的类

```python
class MLP(nn.Module):
    # 初始化函数。
    def __init__(self):
        super(MLP, self).__init__()
        # 定义两个全连接层
        self.hidden = nn.Linear(20, 256)
        self.out = nn.Linear(256, 10)

    # 定义模型的前向计算，即如何根据输入X计算返回所需要的模型输出
    def forward(self, X):
        return self.out(F.relu(self.hidden(X)))
    
net = MLP()
net(X)
```

使用以上的模式可以实现一样的功能

之前使用的Sequential也可以实现

```python
class MySequential(nn.Module):
    def __init__(self, *args):
        super(MySequential, self).__init__()
        for block in args:
            # 这里，`block`是`Module`子类的一个实例。我们把它保存在'Module'类的成员变量
            # `_modules` 中。'_modules' 是一个从字符串属性名映射到Module的字典。
            self._modules[block] = block

    def forward(self, X):
        # OrderedDict保证了按照成员添加的顺序遍历它们
        for block in self._modules.values():
            X = block(X)
        return X
    
net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))
net(X)
```

在实际使用的时候, 可以在Module里面使用Sequential, 也可以在Sequential的参数里面直接使用一个Module

### 参数管理

在实际训练的时候可以使用方法`.state_dict()`获取一个参数的字典

```python
net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))
X = torch.rand(size=(2, 4))
net(X)
print(net[2].state_dict())
"""
OrderedDict([('weight', tensor([[-0.2906, -0.0455, -0.0842,  0.2922, -0.2123, -0.0884, -0.2132, -0.2961]])), ('bias', tensor([0.1226]))])
"""
net.state_dict()['2.bias'].data
"""
net.state_dict()['2.bias'].data
"""
```

+ 可以单独的访问某一个参数

```python
print(type(net[2].bias))
print(net[2].bias)
print(net[2].bias.data)
"""
<class 'torch.nn.parameter.Parameter'>
Parameter containing:
tensor([0.1226], requires_grad=True)
tensor([0.1226])
"""
```

+ 也可以看参数的梯度

```python
net[2].bias.grad == None #还没有计算
```

+ 遍历参数

```python
print(*[(name, param.shape) for name, param in net[0].named_parameters()])
print(*[(name, param.shape) for name, param in net.named_parameters()])
"""
('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))
('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))
"""
```

+ 嵌套的获取参数

```python
def block1():
    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 4), nn.ReLU())

def block2():
    net = nn.Sequential()
    for i in range(4):
        net.add_module(f'block {i}', block1())
    return net

rgnet = nn.Sequential(block2(), nn.Linear(4, 1))
rgnet(X)
```

> 可以通过直接打印的方式获取一下这一个的实际模型样式
>
> ```python
> print(rgnet)
> """
> Sequential(
>   (0): Sequential(
>     (block 0): Sequential(
>       (0): Linear(in_features=4, out_features=8, bias=True)
>       (1): ReLU()
>       (2): Linear(in_features=8, out_features=4, bias=True)
>       (3): ReLU()
>     )
>     (block 1): Sequential(
>       (0): Linear(in_features=4, out_features=8, bias=True)
>       (1): ReLU()
>       (2): Linear(in_features=8, out_features=4, bias=True)
>       (3): ReLU()
>     )
>     (block 2): Sequential(
>       (0): Linear(in_features=4, out_features=8, bias=True)
>       (1): ReLU()
>       (2): Linear(in_features=8, out_features=4, bias=True)
>       (3): ReLU()
>     )
>     (block 3): Sequential(
>       (0): Linear(in_features=4, out_features=8, bias=True)
>       (1): ReLU()
>       (2): Linear(in_features=8, out_features=4, bias=True)
>       (3): ReLU()
>     )
>   )
>   (1): Linear(in_features=4, out_features=1, bias=True)
> )
> """
> # 获取 模型参数
> print(rgnet[0][1][0].bias.data)
> """
> tensor([ 0.2361,  0.1758, -0.4468, -0.2908, -0.0806, -0.0225, -0.1792,  0.0419])
> """
> ```

### 初始化参数

```python
def init_normal(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, mean=0, std=0.01)
        nn.init.zeros_(m.bias)

net.apply(init_normal)
net[0].weight.data[0], net[0].bias.data[0]
```

可以在不同的层使用不同的初始化参数

```python
def xavier(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight) # 使用均匀分布初始化, uniform是均匀分布, axvier是正态分布

def init_42(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight, 42)

net[0].apply(xavier)
net[2].apply(init_42)
print(net[0].weight.data[0])
print(net[2].weight.data)
"""
tensor([-0.1802, -0.4651,  0.5505,  0.2359])
tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])
"""
```

可以直接设置对应的位置

```python
net[0].weight.data[0] += 1
```

### 参数绑定

两个层使用同样的参数, 做一个参数的绑定

```python
shared = nn.Linear(8, 8)
net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), shared, nn.ReLU(), shared, nn.ReLU(), nn.Linear(8, 1))
net(X)
```

使用这一种的时候, 第二层以及第四层的参数的更新是相同的

## 自定义层

```python
class CenteredLayer(nn.Module):
    def __init__(self):
        super(CenteredLayer, self).__init__()

    def forward(self, X):
        return X - X.mean()
    
net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())
Y = net(torch.rand(4, 8))
```



有参数的, 参数需要使用nn.Parameter进行初始化

```python
class MyLinear(nn.Module):
    def __init__(self, in_units, units):
        super(MyLinear, self).__init__()
        # Parameter默认记录梯度
        self.weight = nn.Parameter(torch.randn(in_units, units)) 
        self.bias = nn.Parameter(torch.randn(units,))

    def forward(self, X):
        linear = torch.matmul(X, self.weight.data) + self.bias.data
        return F.relu(linear)
    
linear = MyLinear(5, 3)
linear.weight
```

> 使用Parameter类进行初始化参数, 这一个参数会被记录在Module里面, 在迭代的时候使用

## 读写文件

使用`torch.save`记录一个对象

```python
x = torch.arange(10)
torch.save(x, 'x-file')
x2 = torch.load('x-file')
```

实际存储的是可以记录的是一个list

```python
y = torch.zeros(4)
torch.save([x, y], 'x-files')
x2, y2 = torch.load('x-files')
```

也可以记录一个字典

```python
mydict = {'x': x, 'y': y}
torch.save(mydict, 'mydict')
mydict2 = torch.load('mydict')
```

在记录一个模型的时候, 只需要记录模型的权重, 实际记录的时候可以使用`.state_dict`参数进行存储

```python
class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.hidden = nn.Linear(20, 256)
        self.out = nn.Linear(256, 10)

    def forward(self, X):
        return self.out(F.relu(self.hidden(X)))
    
net = MLP()
X = torch.randn(size=(2, 20))
Y = net(X)
torch.save(net.state_dict(), 'mlp.params')
# 加载, 加载的时候需要使用原有的模型
clone = MLP()
clone.load_state_dict(torch.load('mlp.params'))
clone.eval() # 评估模式, 不使用dropout
Y_clone = clone(X)
Y_clone == Y
```





