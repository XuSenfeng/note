# 目标检测

检测一个图像里面出现的内容以及他的位置

实际使用的有一个边缘框, 把实际的位置框起来, 使用四个数字进行定义, 原点的坐标是在左上角, 这种的数据集一般比较小

可以使用数据集[coco数据集](cocodataset.org)

## 如何标一个框

```python
d2l.set_figsize()
img = d2l.plt.imread('../data/catdog.jpg')
d2l.plt.imshow(img)
```

> 加载一个图片并进行检测

实际的表示有四个角表示以及中心点表示, 写一个转换函数

```python
# 两种表示方式的转换
def box_corner_to_center(boxes):
    """Convert from (upper-left, lower-right) to (center, width, height)"""
    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
    cx = (x1 + x2) / 2
    cy = (y1 + y2) / 2
    w = x2 - x1
    h = y2 - y1
    # stack用于合并张量, axis=-1表示沿着最后一个维度合并
    boxes = torch.stack((cx, cy, w, h), axis=-1) 
    return boxes

def box_center_to_corner(boxes):
    """Convert from (center, width, height) to (upper-left, lower-right)"""
    cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
    x1 = cx - 0.5 * w
    y1 = cy - 0.5 * h
    x2 = cx + 0.5 * w
    y2 = cy + 0.5 * h
    boxes = torch.stack((x1, y1, x2, y2), axis=-1)
    return boxes
```

> 测试一下
>
> ```python
> dog_bbox, cat_bbox = [60.0, 45.0, 378.0, 516.0], [400.0, 112.0, 655.0, 493.0]
> boxes = torch.tensor((dog_bbox, cat_bbox))
> box_center_to_corner(box_corner_to_center(boxes)) == boxes
> ```

实际绘制一下

```python
def bbox_to_rect(bbox, color):
    # Convert the bounding box (top-left x, top-left y, bottom-right x,
    # bottom-right y) format to matplotlib format: ((upper-left x, upper-left y),
    # width, height)
    return d2l.plt.Rectangle(
        xy=(bbox[0], bbox[1]), 
        width=bbox[2] - bbox[0], height=bbox[3] - bbox[1],
        fill=False, edgecolor=color, linewidth=2)

fig = d2l.plt.imshow(img)
fig.axes.add_patch(bbox_to_rect(dog_bbox, 'blue'))
fig.axes.add_patch(bbox_to_rect(cat_bbox, 'red'))
```

![image-20250119204810105](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202501192048311.png)

## 构建数据集

```python
import os
import pandas as pd
import torchvision
```

+ 读取一下数据集

```python
def read_data_bananas(is_train=True):
    data_dir = "../data/bananas"
    csv_fname = os.path.join(data_dir, 'bananas_train' if is_train else 'bananas_val',\
                            'label.csv')
    csv_data = pd.read_csv(csv_fname)
    csv_data = csv_data.set_index('img_name')
    images, targets = [], []
    # iterrows()返回一个迭代器，每次迭代返回一个元组，
    # 元组的第一个元素是行索引，第二个元素是该行的数据
    for img_name, target in csv_data.iterrows():
        # 使用第一个数据加载图片
        images.append(torchvision.io.read_image(
            os.path.join(data_dir, 'bananas_train' if is_train else 'bananas_val', "images", img_name)))
        targets.append(list(target))
    # unsqueeze(1)表示在第二个维度上增加一个维度, 用于记录不同的检测结果
    # / 256表示将像素值缩放到0到1之间
    return images, torch.tensor(targets).unsqueeze(1) / 256
```



看一下这一个到底读取的什么数据

```python
data_dir = "../data/bananas"
csv_fname = os.path.join(data_dir, 'bananas_train',
                        'label.csv')
csv_data = pd.read_csv(csv_fname)
csv_data = csv_data.set_index('img_name')
images, targets = [], []
# iterrows()返回一个迭代器，每次迭代返回一个元组，元组的第一个元素是行索引，第二个元素是该行的数据
for img_name, target in csv_data.iterrows():
    images.append(torchvision.io.read_image(os.path.join(data_dir, 'bananas_train', "images", img_name)))
    targets.append(list(target))
print(images[0].shape, targets[1])

"""
torch.Size([3, 256, 256]) [0, 68, 175, 118, 223]
"""
```

> 前面的是图片信息, 后面的是图片的类别加位置, 使用列表进行存储

```python
class BananasDataset(torch.utils.data.Dataset):
    def __init__(self, is_train):
        self.features, self.labels = read_data_bananas(is_train)
        print('read ' + str(len(self.features)) + 
              (f' training examples' if is_train else f' validation examples'))
        
    def __getitem__(self, idx):
        return (self.features[idx].float(), self.labels[idx])
    
    def __len__(self):
        return len(self.features)
```

> 一个数据集类



+ 加载迭代器

```python
def  load_data_bananas(batch_size):
    train_iter = torch.utils.data.DataLoader(BananasDataset(is_train=True), batch_size, shuffle=True)
    val_iter = torch.utils.data.DataLoader(BananasDataset(is_train=False), batch_size)
    return train_iter, val_iter
```

测试一下效果

```python
batch_size, edge_size = 32, 256
train_iter, _ = load_data_bananas(batch_size)
batch = next(iter(train_iter))
# 标号的1表示每个有一个样本, 5表示每个样本有四个位置和一个类别
batch[0].shape, batch[1].shape 

"""
read 1000 training examples
read 100 validation examples
(torch.Size([32, 3, 256, 256]), torch.Size([32, 1, 5]))
"""
```

显示一下效果

````python
# permute(0, 2, 3, 1)表示将第0维和第3维交换位置
imgs = (batch[0][0:10].permute(0, 2, 3, 1)) / 256
axes = d2l.show_images(imgs, 2, 5, scale=2)
for ax, label in zip(axes, batch[1][0:10]):
    d2l.show_bboxes(ax, [label[0][1:5] * edge_size], colors=['w'])
````

![image-20250120112533660](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202501201125770.png)



## 锚框

首先提出多个锚框区域, 首先预测一下每一个锚框里面是否有关注的物体, 如果是预测从这一个锚框向物体边缘偏移

### IoU交并比

计算两个框之间的重合位置的大小

![image-20250120113042966](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202501201130044.png)

实际训练的时候每一个锚框是一个训练样本, 实际标注的是背景或者一个真实的边缘框(和某一个物体关联)

> 实际使用的时候检测所有值里面IoU最大的一个数值, 取出来, 之后这一个值对应的锚框以及对应的属性不再参与比较, 依次取出概率最大的为预测值

## 使用非极大值抑制NMS

每一个锚框是预测一个边缘框, NMS可以合并相似的预测

实际选中的是非背景类里面的最大预测值, 去掉所有和他的IoU值大于$\theta$的预测, 重复上面的过程直到所有的预测要不被选择, 要不被去掉

## 实际使用

![image-20250120170524887](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202501201705996.png)

> w是图片宽, h是图片高, s是大小占总图片比重, r是宽高比, 只考虑这一个组合是为了避免产生的数量过大, 这里的s1和r1是实际最适合的, 实际的每一个像素的生成的锚框数量是$nums\_size + nums\_ration - 1$

+ 为每一个像素生成若干个锚点

这一个返回的是比例, 所以可以使用一个小的数组, 之后乘图片的长宽, 获取按比例生成

```python
def multibox_prior(data, sizes, ratios):
    """生成以每个像素为中心具有不同形状的锚框。"""
    in_height, in_width = data.shape[-2:] # 输入数据的高和宽
    device, num_sizes, num_ratios = data.device, len(sizes), len(ratios)
    # 每个像素生成的锚框数量
    boxes_per_pixel = (num_sizes + num_ratios - 1) 
    size_tensor = torch.tensor(sizes, device=device)
    ratio_tensor = torch.tensor(ratios, device=device)

    offset_h, offset_w = 0.5, 0.5 # 中心像素的偏移量, 作用是将锚框的中心点放在像素的中心
    steps_h = 1.0 / in_height
    steps_w = 1.0 / in_width # 高和宽的步幅
    # 生成锚框的中心点
    center_h = (torch.arange(in_height, device=device) + offset_h) * steps_h
    center_w = (torch.arange(in_width, device=device) + offset_w) * steps_w
    # 生成以每个像素为中心具有不同形状的锚框, meshgrid函数生成两个矩阵, 
    # 一个是以center_h为行, 一个是以center_w为列
    shift_y, shift_x = torch.meshgrid(center_h, center_w)
    # 将shift_y和shift_x拉平, 作为锚框的中心点
    shift_y, shift_x = shift_y.reshape(-1), shift_x.reshape(-1)

    w = torch.cat((size_tensor * torch.sqrt(ratio_tensor[0]),
                   sizes[0] * torch.sqrt(ratio_tensor[1:])))\
                   * in_height / in_width
    h = torch.cat((size_tensor / torch.sqrt(ratio_tensor[0]),
                   sizes[0] / torch.sqrt(ratio_tensor[1:])))
    anchor_manipulations = torch.stack(
        (-w, -h, w, h)).T.repeat(in_height * in_width, 1) / 2

    out_grid = torch.stack([shift_x, shift_y, shift_x, shift_y],
                           dim=1).repeat_interleave(boxes_per_pixel, dim=0)
    output = out_grid + anchor_manipulations
    return output.unsqueeze(0)
```

实际的效果

```python
img = d2l.plt.imread('../data/catdog.jpg')
h, w = img.shape[:2]

print(h, w)
X = torch.rand(size=(1, 3, h, w))
Y = multibox_prior(X, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5])

Y.shape
"""
560 728
torch.Size([1, 2038400, 4])
"""
```

> 输出的是图片的大小以及实际的生成的锚框的数量

做一个显示锚框的函数, 可以指定锚框显示的图片, 位置, 标签以及颜色

```python
def show_bboxes(axes, bboxes, labels=None, colors=None):
    # bboxes: n * 4, n是锚框的数量

    def _make_list(obj, default_values=None):
        # 如果obj是None, 返回default_values, 否则返回obj
        if obj is None:
            obj = default_values
        elif not isinstance(obj, (list, tuple)):
            # isinstance函数用于判断obj是否是list或者tuple类型
            obj = [obj]
        return obj

    labels = _make_list(labels)
    colors = _make_list(colors, ['b', 'g', 'r', 'm', 'c'])
    for i, bbox in enumerate(bboxes):
        color = colors[i % len(colors)]
        rect = d2l.bbox_to_rect(bbox.detach().numpy(), color)
        axes.add_patch(rect)
        if labels and len(labels) > i:
            text_color = 'k' if color == 'w' else 'w'
            # 将锚框表示成matplotlib格式长方形
            axes.text(rect.xy[0], rect.xy[1], labels[i], va='center',
                      ha='center', fontsize=9, color=text_color,
                      bbox=dict(facecolor=color, lw=0))

d2l.set_figsize()
bbox_scale = torch.tensor((w, h, w, h))
fig = d2l.plt.imshow(img)
show_bboxes(fig.axes, boxes[250, 250, :, :] * bbox_scale, [
    's=0.75, r=1', 's=0.5, r=1', 's=0.25, r=1', 's=0.75, r=2', 's=0.75, r=0.5'
])
```

+ 计算两个图片的相关性

```python
def box_iou(boxes1, boxes2):
    """计算两个锚框或边界框列表中成对的交并比。"""
    box_area = lambda boxes: ((boxes[:, 2] - boxes[:, 0]) *
                              (boxes[:, 3] - boxes[:, 1]))
    areas1 = box_area(boxes1)
    areas2 = box_area(boxes2)
    inter_upperlefts = torch.max(boxes1[:, None, :2], boxes2[:, :2])
    inter_lowerrights = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])
    # clamp函数用于将小于0的值设置为0
    inters = (inter_lowerrights - inter_upperlefts).clamp(min=0) 
    inter_areas = inters[:, :, 0] * inters[:, :, 1]
    union_areas = areas1[:, None] + areas2 - inter_areas
    return inter_areas / union_areas
```

+ 找到每个锚框与其交并比最大的真实边界框，并将这些锚框的索引存储在anchors_bbox_map

ground_truth: 真实边界框的张量。

anchors: 锚框的张量。

iou_threshold: 交并比阈值，默认值为 0.5

> 没有匹配到的锚框的对应返回值是-1, 如果某个锚框有对应的真实边界框，其值为该真实边界框在 ground_truth 张量中的索引。

```python
def assign_anchor_to_bbox(ground_truth, anchors, device, iou_threshold=0.5):
    """获取每一个分类里面的最接近的那一项"""
    num_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0]
    jaccard = box_iou(anchors, ground_truth) # 计算交并比
    anchors_bbox_map = torch.full((num_anchors,), -1, dtype=torch.long,
                                  device=device) # 将返回锚框的索引设置为-1
    max_ious, indices = torch.max(jaccard, dim=1)
    # torch.nonzero函数用于返回非0元素的索引
    anc_i = torch.nonzero(max_ious >= 0.5).reshape(-1) 
    box_j = indices[max_ious >= 0.5]
    anchors_bbox_map[anc_i] = box_j
    col_discard = torch.full((num_anchors,), -1) # 将列设置为-1
    row_discard = torch.full((num_gt_boxes,), -1)
    for _ in range(num_gt_boxes):
        max_idx = torch.argmax(jaccard)
        box_idx = (max_idx % num_gt_boxes).long() # 计算最大值的列索引
        anc_idx = (max_idx / num_gt_boxes).long() # 计算最大值的行索引
        anchors_bbox_map[anc_idx] = box_idx # 将锚框的这一个对应索引设置为真实边界框的索引
        jaccard[:, box_idx] = col_discard # 将列设置为-1
        jaccard[anc_idx, :] = row_discard # 将行设置为-1
    return anchors_bbox_map
```

+ 对获取的锚框计算一下偏移的大小

给定框A和B，中心坐标分别 为(xa, ya)和(xb, yb)，宽度分别为wa和wb，高度分别为ha和hb，可以将A的偏移量标记为

![image-20250120182337742](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202501201823821.png)

其中常量的默认值为 µx = µy = µw = µh = 0, σx = σy = 0.1 ，σw = σh = 0.2。

```python
def offset_boxes(anchors, assigned_bb, eps=1e-6):
    """对锚框偏移量的转换。"""
    c_anc = d2l.box_corner_to_center(anchors)
    c_assigned_bb = d2l.box_corner_to_center(assigned_bb)
    offset_xy = 10 * (c_assigned_bb[:, :2] - c_anc[:, :2]) / c_anc[:, 2:]
    offset_wh = 5 * torch.log(eps + c_assigned_bb[:, 2:] / c_anc[:, 2:])
    offset = torch.cat([offset_xy, offset_wh], axis=1)
    return offset
```

+ 使用以上的两个技术获取到所有锚框里面有效的框以及他对应的类型和偏移

使用随机的边框以及生成的边框来进行生成预测结果, 对随机的边框进行计算种类以及偏移和掩码

```python
def multibox_target(anchors, labels):
    """使用真实边界框标记锚框。"""
    batch_size, anchors = labels.shape[0], anchors.squeeze(0)
    batch_offset, batch_mask, batch_class_labels = [], [], []
    device, num_anchors = anchors.device, anchors.shape[0]
    for i in range(batch_size):
        label = labels[i, :, :]
        anchors_bbox_map = assign_anchor_to_bbox(label[:, 1:], anchors,
                                                 device) # 将最接近的真实边界框分配给锚框
        bbox_mask = ((anchors_bbox_map >= 0).float().unsqueeze(-1)).repeat(
            1, 4)
        class_labels = torch.zeros(num_anchors, dtype=torch.long,
                                   device=device)
        assigned_bb = torch.zeros((num_anchors, 4), dtype=torch.float32,
                                  device=device)
        indices_true = torch.nonzero(anchors_bbox_map >= 0)
        bb_idx = anchors_bbox_map[indices_true]
        class_labels[indices_true] = label[bb_idx, 0].long() + 1
        assigned_bb[indices_true] = label[bb_idx, 1:]
        offset = offset_boxes(anchors, assigned_bb) * bbox_mask
        batch_offset.append(offset.reshape(-1))
        batch_mask.append(bbox_mask.reshape(-1))
        batch_class_labels.append(class_labels)
    bbox_offset = torch.stack(batch_offset)
    bbox_mask = torch.stack(batch_mask)
    class_labels = torch.stack(batch_class_labels)
    # 返回的bbox_offset, bbox_mask, class_labels的形状是(批量大小, 锚框数量 * 4), (批量大小, 锚框数量 * 1), (批量大小, 锚框数量 * 1)
    # bbox_offset是偏移量, bbox_mask是锚框的掩码, class_labels是类别标签
    return (bbox_offset, bbox_mask, class_labels)
```

下面使用这一个函数做一次处理

```python
ground_truth = torch.tensor([[0, 0.1, 0.08, 0.52, 0.92],
                             [1, 0.55, 0.2, 0.9, 0.88]]) # 两个真实边界框
anchors = torch.tensor([[0, 0.1, 0.2, 0.3], [0.15, 0.2, 0.4, 0.4],
                        [0.63, 0.05, 0.88, 0.98], [0.66, 0.45, 0.8, 0.8],
                        [0.57, 0.3, 0.92, 0.9]]) # 5个生成的锚框
 # 显示一下在图片里面的位置
fig = d2l.plt.imshow(img)
show_bboxes(fig.axes, ground_truth[:, 1:] * bbox_scale, ['dog', 'cat'], 'k')
show_bboxes(fig.axes, anchors * bbox_scale, ['0', '1', '2', '3', '4'])
```

![image-20250120193747663](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202501201937793.png)

```python
labels = multibox_target(anchors.unsqueeze(dim=0),
                         ground_truth.unsqueeze(dim=0))

labels[2] # 分类的结果, 0表示背景, 1表示狗, 2表示猫
"""
tensor([[0, 1, 2, 0, 2]])
"""
labels[1] # 掩码, 为1的表示锚框有效, 为0的表示锚框无效
"""
tensor([[0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,
         1., 1.]])
"""
labels[0] # 偏移量
"""
tensor([[-0.00e+00, -0.00e+00, -0.00e+00, -0.00e+00,  1.40e+00,  1.00e+01,
          2.59e+00,  7.18e+00, -1.20e+00,  2.69e-01,  1.68e+00, -1.57e+00,
         -0.00e+00, -0.00e+00, -0.00e+00, -0.00e+00, -5.71e-01, -1.00e+00,
          4.17e-06,  6.26e-01]])
"""
```

+ 这个函数的目的是根据预测的偏移量来调整锚框，从而得到预测的边界框。

函数通过将锚框转换为中心坐标和宽高，然后根据预测的偏移量调整这些值，最后再转换回左上角和右下角坐标，从而得到预测的边界框。这个过程涉及到多个函数的调用，包括坐标转换、指数计算和张量拼接。

```python
def offset_inverse(anchors, offset_preds):
    """根据带有预测偏移量的锚框来预测边界框。"""
    anc = d2l.box_corner_to_center(anchors)
    pred_bbox_xy = (offset_preds[:, :2] * anc[:, 2:] / 10) + anc[:, :2]
    pred_bbox_wh = torch.exp(offset_preds[:, 2:] / 5) * anc[:, 2:]
    pred_bbox = torch.cat((pred_bbox_xy, pred_bbox_wh), axis=1)
    predicted_bbox = d2l.box_center_to_corner(pred_bbox)
    return predicted_bbox
```

+ 通过排序和 IoU 计算，逐步去除重叠较大的锚框，只保留置信度高且独立的锚框。

1. 从L中选取置信度最高的预测边界框B1作为基准，然后将所有与B1的IoU超过预定阈值ϵ的非基准预测 边界框从L中移除。这时，L保留了置信度最高的预测边界框，去除了与其太过相似的其他预测边界框。 简而言之，那些具有非极大值置信度的边界框被抑制了。
2. 从L中选取置信度第二高的预测边界框B2作为又一个基准，然后将所有与B2的IoU大于ϵ的非基准预测 边界框从L中移除。 
3. 重复上述过程，直到L中的所有预测边界框都曾被用作基准。此时，L中任意一对预测边界框的IoU都小 于阈值ϵ；因此，没有一对边界框过于相似。 
4. 输出列表L中的所有预测边界框。

```python
def nms(boxes, scores, iou_threshold):
    # 去除置比较像的锚框, 以及置信度较低的锚框
    B = torch.argsort(scores, dim=-1, descending=True) # 对置信度进行排序
    keep = [] # 记录信息使用
    while B.numel() > 0:
        i = B[0]
        keep.append(i) # 取出当前置信度最高的锚框索引
        if B.numel() == 1: break
        iou = box_iou(boxes[i, :].reshape(-1, 4),
                      boxes[B[1:], :].reshape(-1, 4)).reshape(-1)
        # 将iou小于阈值的锚框索引保留
        inds = torch.nonzero(iou <= iou_threshold).reshape(-1) 
        B = B[inds + 1]
    return torch.tensor(keep, device=boxes.device)
```

+ 从一组候选边界框中选择最有可能包含目标的边界框，同时抑制那些重叠较多的冗余框。

函数的返回值是一个包含预测边界框信息的张量。具体来说，它返回一个形状为 `(batch_size, num_anchors, 6)` 的张量，其中每个元素包含以下信息：

使用的是边框以及每一个边框实际对每一个类的预测准确度, 和边框的偏移以及阈值

1. class_id：预测的类别ID。如果该边界框被抑制或置信度低于阈值，则为 -1。
2. `confidence`：预测的置信度分数。
3. predicted_bb：预测的边界框坐标，格式为 `[xmin, ymin, xmax, ymax]`。

```python
def multibox_detection(cls_probs, offset_preds, anchors, nms_threshold=0.5,
                       pos_threshold=0.009999999):
    """使用非极大值抑制来预测边界框。"""
    # 获取设备和批量大小
    device, batch_size = cls_probs.device, cls_probs.shape[0]
    # 压缩 anchors 的第一个维度
    anchors = anchors.squeeze(0)
    # 获取类别数量和锚框数量
    num_classes, num_anchors = cls_probs.shape[1], cls_probs.shape[2]
    out = []
    for i in range(batch_size):
        # 获取第 i 个样本的类别概率和偏移预测
        cls_prob, offset_pred = cls_probs[i], offset_preds[i].reshape(-1, 4)
        # 获取最大类别概率和对应的类别 ID
        conf, class_id = torch.max(cls_prob[1:], 0)
        # 通过偏移逆变换得到预测的边界框
        predicted_bb = offset_inverse(anchors, offset_pred)
        # 使用非极大值抑制得到保留的边界框索引
        keep = nms(predicted_bb, conf, nms_threshold)

        # 创建所有锚框的索引
        all_idx = torch.arange(num_anchors, dtype=torch.long, device=device)
        # 合并保留的索引和所有索引
        combined = torch.cat((keep, all_idx))
        # 获取唯一索引及其计数
        uniques, counts = combined.unique(return_counts=True)
        # 找到未保留的索引
        non_keep = uniques[counts == 1]
        # 合并保留和未保留的索引
        all_id_sorted = torch.cat((keep, non_keep))
        # 将未保留的类别 ID 设为 -1
        class_id[non_keep] = -1
        # 根据排序后的索引重新排序类别 ID 和置信度
        class_id = class_id[all_id_sorted]
        conf, predicted_bb = conf[all_id_sorted], predicted_bb[all_id_sorted]
        # 找到置信度低于阈值的索引
        below_min_idx = (conf < pos_threshold)
        # 将这些索引的类别 ID 设为 -1
        class_id[below_min_idx] = -1
        # 调整置信度
        conf[below_min_idx] = 1 - conf[below_min_idx]
        # 将类别 ID、置信度和预测的边界框拼接在一起
        pred_info = torch.cat(
            (class_id.unsqueeze(1), conf.unsqueeze(1), predicted_bb), dim=1)
        # 将结果添加到输出列表中
        out.append(pred_info)
    # 返回所有批次的结果
    return torch.stack(out)
```

+ 下面实际使用一下

```python
anchors = torch.tensor([[0.1, 0.08, 0.52, 0.92], [0.08, 0.2, 0.56, 0.95],
                        [0.15, 0.3, 0.62, 0.91], [0.55, 0.2, 0.9, 0.88]]) # 4个锚框
offset_preds = torch.tensor([0] * anchors.numel()) # 偏移量
cls_probs = torch.tensor([[0] * 4,
                          [0.9, 0.8, 0.7, 0.1],
                          [0.1, 0.2, 0.3, 0.9]]) # 类别概率, 三个类, 四个框

fig = d2l.plt.imshow(img)
show_bboxes(fig.axes, anchors * bbox_scale,
            ['dog=0.9', 'dog=0.8', 'dog=0.7', 'cat=0.9'])
```

```python
output = multibox_detection(cls_probs.unsqueeze(dim=0),
                            offset_preds.unsqueeze(dim=0),
                            anchors.unsqueeze(dim=0), nms_threshold=0.5)
output
"""
tensor([[[ 0.00,  0.90,  0.10,  0.08,  0.52,  0.92],
         [ 1.00,  0.90,  0.55,  0.20,  0.90,  0.88],
         [-1.00,  0.80,  0.08,  0.20,  0.56,  0.95],
         [-1.00,  0.70,  0.15,  0.30,  0.62,  0.91]]])
"""
```

+ 绘制

```python
fig = d2l.plt.imshow(img)
for i in output[0].detach().numpy():
    if i[0] == -1:
        continue
    label = ('dog=', 'cat=')[int(i[0])] + str(i[1])
    show_bboxes(fig.axes, [torch.tensor(i[2:]) * bbox_scale], label)
```

![image-20250120195627246](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202501201956401.png)

## 多尺度目标检测

如果为每个像素都生成的锚框，我们最终可能会得到太多需要计算的锚框。我们可以在输入图像中均匀采样一小部分像素，并以它们为中心生成锚框。

```python
def display_anchors(fmap_w, fmap_h, s):
    d2l.set_figsize()
    # 前两个维度上的值不影响输出
    fmap = torch.zeros((1, 10, fmap_h, fmap_w))
    anchors = multibox_prior(fmap, sizes=s, ratios=[1, 2, 0.5])
    bbox_scale = torch.tensor((w, h, w, h))
    show_bboxes(d2l.plt.imshow(img).axes,
        anchors[0] * bbox_scale) # 显示一下

display_anchors(fmap_w=4, fmap_h=4, s=[0.15]) # 按照4*4的生成
```

![image-20250124225408398](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202501242254605.png)

```python
display_anchors(fmap_w=2, fmap_h=2, s=[0.4])
```

![image-20250124225737928](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202501242257099.png)

## 单发多框检测SSD

此模型主要由基础网络组成，其后是几个多尺度特征块。基本网 络用于从输入图像中提取特征，因此它可以使用深度卷积神经网络。

每个多尺度特征块 将上一层提供的特征图的高和宽缩小（如减半），并使特征图中每个单元在输入图像上的感受野变得更广阔。

![image-20250124230844742](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202501242308903.png)

### 类别预测层

设目标类别的数量为q。这样一来，锚框有q + 1个类别，其中0类是背景。

设特征图的高和宽 分别为h和w。如果以其中每个单元为中心生成a个锚框，那么我们需要对hwa个锚框进行分类。如果使用全 连接层作为输出，很容易导致模型参数过多。

类别预测层使用一个保持输入高和宽的卷积层。这样一来，输出和输入在特征图宽和高上的空间 坐标一一对应。考虑输出和输入同一空间坐标$（x、y）$：输出特征图上（x、y）坐标的通道里包含了以输入特征图$（x、y）$坐标为中心生成的所有锚框的类别预测。因此输出通道数为$a(q + 1)$，其中索引为$i(q + 1) + j （0 ≤ j ≤ q）$的通道代表了索引为i的锚框有关类别索引为j的预测。

```python
def cls_predictor(num_inputs, num_anchors, num_classes):
	return nn.Conv2d(num_inputs, 
                     num_anchors * (num_classes +
                     1),kernel_size=3, padding=1)
```

### 连结多尺度的预测

实际的输出大小和每一个图片的锚框数, 批量大小, 类别数, 图片的大小, 有关

````python
def forward(x, block):
return block(x)
Y1 = forward(torch.zeros((2, 8, 20, 20)), cls_predictor(8, 5, 10))
Y2 = forward(torch.zeros((2, 16, 10, 10)), cls_predictor(16, 3, 10))
Y1.shape, Y2.shape
"""
(torch.Size([2, 55, 20, 20]), torch.Size([2, 33, 10, 10]))
第二维的大小是输出的(种类+1)*锚框的大小, 后面的是图片的
"""
````

