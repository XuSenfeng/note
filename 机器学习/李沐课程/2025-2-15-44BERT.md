# BERT

## NLP迁移学习

使用预训练的模型抽取词和句子特征, 比如word2vec, 不更新训练好的模型, 需要构建新的网络抓取新任务使用的信息

> 替换掉embed层

BERT是一个基于微调的NLP模型, 预训练了足够多的信息, 新的任务只需要加一个简单的输出层, 实际是一个只有编码器的Transformer

![img](https://pic1.zhimg.com/v2-bf3ee1a496cde5b6707d2ca2ffaccdda_1440w.jpg)

这个模型有两个版本, 改变的是block的数量以及hidden size, head的大小和参数的数量

## 对输入的修改

每个输入的句子是一个句子对(原来的两个句子需要分别进输入和输出), 句子的开头和分割加入一个分隔符

Embedding由三种Embedding求和而成：

![img](https://pica.zhimg.com/v2-86cc5ff7f5295dbf9587371034ba7abe_1440w.jpg)

- Token Embeddings是词向量，第一个单词是CLS标志，可以用于之后的分类任务

通过建立字向量表将每个字转换成一个一维向量，作为模型输入。特别的，英文词汇会做更细粒度的切分，比如playing 或切割成 play 和 ##ing，中文目前尚未对输入文本进行分词，直接对单子构成为本的输入单位。将词切割成更细粒度的 Word Piece 是为了解决未登录词的常见方法。

假如输入文本 ”I like dog“。下图则为 Token Embeddings 层实现过程。输入文本在送入 Token Embeddings 层之前要先进性 tokenization 处理，且两个特殊的 Token 会插入在文本开头 [CLS] 和结尾 [SEP]。[CLS]表示该特征用于分类模型，对非分类模型，该符号可以省去。[SEP]表示分句符号，用于断开输入语料中的两个句子。

> 使用这两个是为了处理一个NSP分类任务, 使用[CLS]的标志, 其表示整个文本序列的分类信息。在训练过程中，将该[CLS]标志的输出向量传输到一个分类器（如softmax层），从而使BERT模型能够应用于文本分类任务
>
> 通过将文本序列的输出向量与分类器进行连接，BERT模型可以对输入文本进行分类，并根据不同任务对[CLS]标志的输出进行微调。这种方式使得BERT模型可以适应各种分类任务，如情感分析、文本分类、问题回答等。
>
> 这个向量不可以用于作为整个句子的语义

Bert 在处理英文文本时只需要 30522 个词，Token Embeddings 层会将每个词转换成 768 维向量，例子中 5 个Token 会被转换成一个 (6, 768) 的矩阵或 (1, 6, 768) 的张量。

![img](https://pic2.zhimg.com/v2-817b0e44f9afe2d3db8672c1cfbcb809_1440w.jpg)

- Segment Embeddings用来区别两种句子，因为预训练不光做LM还要做以两个句子为输入的分类任务

Bert 能够处理句子对的分类任务，这类任务就是判断两个文本是否是语义相似的。句子对中的两个句子被简单的拼接在一起后送入模型中，Bert 如何区分一个句子对是两个句子呢？答案就是 Segment Embeddings。

Segement Embeddings 层有两种向量表示，前一个向量是把 0 赋值给第一个句子的各个 Token，后一个向量是把1赋值给各个 Token，问答系统等任务要预测下一句，因此输入是有关联的句子。而文本分类只有一个句子，那么 Segement embeddings 就全部是 0。

![img](https://pic1.zhimg.com/v2-888e81eea8ec993cbf676c9dd6750b50_1440w.jpg)

- Position Embeddings和之前文章中的Transformer不一样，不是三角函数而是学习出来的

由于出现在文本不同位置的字/词所携带的语义信息存在差异(如 ”你爱我“ 和 ”我爱你“)，你和我虽然都和爱字很接近，但是位置不同，表示的含义不同。

在 RNN 中，第二个 ”I“ 和 第一个 ”I“ 表达的意义不一样，因为它们的隐状态不一样。对第二个 ”I“ 来说，隐状态经过 ”I think therefore“ 三个词，包含了前面三个词的信息，而第一个 ”I“ 只是一个初始值。因此，RNN 的隐状态保证在不同位置上相同的词有不同的输出向量表

![img](https://pic2.zhimg.com/v2-bcba030285047b347d73721e415d3dad_1440w.jpg)

Transformer 中通过植入关于 Token 的相对位置或者绝对位置信息来表示序列的顺序信息。作者测试用学习的方法来得到 Position Embeddings，最终发现固定位置和相对位置效果差不多，所以最后用的是固定位置的，而正弦可以处理更长的 Sequence，且可以用前面位置的值线性表示后面的位置。

BERT 中处理的最长序列是 512 个 Token，长度超过 512 会被截取，BERT 在各个位置上学习一个向量来表示序列顺序的信息编码进来，这意味着 Position Embeddings 实际上是一个 (512, 768) 的 lookup 表，表第一行是代表第一个序列的每个位置，第二行代表序列第二个位置。

## 预训练

分为两个部分, MLM和NSP两个任务, 同时进行

在进行自训练的时候主要有两种方式

+ MLM是指Masked Language Model

AR（AutoRegressive）模型是一种时间序列模型，它依赖于过去时间步的输入来预测未来的输出。AR模型通常用于时间序列数据的预测和建模，其中当前时间步的输出与前面若干个时间步的输入相关。

> 只使用单侧的信息

AE（AutoEncoder）是一种神经网络模型，用于学习数据的隐藏表示或特征。自编码器包括编码器和解码器两部分，通过将输入数据压缩成低维编码然后重构原始数据来学习数据的有效表示。AE广泛应用于特征提取、降维、去噪和生成模型等任务。在自然语言处理领域，自编码器也经常用于学习文本的表示和特征。

> 可以使用两侧的信息, 但是mask的部分可能选取以后可以的输出不唯一

![image-20250217164217268](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502171642363.png)

实际使用的是AE的方式, mask15%的单词, 这里面10%被替换, 10%不动, 80%被替换为mask

> 这么做主要的原因是为了使得微调的时候任务不出现mask也可以进行

+ NLP是Natural Language Processin, 实际是对文本二分类

使用从同一个文档里面的相邻的句子是正样本, 不同的文档里面的句子负样本, 区分两个句子是不是有关

> 主题和连贯性预测合并为同一个单项任务, 对CLS的输出进行使用

## 实际使用

![img](https://pica.zhimg.com/v2-8cbd2f07c18b85db855b32c424d5177e_1440w.jpg)

> 分类任务使用的cls标志的输出, 问答使用的是最好的输出, 成分划分使用的是出来CLS以外的输出

微调使用

1. 大量数据进行预训练Pertrain
2. 相同的领域继续训练, Domain trasfer
3. 任务相关的小数据集里面继续训练 Task transfer
4. 任务相关数据训练Fine-tune

## 实现

抽取特征的位置

```python
#@save
class BERTEncoder(nn.Module):
    """BERT编码器"""
    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,
            ffn_num_hiddens, num_heads, num_layers, dropout,
            max_len=1000, key_size=768, query_size=768, value_size=768,
            **kwargs):
        super(BERTEncoder, self).__init__(**kwargs)
        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)
        self.segment_embedding = nn.Embedding(2, num_hiddens) # 两个片段的嵌入
        self.blks = nn.Sequential() # 创建多个BERT块
        for i in range(num_layers):
            self.blks.add_module(f"{i}", d2l.EncoderBlock(
                key_size, query_size, value_size, num_hiddens, norm_shape,
                ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))
        # 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数
        self.pos_embedding = nn.Parameter(torch.randn(1, max_len,
            num_hiddens))

    def forward(self, tokens, segments, valid_lens):
        # 在以下代码段中，X的形状保持不变：（批量大小，最大序列长度，num_hiddens）
        X = self.token_embedding(tokens) + self.segment_embedding(segments)
        X = X + self.pos_embedding.data[:, :X.shape[1], :]
        for blk in self.blks:
            X = blk(X, valid_lens)
        return X
    
vocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4
norm_shape, ffn_num_input, num_layers, dropout = [768], 768, 2, 0.2
encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input,
        ffn_num_hiddens, num_heads, num_layers, dropout)

tokens = torch.randint(0, vocab_size, (2, 8)) # 两个样本的词元
# 两个样本的片段标记
segments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])
encoded_X = encoder(tokens, segments, None)
encoded_X.shape
"""
torch.Size([2, 8, 768])
"""
```

用于计算mask的位置

```python
#@save
class MaskLM(nn.Module):
    """BERT的掩蔽语言模型任务"""
    def __init__(self, vocab_size, num_hiddens, num_inputs=768, **kwargs):
        super(MaskLM, self).__init__(**kwargs)
        self.mlp = nn.Sequential(
                        nn.Linear(num_inputs, num_hiddens),
                        nn.ReLU(),
                        nn.LayerNorm(num_hiddens), # 通过层归一化
                        nn.Linear(num_hiddens, vocab_size))
    # 它需要两个输入：BERTEncoder的编码结果和用于预测的词元位置。
    # 输出是这些位置的预测结果
    def forward(self, X, pred_positions):
        num_pred_positions = pred_positions.shape[1] # 预测的词元数
        pred_positions = pred_positions.reshape(-1) # 展平
        batch_size = X.shape[0]
        batch_idx = torch.arange(0, batch_size)
        # 假设batch_size=2，num_pred_positions=3(2行, 每行3个位置)
        # 那么batch_idx是np.array（[0,0,0,1,1,1]）是一个2x3的数量, 和pred_positions配合获取六个位置的词元
        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions) # 重复batch_idx
        masked_X = X[batch_idx, pred_positions] # 先按batch取行，再按pred_positions取列
        masked_X = masked_X.reshape((batch_size, num_pred_positions,-1))
        mlm_Y_hat = self.mlp(masked_X)
        return mlm_Y_hat
```

```python
# 对vocab_size个词元的预测
mlm = MaskLM(vocab_size, num_hiddens)
mlm_positions = torch.tensor([[1, 5, 2], [6, 1, 5]])
# 输入的X是编码器的输出，形状为(2, 8, 768), 输出是对应位置的预测结果(每一行三个)
mlm_Y_hat = mlm(encoded_X, mlm_positions)
# batch 2, 预测3个值, 输出的大小是vocab_size个词的预测
mlm_Y_hat.shape
"""
torch.Size([2, 3, 10000])
"""
```

可以使用交叉熵计算损失函数

```python
mlm_Y = torch.tensor([[7, 8, 9], [10, 20, 30]])
loss = nn.CrossEntropyLoss(reduction='none')
mlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))
mlm_l.shape
"""
torch.Size([6])
"""
```

使用cls的输出判断是不是连续的

```python
#@save
class NextSentencePred(nn.Module):
    """BERT的下一句预测任务"""
    def __init__(self, num_inputs, **kwargs):
        super(NextSentencePred, self).__init__(**kwargs)
        self.output = nn.Linear(num_inputs, 2)
    def forward(self, X):
        # X的形状：(batchsize,num_hiddens)
        return self.output(X)
```

两个batch输出的是两个预期的预测值

```python
encoded_X = torch.flatten(encoded_X, start_dim=1)
# NSP的输入形状:(batchsize，num_hiddens)
nsp = NextSentencePred(encoded_X.shape[-1])
nsp_Y_hat = nsp(encoded_X)
nsp_Y_hat.shape
```

建立完整的模型

```python
class BERTModel(nn.Module):
    """BERT模型"""
    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,
        ffn_num_hiddens, num_heads, num_layers, dropout,
        max_len=1000, key_size=768, query_size=768, value_size=768,
        hid_in_features=768, mlm_in_features=768,
        nsp_in_features=768):
        super(BERTModel, self).__init__()
        self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape,
        ffn_num_input, ffn_num_hiddens, num_heads, num_layers,
            dropout, max_len=max_len, key_size=key_size,
            query_size=query_size, value_size=value_size)
        self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens),
            nn.Tanh())
        self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)
        self.nsp = NextSentencePred(nsp_in_features)
    def forward(self, tokens, segments, valid_lens=None,
    pred_positions=None):
        encoded_X = self.encoder(tokens, segments, valid_lens)
        if pred_positions is not None:
            mlm_Y_hat = self.mlm(encoded_X, pred_positions)
        else:
            mlm_Y_hat = None
        # 用于下一句预测的多层感知机分类器的隐藏层，0是“<cls>”标记的索引
        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))
        return encoded_X, mlm_Y_hat, nsp_Y_hat
```

```python
net = d2l.BERTModel(len(vocab), num_hiddens=128, norm_shape=[128],
        ffn_num_input=128, ffn_num_hiddens=256, num_heads=2,
        num_layers=2, dropout=0.2, key_size=128, query_size=128,
        value_size=128, hid_in_features=128, mlm_in_features=128,
        nsp_in_features=128)
devices = d2l.try_all_gpus()
loss = nn.CrossEntropyLoss()
```

一个计算损失的函数

```python
#@save
def _get_batch_loss_bert(net, loss, vocab_size, tokens_X,
        segments_X, valid_lens_x,
        pred_positions_X, mlm_weights_X,
        mlm_Y, nsp_y):
    # 前向传播
    _, mlm_Y_hat, nsp_Y_hat = net(tokens_X, segments_X,
    valid_lens_x.reshape(-1),
    pred_positions_X)
    # 计算遮蔽语言模型损失
    mlm_l = loss(mlm_Y_hat.reshape(-1, vocab_size), mlm_Y.reshape(-1)) *\
    mlm_weights_X.reshape(-1, 1)
    mlm_l = mlm_l.sum() / (mlm_weights_X.sum() + 1e-8)
    # 计算下一句子预测任务的损失
    nsp_l = loss(nsp_Y_hat, nsp_y)
    l = mlm_l + nsp_l
    return mlm_l, nsp_l, l
```

训练函数

````python
def train_bert(train_iter, net, loss, vocab_size, devices, num_steps):
    net = nn.DataParallel(net, device_ids=devices).to(devices[0])
    trainer = torch.optim.Adam(net.parameters(), lr=0.01)
    step, timer = 0, d2l.Timer()
    animator = d2l.Animator(xlabel='step', ylabel='loss',
    xlim=[1, num_steps], legend=['mlm', 'nsp'])
    # 遮蔽语言模型损失的和，下一句预测任务损失的和，句子对的数量，计数
    metric = d2l.Accumulator(4)
    num_steps_reached = False
    while step < num_steps and not num_steps_reached:
        for tokens_X, segments_X, valid_lens_x, pred_positions_X,\
            mlm_weights_X, mlm_Y, nsp_y in train_iter:
            tokens_X = tokens_X.to(devices[0])
            segments_X = segments_X.to(devices[0])
            valid_lens_x = valid_lens_x.to(devices[0])
            pred_positions_X = pred_positions_X.to(devices[0])
            mlm_weights_X = mlm_weights_X.to(devices[0])
            mlm_Y, nsp_y = mlm_Y.to(devices[0]), nsp_y.to(devices[0])
            trainer.zero_grad()
            timer.start()
            mlm_l, nsp_l, l = _get_batch_loss_bert(
                net, loss, vocab_size, tokens_X, segments_X, valid_lens_x,
                pred_positions_X, mlm_weights_X, mlm_Y, nsp_y)
            l.backward()
            trainer.step()
            metric.add(mlm_l, nsp_l, tokens_X.shape[0], 1)
            timer.stop()
            animator.add(step + 1,
            (metric[0] / metric[3], metric[1] / metric[3]))
            step += 1
            if step == num_steps:
                num_steps_reached = True
                break
    print(f'MLM loss {metric[0] / metric[3]:.3f}, '
    f'NSP loss {metric[1] / metric[3]:.3f}')
    print(f'{metric[2] / timer.sum():.1f} sentence pairs/sec on '
    f'{str(devices)}')
````

一个获取encode的结果的函数

```python
def get_bert_encoding(net, tokens_a, tokens_b=None):
    tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)
    token_ids = torch.tensor(vocab[tokens], device=devices[0]).unsqueeze(0)
    segments = torch.tensor(segments, device=devices[0]).unsqueeze(0)
    valid_len = torch.tensor(len(tokens), device=devices[0]).unsqueeze(0)
    encoded_X, _, _ = net(token_ids, segments, valid_len)
    return encoded_X
```

