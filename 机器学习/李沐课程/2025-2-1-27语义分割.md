# 语义分割

语义分割把图片里面的每一个像素分类到对应的类

![image-20250201171348079](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502011713011.png)

实际的应用可以有背景虚化, 还有在无人驾驶的时候分割出来路面

还有两个类似的定义

• **图像分割**将图像划分为若干组成区域，这类问题的方法通常利用图像中像素之间的相关性。它在训练 时不需要有关图像像素的标签信息，在预测时也无法保证分割出的区域具有我们希望得到的语义。以 图13.9.1中的图像作为输入，图像分割可能会将狗分为两个区域：一个覆盖以黑色为主的嘴和眼睛，另 一个覆盖以黄色为主的其余部分身体。

 • **实例分割**也叫同时检测并分割（simultaneousdetectionandsegmentation），它研究如何识别图像中 各个目标实例的像素级区域。与语义分割不同，实例分割不仅需要区分语义，还要区分不同的目标实 例。例如，如果图像中有两条狗，则实例分割需要区分像素属于的两条狗中的哪一条。

## 示例

使用的是Pascal VOC2012这一个数据集

### 数据集加载

```python
%matplotlib inline
import os
import torch
import torchvision
from d2l import torch as d2l
```

+ 下载数据集

```python
d2l.DATA_HUB['voc2012'] = (d2l.DATA_URL + 'VOCtrainval_11-May-2012.tar',
 '4e443f8a2eca6b1dac8a6c57641b67dd40621a49')
voc_dir = d2l.download_extract('voc2012', 'VOCdevkit/VOC2012')
```

+ 读取所有的图片加载到内存里面, 并且显示一下效果

```python
def read_voc_images(voc_dir, is_train=True):
    """读取所有VOC图像并标注"""
    txt_fname = os.path.join(voc_dir, 'ImageSets', 'Segmentation',
        'train.txt' if is_train else 'val.txt') # 使用这两个文件记录训练和测试样本
    mode = torchvision.io.image.ImageReadMode.RGB
    with open(txt_fname, 'r') as f:
        images = f.read().split()
    features, labels = [], []
    for i, fname in enumerate(images):
        features.append(torchvision.io.read_image(os.path.join(
            voc_dir, 'JPEGImages', f'{fname}.jpg')))
        labels.append(torchvision.io.read_image(os.path.join(
            voc_dir, 'SegmentationClass' ,f'{fname}.png'), mode))
    return features, labels

train_features, train_labels = read_voc_images(voc_dir, True)
n = 5
imgs = train_features[0:n] + train_labels[0:n]
imgs = [img.permute(1,2,0) for img in imgs]
d2l.show_images(imgs, 2, n)
```

![image-20250203194612033](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502031946730.png)

+ 定义一下分类使用的颜色和类别的对应

```python
#@save 不同的类别使用不同的颜色来标注
VOC_COLORMAP = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],
            [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],
            [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],
            [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],
            [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],
            [0, 64, 128]]
#@save
VOC_CLASSES = ['background', 'aeroplane', 'bicycle', 'bird', 'boat',
            'bottle', 'bus', 'car', 'cat', 'chair', 'cow',
            'diningtable', 'dog', 'horse', 'motorbike', 'person',
            'potted plant', 'sheep', 'sofa', 'train', 'tv/monitor']
```

+ 实际转换使用的函数

```python
#@save
def voc_colormap2label():
    """构建从RGB到VOC类别索引的映射, 使用一个长度为256^3的数组, 某一个RGB值的索引为其类别索引"""
    colormap2label = torch.zeros(256 ** 3, dtype=torch.long)
    for i, colormap in enumerate(VOC_COLORMAP):
        colormap2label[(colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i
    return colormap2label

#@save
def voc_label_indices(colormap, colormap2label):
    """将VOC标签中的RGB值映射到它们的类别索引"""
    colormap = colormap.permute(1, 2, 0).numpy().astype('int32')
    idx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256 + colormap[:, :, 2]) # 从通道维度取出来值相加
    return colormap2label[idx]
```

```python
y = voc_label_indices(train_labels[0], voc_colormap2label())
y[105:115, 130:140], VOC_CLASSES[1]

"""
(tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
         [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]),
 'aeroplane')
"""
```

+ 进行一下增广, 随机裁剪

```python
def voc_rand_crop(feature, label, height, width):
    """随机裁剪特征和标签图像"""
    rect = torchvision.transforms.RandomCrop.get_params(feature, (height, width))
    feature = torchvision.transforms.functional.crop(feature, *rect)
    label = torchvision.transforms.functional.crop(label, *rect)
    return feature, label
```

+ 显示一下效果

````python
imgs = []
for _ in range(n):
    imgs += voc_rand_crop(train_features[0], train_labels[0], 200, 300)
imgs = [img.permute(1, 2, 0) for img in imgs]
d2l.show_images(imgs[::2] + imgs[1::2], 2, n)
````

![image-20250203194633611](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502031946692.png)

+ 加载为数据集

```python
class VOCSegDataset(torch.utils.data.Dataset):
    """一个用于加载VOC数据集的自定义数据集"""
    def __init__(self, is_train, crop_size, voc_dir):
        self.transform = torchvision.transforms.Normalize(
            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        self.crop_size = crop_size
        features, labels = read_voc_images(voc_dir, is_train=is_train) # 读取所有VOC图像和标签
        self.features = [self.normalize_image(feature)
            for feature in self.filter(features)]
        self.labels = self.filter(labels)
        self.colormap2label = voc_colormap2label() # 获取一个索引表
        print('read ' + str(len(self.features)) + ' examples')

    def normalize_image(self, img):
        return self.transform(img.float() / 255) # 标准化图像
    
    def filter(self, imgs):
        return [img for img in imgs if (
            img.shape[1] >= self.crop_size[0] and
            img.shape[2] >= self.crop_size[1])] # 过滤掉小于裁剪尺寸的图像
    
    def __getitem__(self, idx):
        feature, label = voc_rand_crop(self.features[idx], self.labels[idx],
            *self.crop_size) # 随机裁剪
        return (feature, voc_label_indices(label, self.colormap2label)) # 返回特征和标签
    
    def __len__(self):
        return len(self.features)
    
crop_size = (320, 480)
voc_train = VOCSegDataset(True, crop_size, voc_dir)
voc_test = VOCSegDataset(False, crop_size, voc_dir)

batch_size = 64
train_iter = torch.utils.data.DataLoader(voc_train, batch_size, shuffle=True,
                                        drop_last=True,
                                        num_workers=d2l.get_dataloader_workers())
for X, Y in train_iter:
    print(X.shape)
    print(Y.shape)
    break
        
"""
 torch.Size([64, 3, 320, 480])
 torch.Size([64, 320, 480])
"""
```

+ 实际使用

````python
def load_data_voc(batch_size, crop_size):
    """加载VOC语义分割数据集"""
    voc_dir = d2l.download_extract('voc2012', os.path.join(
        'VOCdevkit', 'VOC2012'))
    num_workers = d2l.get_dataloader_workers()
    train_iter = torch.utils.data.DataLoader(
    VOCSegDataset(True, crop_size, voc_dir), batch_size,
        shuffle=True, drop_last=True, num_workers=num_workers)
    test_iter = torch.utils.data.DataLoader(
        VOCSegDataset(False, crop_size, voc_dir), batch_size,
        drop_last=True, num_workers=num_workers)
    return train_iter, test_iter
````

## 旋转卷积

卷积的时候一般不会增大输入的高和宽, 通常是不变或者减半, 但是旋转卷积可以用于增大输入的高和宽

语义分割需要对每一个像素进行处理, 只是减小的话不太使用

![image-20250203182338402](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502031823860.png)

padding实在输出上面做填充, 但会导致实际的输出变小(1会使得最外面的一圈消失), stride还是相同的做法

> 起这个名字的原因是卷积可以看作是一次矩阵乘法, Y^'^ = VX^'^, 这里的V是mxn, 则Y^'^是n, X^'^是m, 旋转卷积乘的是V^T^, 所以x和y的维度是反过来的, 所以可以看做是卷积的反运算

输入的高和宽是n, 核是k, 填充p, 步幅为s

n^'^ = sn +k - 2p - s

> 卷积的是n^'^ = ((n - k - 2p + s) / s)->n ≥ sn +k - 2p - s

想要成倍增加的时候k = 2p + s

### 实际使用

```python
import torch
from torch import nn
from d2l import torch as d2l
```

+ 实现最简单代码

```python
def trans_conv(X, K):
    h, w = K.shape
    Y = torch.zeros((X.shape[0] + h- 1, X.shape[1] + w- 1))
    for i in range(X.shape[0]):
        for j in range(X.shape[1]):
            Y[i: i + h, j: j + w] += X[i, j] * K
    return Y
```

```python
X = torch.tensor([[0.0, 1.0], [2.0, 3.0]])
K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])
trans_conv(X, K)

"""
tensor([[ 0.,  0.,  1.],
        [ 0.,  4.,  6.],
        [ 4., 12.,  9.]])
"""
```

+ 使用现成的函数

````python
X, K = X.reshape(1, 1, 2, 2), K.reshape(1, 1, 2, 2)
tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, bias=False)
tconv.weight.data = K
tconv(X)

"""
tensor([[[[ 0.,  0.,  1.],
          [ 0.,  4.,  6.],
          [ 4., 12.,  9.]]]], grad_fn=<ConvolutionBackward0>)
"""
tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, padding=1, bias=False)
tconv.weight.data = K
tconv(X)
"""
tensor([[[[4.]]]], grad_fn=<ConvolutionBackward0>)
"""
tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, stride=2, bias=False)
tconv.weight.data = K
tconv(X)
"""
tensor([[[[0., 0., 0., 1.],
          [0., 0., 2., 3.],
          [0., 2., 0., 3.],
          [4., 6., 6., 9.]]]], grad_fn=<ConvolutionBackward0>)
"""
````

## 全连接神经网络FCN

fully convolutional network

使用转置卷积替代CNN最后的全连接层, 从而可以预测每一个像素, 实际获得的通道数是记录个各类的信息

![image-20250203201551939](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502032015247.png)

+ 加载模型

```python
pretrained_net = torchvision.models.resnet18(pretrained=True)
list(pretrained_net.children())[-3:]
"""
[Sequential(
   (0): BasicBlock(
     (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
     (relu): ReLU(inplace=True)
     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
     (downsample): Sequential(
       (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
       (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
     )
   )
   (1): BasicBlock(
     (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
     (relu): ReLU(inplace=True)
     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
   )
 ),
 AdaptiveAvgPool2d(output_size=(1, 1)),
 Linear(in_features=512, out_features=1000, bias=True)]
"""
```

+ 获取非线性的模型

```python
from torch import nn
net = nn.Sequential(*list(pretrained_net.children())[:-2])
```

> ```python
> X = torch.rand(size=(1, 3, 320, 480))
> net(X).shape
> ```
>
> torch.Size([1, 512, 10, 15])

+ 初始化新的模型

```python
num_classes = 21
net.add_module('final_conv', nn.Conv2d(512, num_classes, kernel_size=1))
net.add_module('transpose_conv', nn.ConvTranspose2d(num_classes, num_classes,
kernel_size=64, padding=16, stride=32))
```

+ 一个初始的参数, 这个参数可以用于

```python
def bilinear_kernel(in_channels, out_channels, kernel_size):
    # 使用这一个函数来初始化转置卷积层的权重, 可以达到图像放大的效果
    factor = (kernel_size + 1) // 2
    if kernel_size % 2 == 1:
        center = factor- 1
    else:
        center = factor- 0.5
    og = (torch.arange(kernel_size).reshape(-1, 1),
        torch.arange(kernel_size).reshape(1,-1))
    filt = (1- torch.abs(og[0]- center) / factor) * \
        (1- torch.abs(og[1]- center) / factor)
    weight = torch.zeros((in_channels, out_channels,
        kernel_size, kernel_size))
    weight[range(in_channels), range(out_channels), :, :] = filt
    return weight
```

+ 初始化参数

```python
conv_trans = nn.ConvTranspose2d(3, 3, kernel_size=4, padding=1, stride=2,
                    bias=False)
conv_trans.weight.data.copy_(bilinear_kernel(3, 3, 4))
```

