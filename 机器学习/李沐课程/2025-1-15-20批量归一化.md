# 批量归一化

之前使用的模型对出现问题, 损失函数在最后面, 后面的训练比较快

数据在训练的开始位置, 开始位置的训练比较慢, 不同位置的收敛的速度是不同的(学习率相同, 但是不同位置的方差差的比较大0325)

数据变化影响全局, 底部数据变化的可能导致后面的数据重新训练

目标: 改变底层的数据, 使得顶部的数据收到的影响比较少

![image-20250115103709061](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501151037215.png)

![image-20250115104024280](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501151040536.png)

> 其他两个参数是可以学习的参数, 防止标准正态分布的数据不太合理, 学习出来适合的偏移和缩放

这一层的位置是在全连接以及卷积层的输出上面, 激活函数的前面, 以及全连接和卷积层的输入

全连接层是作用在特征维, 卷积层是在通道维, 实际使用的时候可能是加入了小批量的噪音, 起到一部分的dropout层的作用(不要混用), 可以用于加速收敛但是不改变精度

## 计算滑动平均

```python
moving_mean = (moving_mean * (n-1) + batch_mean) / n = (1-1/n) * moving_mean + 1/n * batch_mean
moving_var = (moving_var * (n-1) + batch_var) / n = (1-1/n) * moving_var + 1/n * batch_var
```

> 正常使用的时候是这样的, 但是，当n 逐步增大的时候，1/n 逐渐减小，1/n * batch_mean 起的影响会越来越小，(moving_mean, moving_var) 会趋于不变。

而对于一个小的常量 1 - momentum 值，能够让 (moving_mean, moving_var) 在整个训练集真实的均值和方差附近波动，这对于训练来说，能够增加模型的鲁棒性，也会在一定程度上减少过拟合。但这个波动不能很大，所以 1 - momentum 需要是一个比较小的值。

```python3
moving_mean = moving_mean * momentum + batch_mean * (1 - momentum)
moving_var = moving_var * momentum + batch_var * (1 - momentum)
```

## 代码实现

```python
import torch
from torch import nn
from d2l import torch as d2l
```

+ 实现一次计算题

```python
# X:这一层的输入，形状为（batch_size，num_features）
# gamma：拉伸参数
# beta：偏移参数, 以上两个参数是可以学习的
# moving_mean：全连接层的均值(全局的均值)
# moving_var：全连接层的方差
# eps：为了维持数值稳定性而添加的小常数, 避免分母为0
# momentum：动量超参数, 用于更新全连接层的均值和方差一般是0.9
# 返回值：批量归一化的输出和更新的全连接层的均值和方差
def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):
    if not torch.is_grad_enabled():
        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差
        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)
    else:
        assert len(X.shape) in (2, 4) # X的维度只能是2或4(全连接以及卷积)
        if len(X.shape) == 2: # 全连接
            mean = X.mean(dim=0) # 沿着batch_size维度求均值, 0表示第一个维度, 对所有batch的对应特征求均值
            var = ((X - mean) ** 2).mean(dim=0) # 沿着batch_size维度求方差
        else:
            # dim=(0, 2, 3)表示沿着通道维度求均值
            mean = X.mean(dim=(0, 2, 3), keepdim=True) # 沿着通道维度求均值
            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True) # 沿着通道维度求方差
        X_hat = (X - mean) / torch.sqrt(var + eps)
        # 计算一下移动平均, 这里的mean和var是当前batch的均值和方差, 而moving_mean和moving_var是全局的均值和方差
        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean
        moving_var = momentum * moving_var + (1.0 - momentum) * var
    Y = gamma * X_hat + beta
    return Y, moving_mean, moving_var
```

+ 实现一层

```python
class BatchNorm(nn.Module):
    def __init__(self, num_features, num_dims):
        # num_features: 全连接层的输出个数或者卷积层的输出通道数
        # num_dims: 2表示全连接层，4表示卷积层
        super(BatchNorm, self).__init__()
        if num_dims == 2:
            shape = (1, num_features)
        else:
            shape = (1, num_features, 1, 1)
        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化为0和1
        self.gamma = nn.Parameter(torch.ones(shape))
        self.beta = nn.Parameter(torch.zeros(shape))
        # 不参与求梯度和迭代的变量，全连接层的均值和方差
        self.moving_mean = torch.zeros(shape)
        self.moving_var = torch.zeros(shape)

    def forward(self, X):
        if self.moving_mean.device != X.device:
            # 如果moving_mean和moving_var不在X的设备上, 
            # 就将它们复制到X所在的设备上
            # 其他的参数gamma和beta是在net.parameters()中, 会自动转移到X所在的设备上
            self.moving_mean = self.moving_mean.to(X.device)
            self.moving_var = self.moving_var.to(X.device)
        # 保存更新过的moving_mean和moving_var
        Y, self.moving_mean, self.moving_var = batch_norm(
            X, self.gamma, self.beta, self.moving_mean, self.moving_var,
            eps=1e-5, momentum=0.9)
        return Y
```

+ 开始训练

```python
# 构造一个LeNet
net  = nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=5),
    BatchNorm(6, num_dims=4),
    nn.Sigmoid(),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Conv2d(6, 16, kernel_size=5),
    BatchNorm(16, num_dims=4), # 16是卷积层的输出通道数
    nn.Sigmoid(),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(16*4*4, 120),
    BatchNorm(120, num_dims=2),
    nn.Sigmoid(),
    nn.Linear(120, 84),
    BatchNorm(84, num_dims=2),
    nn.Sigmoid(),
    nn.Linear(84, 10)
)

lr, num_epochs, batch_size = 0.2, 10, 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

![image-20250115131131636](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501151311132.png)

### 简单实现

```python
net = nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=5),
    nn.BatchNorm2d(6),
    nn.Sigmoid(),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Conv2d(6, 16, kernel_size=5),
    nn.BatchNorm2d(16),
    nn.Sigmoid(),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(16*4*4, 120),
    nn.BatchNorm1d(120),
    nn.Sigmoid(),
    nn.Linear(120, 84),
    nn.BatchNorm1d(84),
    nn.Sigmoid(),
    nn.Linear(84, 10)
)
```

