# seq2seq序列到序列

最早的应用是作为机器翻译, 实际是做一个sequence to sequence的转化, 可以使用编码器解码器的架构进行实现

循环神经网络编码器使用长度可变的序列作为输入，将其转换为固定 形状的隐状态。

输入序列的信息被编码到循环神经网络编码器的隐状态中。为了连续生成输出序列 的词元，独立的循环神经网络解码器是基于输入序列的编码信息和输出序列已经看见的或者生成的词元来预 测下一个词元。

![image-20250211110109509](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502111101558.png)

> 这一个模型可以使用双向的RNN

编码器最终的隐状态在每一个时间步都作 为解码器的输入序列的一部分。

![image-20250211111658630](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502111116693.png)

![image-20250211111941474](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502111119537.png)

> 推理的时候使用上一次的输出作为输入

输出的序列需要使用新的方式进行检测质量, 因为同一个含义实际的句子可能是不同的

![image-20250211112220610](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502111122670.png)

> 最好的结果是1, 数字越小越不好
>
> n-grad是使用n个字符对输出进行分割, 看这个分割是不是在目标的里面出现(单次匹配)
>
> 计算的时候, 如果预测的长度比实际的长度小的时候exp会计算一个负数(很小)
>
> 后面的计算的时候n越大, 1 / 2^n^变小, p~n~<1会导致实际的数值变大, 所以长的匹配权重更高

## 嵌入层

嵌入层的权重是一个矩阵，其行数等于输入词表的大小（vocab_size），其列数等于特征 向量的维度（embed_size）。对于任意输入词元的索引i，嵌入层获取权重矩阵的第i行（从0开始）以返回其 特征向量。

## 代码实现