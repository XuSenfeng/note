# 双向循环神经网络

如果是一个文本填空, 可以使用后面的信息对空里面的信息进行预测

如果我们想用概率图模型来解决这个问题，可以设计一个隐变量模型：在任意时间步t，假设存在某个隐变量h~t~， 通过概率P(x~t~ |h~t~)控制我们观测到的x~t~

任何h~t~ →h~t+1~转移都是由一些状态转移概率P(h~t+1~ |h~t~)给 出。这个概率图模型就是一个隐马尔可夫模型

![image-20250210221807615](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502102218677.png)

> 输入可以使用一个值预测

![image-20250210220911430](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502102209488.png)

![image-20250210221146700](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502102211740.png)

> 把两个输出连在一起, 所以最后的输出liner需要使用两倍的输入大小

这个在训练的时候比较好实现, 但是推理的时候不太适合推理未来发生的数据, 一般使用的场景是给一个完整的句子以后使用这个句子预测一些东西(翻译, 填空)

## 错误示例

```python
import torch
from torch import nn
from d2l import torch as d2l
# 加载数据
batch_size, num_steps, device = 32, 35, d2l.try_gpu()
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
# 通过设置“bidirective=True”来定义双向LSTM模型
vocab_size, num_hiddens, num_layers = len(vocab), 256, 2
num_inputs = vocab_size
lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers, bidirectional=True)
model = d2l.RNNModel(lstm_layer, len(vocab))
model = model.to(device)
# 训练模型
num_epochs, lr = 500, 1
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
```

![image-20250210223445301](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502102234356.png)

> 可以看出来这个的预测效果很差

## 机器翻译数据集

下载数据集

```python
#@save
d2l.DATA_HUB['fra-eng'] = (d2l.DATA_URL + 'fra-eng.zip',
'94646ad1522d915e7b0f9296181140edcf86a4f5')
#@save
def read_data_nmt():
    """载入“英语－法语”数据集"""
    data_dir = d2l.download_extract('fra-eng')
    with open(os.path.join(data_dir, 'fra.txt'), 'r',
        encoding='utf-8') as f:
        return f.read()
raw_text = read_data_nmt()
print(raw_text[:75])
"""
Downloading ../data\fra-eng.zip from http://d2l-data.s3-accelerate.amazonaws.com/fra-eng.zip...
Go.	Va !
Hi.	Salut !
Run!	Cours !
Run!	Courez !
Who?	Qui ?
Wow!	Ça alors !
"""
```

处理一下数据, 把数据的按照格式进行排列

```python
def preprocess_nmt(text):
    """预处理“英语－法语”数据集"""
    def no_space(char, prev_char):
        return char in set(',.!?') and prev_char != ' '
    
    # 使用空格替换不间断空格
    # 使用小写字母替换大写字母
    # \u202f是一个不间断的空白符, \xa0是不间断空格
    text = text.replace('\u202f', ' ').replace('\xa0', ' ').lower()
    # 在单词和标点符号之间插入空格
    out = [' ' + char if i > 0 and no_space(char, text[i- 1]) else char
    for i, char in enumerate(text)]
    return ''.join(out)
text = preprocess_nmt(raw_text)
print(text[:300])
"""
go .	va !
hi .	salut !
run !	cours !
run !	courez !
who ?	qui ?
wow !	ça alors !
fire !	au feu !
help !	à l'aide !
jump .	saute .
stop !	ça suffit !
stop !	stop !
stop !	arrête-toi !
wait !	attends !
wait !	attendez !
go on .	poursuis .
go on .	continuez .
go on .	poursuivez .
hello !	bonjour !
hell
"""
```

把两个语言的数据分割开来, 之后使用链表进行存储

```python
#@save
def tokenize_nmt(text, num_examples=None):
    """词元化“英语－法语”数据数据集"""
    source, target = [], []
    for i, line in enumerate(text.split('\n')):
        if num_examples and i > num_examples:
            break
        parts = line.split('\t')
        if len(parts) == 2:
            source.append(parts[0].split(' '))
            target.append(parts[1].split(' '))
    return source, target

source, target = tokenize_nmt(text)
source[:6], target[:6]
"""
([['go', '.'],
  ['hi', '.'],
  ['run', '!'],
  ['run', '!'],
  ['who', '?'],
  ['wow', '!']],
 [['va', '!'],
  ['salut', '!'],
  ['cours', '!'],
  ['courez', '!'],
  ['qui', '?'],
  ['ça', 'alors', '!']])
"""
```

显示一下数据的分布

```python
#@save 查看一下数据集中数据的分布
def show_list_len_pair_hist(legend, xlabel, ylabel, xlist, ylist):
    """绘制列表长度对的直方图"""
    d2l.set_figsize()
    _, _, patches = d2l.plt.hist(
        [[len(l) for l in xlist], [len(l) for l in ylist]]) 
    d2l.plt.xlabel(xlabel)
    d2l.plt.ylabel(ylabel)
    for patch in patches[1].patches:
        patch.set_hatch('/')
    d2l.plt.legend(legend)
show_list_len_pair_hist(['source', 'target'], '# tokens per sequence',
    'count', source, target)
```

建立一个词表

```python
src_vocab = d2l.Vocab(source, min_freq=2,
    reserved_tokens=['<pad>', '<bos>', '<eos>'])
len(src_vocab)
"""
10012
"""
```

打印一下效果

```python
for i in range(7):
    print(src_vocab.idx_to_token[i])
"""
<unk>
<pad>
<bos>
<eos>
.
i
you
"""
```

实际的数据需要长度一致

```python
#@save
def truncate_pad(line, num_steps, padding_token):
    """截断或填充文本序列, num_steps是最大的长度"""
    if len(line) > num_steps:
        return line[:num_steps] # 截断
    return line + [padding_token] * (num_steps- len(line)) # 填充
truncate_pad(src_vocab[source[0]], 10, src_vocab['<pad>'])
```

长度一致的同时记录一下有效数据的长度

```python
#@save
def build_array_nmt(lines, vocab, num_steps):
    """将机器翻译的文本序列转换成小批量"""
    lines = [vocab[l] for l in lines] # 将每个词转换为词索引
    lines = [l + [vocab['<eos>']] for l in lines] # 添加eos结尾
    array = torch.tensor([truncate_pad(
        l, num_steps, vocab['<pad>']) for l in lines])
    # 每个序列的有效长度, 不是填充的长度
    valid_len = (array != vocab['<pad>']).type(torch.int32).sum(1)
    return array, valid_len
```

使用一个函数完成

```python
#@save 上面的函数的集合
def load_data_nmt(batch_size, num_steps, num_examples=600):
    """返回翻译数据集的迭代器和词表"""
    text = preprocess_nmt(read_data_nmt()) # 加载, 按格式预处理
    source, target = tokenize_nmt(text, num_examples) # 分词
    src_vocab = d2l.Vocab(source, min_freq=2,
        reserved_tokens=['<pad>', '<bos>', '<eos>'])
    tgt_vocab = d2l.Vocab(target, min_freq=2,
        reserved_tokens=['<pad>', '<bos>', '<eos>'])
    # 获取定长的数据集
    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)
    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)
    # 返回数据集迭代器
    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)
    data_iter = d2l.load_array(data_arrays, batch_size)
    return data_iter, src_vocab, tgt_vocab
```

结果

```python
train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size=2, num_steps=8)
for X, X_valid_len, Y, Y_valid_len in train_iter:
    print('X:', X.type(torch.int32))
    print('X的有效长度:', X_valid_len)
    print('Y:', Y.type(torch.int32))
    print('Y的有效长度:', Y_valid_len)
    break
```

