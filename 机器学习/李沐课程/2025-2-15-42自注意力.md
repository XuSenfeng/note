# 自注意力

Self-attention

给定一个序列, 每一个x~i~是一个长度为d的序列, 自注意力把x~i~当做key, value, query来对序列抽取特征得到y~1~ ... y~n~ 

![image-20250215132122542](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502151321689.png)

输出也是一个长序列, 是考虑到每一个输入以后对着一个输入的一个输出

![image-20250215133841094](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502151338231.png)

输入的长度是不一定的, 需要计算出来一个输入和其他的输入之间的关联度

![image-20250215134413932](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502151344118.png)

最经常使用的是左边的那一种处理方式, 也是transformer里面是使用的方法, 计算出一个关联度

自注意力需要把每一个输入和其他的输入计算出来一个关联度(加权)

![image-20250215134828912](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502151348981.png)

> 计算的时候还会和自己计算关联性, 所有计算出来以后再做一个softmax(归一化, 可以使用其他函数比如relu进行激活)

![image-20250215134944381](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502151349470.png)

现在已经获取关联度, 可以使用这个关联度计算最后的输出

![image-20250215135320682](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502151353783.png)

![image-20250215133841094](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502151338231.png)

实际计算的时候权重的计算可以在一起进行

![image-20250215135808710](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502151358802.png)

![image-20250215140001314](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502151400401.png)

![image-20250215140058540](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502151400602.png)

![image-20250215140303152](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502151403236.png)

有一个进阶的版本, Multi-head Self-attention, 计算出来更多的a-hat, 在计算出来q^1^, k^1^之类以后, 乘以多个权重获取到q^i,1^, q^i,2^等

![image-20250215140914218](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502151409306.png)

![image-20250215140951023](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502151409071.png)

![image-20250215220342259](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502152203424.png)

使用两个函数把隐藏层分为多个不同的头

```python
#@save
def transpose_qkv(X, num_heads):
    """为了多注意力头的并行计算而变换形状, 把隐藏层分为多个头"""
    # 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens)
    # 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，
    # num_hiddens/num_heads)
    X = X.reshape(X.shape[0], X.shape[1], num_heads,-1)
    # 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数,
    # num_hiddens/num_heads)
    X = X.permute(0, 2, 1, 3)
    # 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数,
    # num_hiddens/num_heads)
    return X.reshape(-1, X.shape[2], X.shape[3])
#@save
def transpose_output(X, num_heads):
    """逆转transpose_qkv函数的操作"""
    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])
    X = X.permute(0, 2, 1, 3)
    return X.reshape(X.shape[0], X.shape[1],-1)
```

实际的实现

```python
#@save
class MultiHeadAttention(nn.Module):
    """
    多头注意力, num_heads个并行的注意力头
    把number_hiddens个头的分割, 得到num_hiddens/num_heads
    """
    def __init__(self, key_size, query_size, value_size, num_hiddens,
        num_heads, dropout, bias=False, **kwargs):
        # 这里的num_hiddens是二维的
        super(MultiHeadAttention, self).__init__(**kwargs)
        self.num_heads = num_heads
        self.attention = d2l.DotProductAttention(dropout)
        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)
        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)
        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)
        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)
    def forward(self, queries, keys, values, valid_lens):
        # queries，keys，values的形状:
        # (batch_size，查询或者“键－值”对的个数，num_hiddens)
        # valid_lens　的形状:
        # (batch_size，)或(batch_size，查询的个数)
        # 经过变换后，输出的queries，keys，values　的形状:
        # (batch_size*num_heads，查询或者“键－值”对的个数，
        #   num_hiddens/num_heads)
        # 这么做的原因是为了使用batch_dot函数
        queries = transpose_qkv(self.W_q(queries), self.num_heads)
        keys = transpose_qkv(self.W_k(keys), self.num_heads)
        values = transpose_qkv(self.W_v(values), self.num_heads)
        if valid_lens is not None:
            # 在轴0，将第一项（标量或者矢量）复制num_heads次，
            # 然后如此复制第二项，然后诸如此类。
            valid_lens = torch.repeat_interleave(
                valid_lens, repeats=self.num_heads, dim=0)
        # output的形状:(batch_size*num_heads，查询的个数，
        # num_hiddens/num_heads)
        output = self.attention(queries, keys, values, valid_lens)
        # output_concat的形状:(batch_size，查询的个数，num_hiddens)
        output_concat = transpose_output(output, self.num_heads)
        return self.W_o(output_concat)
```

```python
num_hiddens, num_heads = 100, 5
attention = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,
        num_hiddens, num_heads, 0.5)
attention.eval()
"""
MultiHeadAttention(
  (attention): DotProductAttention(
    (dropout): Dropout(p=0.5, inplace=False)
  )
  (W_q): Linear(in_features=100, out_features=100, bias=False)
  (W_k): Linear(in_features=100, out_features=100, bias=False)
  (W_v): Linear(in_features=100, out_features=100, bias=False)
  (W_o): Linear(in_features=100, out_features=100, bias=False)
)
"""
```

测试一下, 这个不会改变输入的形状

```python
batch_size, num_queries = 2, 4
num_kvpairs, valid_lens = 6, torch.tensor([3, 2])
X = torch.ones((batch_size, num_queries, num_hiddens))
Y = torch.ones((batch_size, num_kvpairs, num_hiddens))
attention(X, Y, Y, valid_lens).shape
```

## 位置信息

![image-20250215141334285](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502151413386.png)

+ 三角函数

只使用这部分的时候没有位置的关系, 可以使用Positional Encoding, 可以为每一个位置添加一个独特的Vector e^i^ , 最开始的e^i^是人设置的, 可以通过学习得到也可以直接固定得到, 接下来描述的是基于正弦函数和余弦函数 的固定位置编码

![image-20250215184118243](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502151841293.png)

![image-20250215185242796](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502151852855.png)

```python
#@save
class PositionalEncoding(nn.Module):
    """位置编码"""
    def __init__(self, num_hiddens, dropout, max_len=1000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(dropout)
        # 创建一个足够长的P
        self.P = torch.zeros((1, max_len, num_hiddens))
        X = torch.arange(max_len, dtype=torch.float32).reshape(-1, 1) / torch.pow(10000, torch.arange(
                0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)
        self.P[:, :, 0::2] = torch.sin(X)
        self.P[:, :, 1::2] = torch.cos(X)
    def forward(self, X):
        X = X + self.P[:, :X.shape[1], :].to(X.device)
        return self.dropout(X)
```



> 假设输入表示X ∈ R^n×d^ 包含一个序列中n个词元的d维嵌入表示。位置编码使用相同形状的位置嵌入矩阵 P ∈R^n×d^输出X+P，矩阵第i行、第2j列和2j+1列上的元素(不同的列的周期是不一样的)

+ 绝对位置

也可以使用绝对位置信息, 在二进制表示中，较高比特位的交替频率低于较低比特位，与下面的热图所示相似，只是位置编码通过使用 三角函数在编码维度上降低频率。由于输出是浮点数，因此此类连续表示比二进制表示法更节省空间

![image-20250215185212150](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502151852209.png)

+ 相对位置

模型学习得到输入序列中相对位置信息, 。这是因为对于 任何确定的位置偏移δ，位置i+δ处的位置编码可以线性投影位置i处的位置编码来表示(可以不用限定于出现在句子的某一个位置)

![image-20250215190007919](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502151900966.png)



> 这部分的应用主要是在transformer和BERT里面

## 对比

在处理数据的时候, 如果使用的数据的数量比较大, 使得计算数量非常大, 所以有一种Truncated Self-attention的机制, Self-attention只注意他附近的数据

在处理图像的时候也可以使用, Self-Attention GAN和DEtection Transformer 之前使用CNN进行计算的时候, 范围是划定的, 但是使用Self-Attention进行处理可以自行计算使用的范围, 资料量非常大的时候实际的效果可以超过CNN

![image-20250215151555326](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502151515414.png)

相比于RNN的数据必须来自于前面的数据, 计算的时间比较长, 以此计算而使用Self-Attention可以查找全局, 并且可以同时计算, 所以实际使用的时间比较少

两个数据的关系如果可以使用图表示, 计算的时候无关的数据直接设置为0, 这样的计算方式是GNN

![](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502151515414.png)

![image-20250215183124520](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502151831582.png)

![image-20250215183337456](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202502151833516.png)