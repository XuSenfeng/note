# 感知机

给定输入x, 权重w和偏移b, 感知机输出 

![image-20250108232238937](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501082322970.png)

softmax输出的是一个概率, 线性回归输出的是一个实数

![image-20250108232610625](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501082326667.png)

> y~i~是+1或者-1, 所以分类正确的时候, 两个的乘积一定是大于0, 如果判断失败的话更新一下权重, 使用以下的函数进行更新
>
> ![image-20250108232948834](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501082329864.png)

![image-20250108233508436](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501082335493.png)

![image-20250108233550379](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501082335461.png)

## 问题

对于一个XOR的数据不可以拟合, 只可以产生一个线性的分割面

![image-20250108233723294](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501082337352.png)

不可以使用一条线把数据分开

> 实际是一个批量大小为一的梯度下降, 是一个二分算法

## 多层感知机

![image-20250109093325058](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501090933096.png)

![image-20250109093339684](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501090933728.png)

> 进行多次分类, 使用多次的结果进行获取结果

![image-20250109093613244](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501090936314.png)

> 输入和输出的个数是不可以改变的, 唯一可以改变的是隐藏层

![image-20250109094125299](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501090941354.png)

> 这里的隐藏层是一个非线性的原因是如果使用两个线性层, 回事的实际使用的分类函数还是一个线性函数, 所以激活函数不可以是一个线性函数(避免单层感知机)

## 激活函数

激活函数是用来加入非线性因素的，因为线性模型的表达能力不够，引入非线性函数作为激励函数，这样深层神经网络表达能力就更加强大（不再是输入的线性组合，而是几乎可以逼近任意函数）

### Sigmoid激活函数

![image-20250109094540829](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501090945862.png)

![image-20250109094600852](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501090946891.png)

把这一个数据均匀的分布到0和1之间, 相比于最初的二分类模型更加平滑

### Tanh函数

把输入投射到(-1, 1)

![image-20250109094812835](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501090948866.png)

![image-20250109094829575](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501090948618.png)

### ReLU函数

ReLU(x) = max(x, 0), 小于0的时候是0, 大于的时候是x

> 使用这个函数的计算非常快, 所以使用比较多

## 多类分类

![image-20250109095256125](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501090952151.png)

这是之前的分类方法

![image-20250109095405411](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501090954466.png)

实际是在softmax分类里面加入一层

![image-20250109095513488](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501090955541.png)

![image-20250109095709557](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501090957611.png)

> 一般数据比较复杂的时候, 可以使用比较大的隐藏层或者使用多个隐藏层, 一般在使用的时候不会先进行压缩, 避免数据的丢失

## 实际实现

+ 导入包和数据集

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
```

+ 建立参数列表

```python
# 建立一个有单隐藏层的多层感知机
num_inputs, num_outputs, num_hiddens = 784, 10, 256
# 输入层到隐藏层的参数, 784 * 256, 初始化为正态分布, 使用nn.Parameter来告诉pytorch这是一个参数(可训练)
# 这里不使用同样的参数是因为如果参数相同, 那么在反向传播时, 两个参数的梯度会相同, 从而导致两个参数的更新相同
W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens, requires_grad=True) * 0.01)
# 隐藏层到输出层的参数, 256 * 10, 初始化为正态分布
W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs, requires_grad=True) * 0.01)
# 偏置参数
b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))
b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))

params = [W1, b1, W2, b2]
```

+ 加一个激活函数

```python
#Relu激活函数
def relu(X):
    a = torch.zeros_like(X) # 获取一个和X相同形状的全0张量
    return torch.max(X, a)
```

+ 建立一个实际的网络函数, 以及使用现成的损失函数

```python
# 定义模型, 之前的模型只进行了矩阵乘法, 这里加入了激活函数
def net(X):
    X = X.reshape((-1, num_inputs)) # 将输入转换为2维张量
    # @表示矩阵乘法, n*784 @ 784*256 = n*256
    H = relu(X @ W1 + b1)
    # n*256 @ 256*10 = n*10
    return (H @ W2 + b2) 

# 损失函数
loss = nn.CrossEntropyLoss()
```

+ 开始训练

```python
num_epochs, lr = 10, 0.1
# 优化函数
updater = torch.optim.SGD(params, lr=lr)
# 实际训练
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)
```

![image-20250109131803953](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501091318018.png)

> 实际的效果比之前要好一点
>
> ![image-20250108193835446](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501081938604.png)

### 简单实现

````python
import torch
from torch import nn
from d2l import torch as d2l
````

+ 获取参数

```python
net = nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(),nn.Linear(256, 10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)

net.apply(init_weights)
```

+ 获取使用的算法以及开始训练

```python
betch_size, lr, num_epochs = 256, 0.1, 10
loss = nn.CrossEntropyLoss()
trainer = torch.optim.SGD(net.parameters(), lr=lr)

train_iter, test_iter = d2l.load_data_fashion_mnist(betch_size)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

![image-20250109133045270](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501091330318.png)

