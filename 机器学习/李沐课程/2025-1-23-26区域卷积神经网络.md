# 区域卷积神经网络

## R-CNN

使用启发式搜索算法选择锚框, 预训练模型对每一个锚框抽取特征

训练一个SVM对类别分类, 再训练一个线性回归模型预测边缘框偏移

### Rol兴趣区域池化层

其目的是对非均匀尺寸的输入执行最大池化以获得固定尺寸的特征图

给定一个锚框, 均匀的进行分割为n x m块, 输出每一个块里面的最大值(最大池化)

不管这一个锚框多大, 总输出一个nm的值

![image-20250123224349974](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202501232243010.png)

## Fast-RCNN

使用CNN对图像进行抽取特征, 之后使用比较好的锚框进行拟合, 而不是使用所有的锚框, 选择锚框的算法是图像识别里面的算法

![image-20250123224313652](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202501232243728.png)

## Faster RCNN

图像识别的算法速度比较慢, 所以使用一个神经网络进行选择

![image-20250123224558588](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202501232245636.png)

## Mask RCNN

![image-20250123224720530](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202501232247571.png)

进行像素级别的预测, 提高一个图片边缘的质量, 因为之前使用的Rol pool会使得输出有偏移(不能整除的时候向一侧取整)

## 单发多框检测SSD

### 多尺度目标检测

如果为每个像素都生成的锚框，我们最终可能会得到太多需要计算的锚框。我们可以在输入图像中均匀采样一小部分像素，并以它们为中心生成锚框。

```python
def display_anchors(fmap_w, fmap_h, s):
    d2l.set_figsize()
    # 前两个维度上的值不影响输出
    fmap = torch.zeros((1, 10, fmap_h, fmap_w))
    anchors = multibox_prior(fmap, sizes=s, ratios=[1, 2, 0.5])
    bbox_scale = torch.tensor((w, h, w, h))
    show_bboxes(d2l.plt.imshow(img).axes,
        anchors[0] * bbox_scale) # 显示一下

display_anchors(fmap_w=4, fmap_h=4, s=[0.15]) # 按照4*4的生成
```

![image-20250124225408398](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202501242254605.png)

```python
display_anchors(fmap_w=2, fmap_h=2, s=[0.4])
```

![image-20250124225737928](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202501242257099.png)

### 实现原理

对每一个像素生成多个锚框, 对锚框进行计算

此模型主要由基础网络组成，其后是几个多尺度特征块。基本网 络用于从输入图像中提取特征，因此它可以使用深度卷积神经网络。

每个多尺度特征块 将上一层提供的特征图的高和宽缩小（如减半），并使特征图中每个单元在输入图像上的感受野变得更广阔。

![image-20250124230844742](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202501242308903.png)

### 类别预测层

设目标类别的数量为q。这样一来，锚框有q + 1个类别，其中0类是背景。

设特征图的高和宽 分别为h和w。如果以其中每个单元为中心生成a个锚框，那么我们需要对hwa个锚框进行分类。如果使用全 连接层作为输出，很容易导致模型参数过多。

类别预测层使用一个保持输入高和宽的卷积层。这样一来，输出和输入在特征图宽和高上的空间 坐标一一对应。考虑输出和输入同一空间坐标（、）（𝑥、𝑦）：输出特征图上（x、y）坐标的通道里包含了以输入特征图（、）（𝑥、𝑦）坐标为中心生成的所有锚框的类别预测。因此输出通道数为𝑎(𝑞+1)，其中索引为（）𝑖(𝑞+1)+𝑗（0≤𝑗≤𝑞）的通道代表了索引为i的锚框有关类别索引为j的预测。

```python
def cls_predictor(num_inputs, num_anchors, num_classes):
    return nn.Conv2d(num_inputs, 
                     num_anchors * (num_classes +
                     1),kernel_size=3, padding=1)
```

### 边框预测层

和上面的比较类似, 但是为每一个锚框预测出四个偏移量

```python
def bbox_predictor(num_inputs, num_anchors):
    return nn.Conv2d(num_inputs, num_anchors * 4, kernel_size=3, padding=1)
```



### 连结多尺度的预测

实际的输出大小和每一个图片的锚框数, 批量大小, 类别数, 图片的大小, 有关

```python
def forward(x, block):
	return block(x)
Y1 = forward(torch.zeros((2, 8, 20, 20)), cls_predictor(8, 5, 10))
Y2 = forward(torch.zeros((2, 16, 10, 10)), cls_predictor(16, 3, 10))
Y1.shape, Y2.shape
"""
(torch.Size([2, 55, 20, 20]), torch.Size([2, 33, 10, 10]))
第二维的大小是输出的(种类+1)*锚框的大小, 后面的是图片的大小
"""
```

到这一步的时候, 由于图片的大小不同的原因, 所以把图片进行展平

```python
def flatten_pred(pred):
    # permute变化把输出的通道数放在最后, 展平以后实际的相同位置的不同通道是在一起的
    return torch.flatten(pred.permute(0, 2, 3, 1), start_dim=1)
def concat_preds(preds):
    # 把第二维拼接起来
    return torch.cat([flatten_pred(p) for p in preds], dim=1)

```

### 高宽减半块

每个高和宽减 半块由两个填充为1的3 × 3的卷积层、以及步幅为2的2 × 2最大汇聚层组成。高和宽减半块会扩大每个单元在其输出特征图中的感受野。

```python
def down_sample_blk(in_channels, out_channels):
    blk = []
    for _ in range(2):
        blk.append(nn.Conv2d(in_channels, out_channels,
            kernel_size=3, padding=1))
        blk.append(nn.BatchNorm2d(out_channels)) // 归一化
        blk.append(nn.ReLU())
        in_channels = out_channels
    blk.append(nn.MaxPool2d(2))
    return nn.Sequential(*blk)
```

### 基本网络快

用于从图像里面抽取特征, 该网络串联3个高和 宽减半块，并逐步将通道数翻倍。给定输入图像的形状为256×256，此基本网络块输出的特征图形状为32×32 （256/23 = 32）。

```python
def base_net():
    blk = []
    num_filters = [3, 16, 32, 64]
    for i in range(len(num_filters) - 1):
        blk.append(down_sample_blk(num_filters[i], num_filters[i+1]))
    return nn.Sequential(*blk)

forward(torch.zeros((2, 3, 256, 256)), base_net()).shape
```

### 完整的模型

完整的单发多框检测模型由五个模块组成。每个块生成的特征图既用于生成锚框，又用于预测这些锚框的类 别和偏移量。在这五个模块中，第一个是基本网络块，第二个到第四个是高和宽减半块，最后一个模块使用 全局最大池将高度和宽度都降到1。

```python
def get_blk(i):
    if i == 0:
        blk = base_net()
    elif i == 1:
        blk = down_sample_blk(64, 128)
    elif i == 4:
        blk = nn.AdaptiveMaxPool2d((1,1))
    else:
        blk = down_sample_blk(128, 128)
    return blk
```

现在我们为每个块定义前向传播。与图像分类任务不同，此处的输出包括：CNN特征图Y；在当前尺度下根 据Y生成的锚框；预测的这些锚框的类别和偏移量（基于Y）。

```python
def blk_forward(X, blk, size, ratio, cls_predictor, bbox_predictor):
    Y = blk(X)
    # 生成多个锚框
    anchors = d2l.multibox_prior(Y, sizes=size, ratios=ratio)
    cls_preds = cls_predictor(Y) # 卷积预测每一个框的输出种类
    bbox_preds = bbox_predictor(Y) # 预测一下偏移
    return (Y, anchors, cls_preds, bbox_preds)

```

一个较接近顶部的多尺度特征块是用于检测较大目标的，因此需要生成更大的锚框。 在上面的前向传播中，在每个多尺度特征块上，我们通过调用的multibox_prior函数的sizes参 数传递两个比例值的列表。

在下面，0.2和1.05之间的区间被均匀分成五个部分，以确定五个模块的在不同尺度 下的较小值：0.2、0.37、0.54、0.71和0.88。之后，他们较大的值由√ 0.2 × 0.37 = 0.272、 √ 0.37 × 0.54 = 0.447等 给出。

```python
sizes = [[0.2, 0.272], [0.37, 0.447], [0.54, 0.619], [0.71, 0.79],
[0.88, 0.961]]
ratios = [[1, 2, 0.5]] * 5
num_anchors = len(sizes[0]) + len(ratios[0]) - 1
```

构建模型, 每一层都有一个输出, 最后把输出合并起来

```python
class TinySSD(nn.Module):
    def __init__(self, num_classes, **kwargs):
        super(TinySSD, self).__init__(**kwargs)
        self.num_classes = num_classes
        idx_to_in_channels = [64, 128, 128, 128, 128]
        for i in range(5):
            # 即赋值语句self.blk_i=get_blk(i)
            setattr(self, f'blk_{i}', get_blk(i)) # 五个不同的块
            setattr(self, f'cls_{i}', cls_predictor(idx_to_in_channels[i],
                num_anchors, num_classes)) # 预测每一个框的输出种类
            setattr(self, f'bbox_{i}', bbox_predictor(idx_to_in_channels[i],
                num_anchors)) # 预测每一个框的输出坐标偏移量
        
    def forward(self, X):
        anchors, cls_preds, bbox_preds = [None] * 5, [None] * 5, [None] * 5
        for i in range(5):
            # getattr(self,'blk_%d'%i)即访问self.blk_i
            X, anchors[i], cls_preds[i], bbox_preds[i] = blk_forward(
                X, getattr(self, f'blk_{i}'), sizes[i], ratios[i],
                getattr(self, f'cls_{i}'), getattr(self, f'bbox_{i}'))
        anchors = torch.cat(anchors, dim=1) # 将锚框坐标展平
        cls_preds = concat_preds(cls_preds) # 边框预测种类
        cls_preds = cls_preds.reshape(
            cls_preds.shape[0], -1, self.num_classes + 1) # 把类的每一个预测作为一个维度
        bbox_preds = concat_preds(bbox_preds) # 边框预测坐标
        return anchors, cls_preds, bbox_preds
```



```python
net = TinySSD(num_classes=1)
X = torch.zeros((32, 3, 256, 256))
anchors, cls_preds, bbox_preds = net(X)

print('output anchors:', anchors.shape)
print('output class preds:', cls_preds.shape)
print('output bbox preds:', bbox_preds.shape)

"""
output anchors: torch.Size([1, 5444, 4])    # 锚框的位置
output class preds: torch.Size([32, 5444, 2]) # 锚框的种类预测, 32批量大小, 5444锚框数量
output bbox preds: torch.Size([32, 21776]) # 预测一下偏移
"""
```

加载数据集

```python
batch_size = 32
train_iter, _ = d2l.load_data_bananas(batch_size)
```

加载模型

```python
device, net = d2l.try_gpu(), TinySSD(num_classes=1)
trainer = torch.optim.SGD(net.parameters(), lr=0.2, weight_decay=5e-4)
```

损失函数

```python
cls_loss = nn.CrossEntropyLoss(reduction='none')
bbox_loss = nn.L1Loss(reduction='none') # 
def calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks):
    batch_size, num_classes = cls_preds.shape[0], cls_preds.shape[2]
    # 计算一下分类的偏移
    cls = cls_loss(cls_preds.reshape(-1, num_classes),
        cls_labels.reshape(-1)).reshape(batch_size, -1).mean(dim=1)
    # 计算一下边框位置的偏移, 乘一个mask去除无关的框
    bbox = bbox_loss(bbox_preds * bbox_masks, bbox_labels * bbox_masks).mean(dim=1)
    return cls + bbox # 把两个误差加在一起
def cls_eval(cls_preds, cls_labels):
    # 由于类别预测结果放在最后一维，argmax需要指定最后一维。
    # 计算一下算对的个数
    return float((cls_preds.argmax(dim=-1).type(cls_labels.dtype) == cls_labels).sum())
def bbox_eval(bbox_preds, bbox_labels, bbox_masks):
    # 计算一下偏移
    return float((torch.abs((bbox_labels - bbox_preds) * bbox_masks))
```

开始训练

```python
num_epochs, timer = 20, d2l.Timer()
animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],
            legend=['class error', 'bbox mae'])
net = net.to(device)
for epoch in range(num_epochs):
    # 训练精确度的和，训练精确度的和中的示例数
    # 绝对误差的和，绝对误差的和中的示例数
    metric = d2l.Accumulator(4)
    net.train()
    for features, target in train_iter:
        timer.start()
        trainer.zero_grad()
        X, Y = features.to(device), target.to(device)
        # 生成多尺度的锚框，为每个锚框预测类别和偏移量
        anchors, cls_preds, bbox_preds = net(X)
        # 获取真实的每一个锚框的分类以及偏移
        bbox_labels, bbox_masks, cls_labels = d2l.multibox_target(anchors, Y)
        # 根据类别和偏移量的预测和标注值计算损失函数
        l = calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels,
            bbox_masks)
        l.mean().backward()
        trainer.step()
        metric.add(cls_eval(cls_preds, cls_labels), cls_labels.numel(),
            bbox_eval(bbox_preds, bbox_labels, bbox_masks),
            bbox_labels.numel())
    cls_err, bbox_mae = 1 - metric[0] / metric[1], metric[2] / metric[3]
    animator.add(epoch + 1, (cls_err, bbox_mae))
print(f'class err {cls_err:.2e}, bbox mae {bbox_mae:.2e}')
```

![image-20250125153326312](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202501251533557.png)

实际预测一下

```python
import torchvision
# 加载一个图片
X = torchvision.io.read_image('E:/JHY/python/2024-11-3-pytorchLiMu/jupyrt/data/bananas/bananas_val/images/1.png').unsqueeze(0).float()
img = X.squeeze(0).permute(1, 2, 0).long() # 加一个批量维度

import torch.nn.functional as F

def predict(X):
    net.eval()
    anchors, cls_preds, bbox_preds = net(X.to(device))
    cls_probs = F.softmax(cls_preds, dim=2).permute(0, 2, 1)
    # 使用真实的锚框以及偏移进行抑制以及获取实际的输出
    output = multibox_detection(cls_probs, bbox_preds, anchors)
    idx = [i for i, row in enumerate(output[0]) if row[0] != -1] # 获取有效的类
    return output[0, idx]
output = predict(X)
output

"""
tensor([[ 0.00,  0.99,  0.09,  0.31,  0.30,  0.53],
        [ 0.00,  0.04, -0.18,  0.18,  1.13,  0.81],
        [ 0.00,  0.04,  0.12, -0.05,  0.98,  0.96],
        [ 0.00,  0.03,  0.10,  0.37,  0.26,  0.58],
        [ 0.00,  0.03, -0.22,  0.52,  0.44,  1.18],
        [ 0.00,  0.03,  0.08,  0.24,  0.28,  0.45],
        [ 0.00,  0.02,  0.62,  0.32,  1.14,  1.31],
        [ 0.00,  0.02,  0.63, -0.32,  1.14,  0.64],
        [ 0.00,  0.01, -0.11, -0.36,  0.26,  0.45],
        [ 0.00,  0.01,  0.37,  0.61,  1.36,  1.13],
        [ 0.00,  0.01,  0.37, -0.17,  1.32,  0.34],
        [ 0.00,  0.01,  0.03,  0.33,  0.21,  0.52],
        [ 0.00,  0.01,  0.09,  0.19,  0.30,  0.37]], device='cuda:0',
       grad_fn=<IndexBackward0>)
"""
def display(img, output, threshold):
    d2l.set_figsize((5, 5))
    fig = d2l.plt.imshow(img)
    for row in output:
        score = float(row[1])
        if score < threshold:
            continue
        h, w = img.shape[0:2]
        bbox = [row[2:6] * torch.tensor((w, h, w, h), device=row.device)]
        d2l.show_bboxes(fig.axes, bbox, '%.2f' % score, 'w')
display(img, output.cpu(), threshold=0.9)
```

![image-20250125153705020](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/picture/202501251537278.png)

## YOLO

SSD框里面锚框大量的重叠, 浪费很多的计算, YOLO把图片均匀的分割为SxS个锚框, 每个锚框预测B个边缘框(有B个物体和这一个边缘框比较相近)

所有的锚框之间不会重合