# 丢弃法Drop out

一个好的模型, 如果输入的数据有一部分的扰动也是可以正常分析的, 使用有噪音的数据等价于Tikhonov正则

丢弃法: 在层之间加入噪音

## 没有偏差的加入噪音

对于x加入噪音x'使得E[x'] = x, 期望值不变

![image-20250110233531804](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501102335922.png)

适中的一部分数据进行丢弃, 另一部分数据概率放大以后总概率变为1

这一层一般使用在隐藏全连接层的输出上面

![image-20250110233809902](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501102338011.png)

![image-20250110233858777](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501102338862.png)

> 这一个只在训练的时候使用, 是一个正则项, 推理的时候输入等于输出, 实际相当于随机使用一部分的小的网络, 这一部分的梯度也会降低为0
>
> 这一个方式一般是全连接层在使用, 卷积层使用的是权重衰退多一点

### 实际实现

```python
import torch
import torch.nn as nn
from d2l import torch as d2l

```

+ 初始化一下使用的drop_out函数

```python
def dropout_layer(X, dropout):
    assert 0 <= dropout <= 1
    if dropout == 1:
        return torch.zeros_like(X)
    if dropout == 0:
        return X
    # 实际的计算, 生成一个随机数, 转换为0, 1, 然后和X相乘, 除以(1 - dropout)
    mask = (torch.randn(X.shape) > dropout).float()
    return mask * X / (1.0 - dropout) # 使用乘法的计算速度比较快
```

+ 打印一下实验

```python
X = torch.arange(16, dtype=torch.float32).reshape((2, 8))
print(X)
print(dropout_layer(X, 0.))
print(dropout_layer(X, 0.5))
print(dropout_layer(X, 1.))
"""
tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])
tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])
tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 14.],
        [ 0.,  0.,  0.,  0., 24., 26.,  0.,  0.]])
tensor([[0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0.]])
"""
```

+ 实际建立网络

```python
num_inputs, num_outputs, num_hidden1, num_hidden2 = 784, 10, 256, 256

dropout1, dropout2 = 0.2, 0.5
class Net(nn.Module):
    def __init__(self, num_inputs, num_outputs, num_hidden1, num_hidden2,
                 is_training=True):
        super(Net, self).__init__()
        self.num_inputs = num_inputs
        self.training = is_training
        self.lin1 = nn.Linear(num_inputs, num_hidden1)
        self.lin2 = nn.Linear(num_hidden1, num_hidden2)
        self.lin3 = nn.Linear(num_hidden2, num_outputs)
        self.relu = nn.ReLU() # 激活函数, 返回max(0, x)

    def forward(self, X):
        H1 = self.relu(self.lin1(X.reshape((-1, self.num_inputs))))
        if self.training:
            H1 = dropout_layer(H1, dropout1)
        H2 = self.relu(self.lin2(H1))
        if self.training:
            H2 = dropout_layer(H2, dropout2)
        out = self.lin3(H2)
        return out
    
net = Net(num_inputs, num_outputs, num_hidden1, num_hidden2)
```

+ 运行

```python
num_epochs, lr, batch_size = 10, 0.5, 256
loss = nn.CrossEntropyLoss()
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
trainer = torch.optim.SGD(net.parameters(), lr=lr)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

![image-20250111105045907](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501111050148.png)



### 简单实现

```python
net = nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(),
                    nn.Dropout(dropout1), nn.Linear(256, 256), nn.ReLU(),
                    nn.Dropout(dropout2), nn.Linear(256, 10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)

net.apply(init_weights)

trainer = torch.optim.SGD(net.parameters(), lr=lr)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

![image-20250111105135572](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501111051656.png)

> drop为0的时候
>
> ![image-20250111105152631](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501111051718.png)