# GPU

检测

```python
import torch
from torch import nn

torch.device('cpu'), torch.cuda.device('cuda:0')
torch.cuda.device_count()
```

实际使用的时候可以检测一下GPU是不是可以使用

```python
def try_gpu(i=0):  #@save
    if torch.cuda.device_count() >= i + 1:
        return torch.device(f'cuda:{i}')
    return torch.device('cpu')
```

也可以返回一下所有的GPU

```python
def try_all_gpus():  #@save
    devices = [torch.device(f'cuda:{i}')
               for i in range(torch.cuda.device_count())]
    return devices if devices else [torch.device('cpu')]
```

在默认的时候可以使用`.device`查看当前的设备

```python
x = torch.tensor([1, 2, 3])
x.device
"""
device(type='cpu')
"""
```

可以在建立一个变量的时候尝试放在GPU

```python
x = torch.ones(2, 3, device=try_gpu())
x
"""
tensor([[1., 1., 1.],
        [1., 1., 1.]], device='cuda:0')
"""
```

在实际计算两个数据时候, 这两个数据尽量在一个设备上面, 数据的挪动很消耗资源, 所以会直接报错

```python
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
```

神经网络可以使用`.to`把这一个神经网络进行转移, 实际是把模型的参数放上去

## 多GPU运行

一个机器可以有多个GPU, 在实际运算时候可以把一个小批量的运算放到多个GPU里面实现加速

常见的方案有数据并行, 模型并行以及通道并行

+ 数据并行

将小批量分成n块, 每一个GPU使用完整的参数进行计算一个数据的梯度, 最后把梯度进行汇总然后计算

+ 模型并行

把模型进行拆分, 每一块GPU拿到模型以后计算前向方向的结果

### 代码实现

```python
%matplotlib inline
import torch 
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l
```

+ 初始化一个模型以及使用的参数

```python
scale = 0.01
# 初始化模型参数
W1 = nn.Parameter(torch.randn(size=(20, 1, 3, 3)) * scale, requires_grad=True)
b1 = nn.Parameter(torch.zeros(20), requires_grad=True)
W2 = nn.Parameter(torch.randn(size=(50, 20, 5, 5)) * scale, requires_grad=True)
b2 = nn.Parameter(torch.zeros(50), requires_grad=True)
W3 = nn.Parameter(torch.randn(size=(800, 128)) * scale, requires_grad=True)
b3 = nn.Parameter(torch.zeros(128), requires_grad=True)
W4 = nn.Parameter(torch.randn(size=(128, 10)) * scale, requires_grad=True)
b4 = nn.Parameter(torch.zeros(10), requires_grad=True)
params = [W1, b1, W2, b2, W3, b3, W4, b4]

def lenet(X, params):
    h1_conv = F.conv2d(input=X, weight=params[0], bias=params[1])
    h1_activation = F.relu(h1_conv)
    h1 = F.avg_pool2d(input=h1_activation, kernel_size=(2, 2), stride=(2, 2))
    h2_conv = F.conv2d(input=h1, weight=params[2], bias=params[3])
    h2_activation = F.relu(h2_conv)
    h2 = F.avg_pool2d(input=h2_activation, kernel_size=(2, 2), stride=(2, 2))
    h2 = h2.reshape(h2.shape[0], -1)
    h3_linear = torch.mm(h2, params[4]) + params[5]
    h3 = F.relu(h3_linear)
    y_hat = torch.mm(h3, params[6]) + params[7]
    return y_hat

loss = nn.CrossEntropyLoss(reduction='none')
```

+ 所有的参数放在对应使用的设备

```python
def get_params(params, devices):
    new_params = [p.to(devices) for p in params]
    for p in new_params:
        p.requires_grad_()
        p.retain_grad()
    print(new_params[0].grad)
    return new_params

new_params = get_params(params, d2l.gpu(0))
print('b1 weight:', new_params[1])
print('b1 grad:', new_params[1].grad)
```

+ 初始化一个用于计算不同设备的梯度和的函数

```python
# 用于把所有的loss加起来并分配到对应的设备
def allreduce(data):
    # 对data求和后广播
    for i in range(1, len(data)):
        # 这里使用广播机制
        data[0][:] += data[i].to(data[0].device)
    for i in range(1, len(data)):
        # 把data[0]的值广播到其他设备上
        data[i][:] = data[0].to(data[i].device)
        
data = [torch.ones((1, 2), device=d2l.try_gpu(i)) * (i + 1) for i in range(2)]
print('before allreduce:', data)
allreduce(data)
print('after allreduce:', data)

```

+ 测试一下一个分割函数

```python
data = torch.arange(20).reshape(4, 5)
devices = torch.device('cuda:0'), torch.device('cuda:0')
split = nn.parallel.scatter(data, devices)
print('input :', data)
print('load into', devices)
print('output:', split)
"""
input : tensor([[ 0,  1,  2,  3,  4],
        [ 5,  6,  7,  8,  9],
        [10, 11, 12, 13, 14],
        [15, 16, 17, 18, 19]])
load into (device(type='cuda', index=0), device(type='cuda', index=0))
output: (tensor([[0, 1, 2, 3, 4],
        [5, 6, 7, 8, 9]], device='cuda:0'), tensor([[10, 11, 12, 13, 14],
        [15, 16, 17, 18, 19]], device='cuda:0'))
"""
```

+ 一个实际用于分割数据集的函数

```python
def split_batch(X, y, devices):
    assert X.shape[0] == y.shape[0]
    return (nn.parallel.scatter(X, devices), nn.parallel.scatter(y, devices))
```

+ 写一个训练一轮使用的函数

```python
def train_batch(X, y, device_params, devices, lr):
    X_shards, y_shards = split_batch(X, y, devices)
    # 在每个GPU上分别计算损失
    ls = [
        loss(lenet(X_shard, device_W), y_shard).sum() 
        for X_shard, y_shard, device_W in zip(X_shards, y_shards, device_params)
    ]
    for l in ls:
        l.backward()
    # 把梯度加起来
    with torch.no_grad():
        for i in range(len(device_params[0])):
            allreduce([device_params[c][i].grad for c in range(len(devices))])
    # 在每个GPU上分别更新模型参数
    for param in device_params:
        d2l.sgd(param, lr, X.shape[0])  # 在这里，我们使用全尺寸的小批量
```

+ 实际的训练函数

```python
def train(num_gpus, batch_size, lr):
    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
    devices = [d2l.try_gpu(i) for i in range(num_gpus)]
    device_params = [get_params(params, d) for d in devices]
    num_epochs = 10
    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])
    timer = d2l.Timer()
    for epoch in range(num_epochs):
        timer.start()
        for X, y in train_iter:
            train_batch(X, y, device_params, devices, lr)
            torch.cuda.synchronize()
        timer.stop()
        animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(
            lambda x: lenet(x, device_params[0]), test_iter, devices[0]),))
    print(f'test acc: {animator.Y[0][-1]:.2f}, {timer.avg():.1f} sec/epoch '
          f'on {str(devices)}')
    
train(num_gpus=1, batch_size=256, lr=0.2)
```

![image-20250117151040478](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501171510709.png)

> 实际使用的时候不同的GPU的batchsize最好可以不变, 所以需要调大batch_size以及lr

### 简单实现

```python
# 初始化一个网络模型
def resnet18(num_classes, in_channels=1):
    """Return a ResNet-18 model."""
    def resnet_block(in_channels, out_channels, num_residuals, first_block=False):
        blk = []
        for i in range(num_residuals):
            if i == 0 and not first_block:
                blk.append(d2l.Residual(out_channels, use_1x1conv=True, strides=2))
            else:
                blk.append(d2l.Residual(out_channels))
        return blk

    net = nn.Sequential(
        nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3),
        nn.BatchNorm2d(64), nn.ReLU(),
        nn.MaxPool2d(kernel_size=3, stride=2, padding=1))

    net.add_module("resnet_block1", nn.Sequential(*resnet_block(64, 64, 2, first_block=True)))
    net.add_module("resnet_block2", nn.Sequential(*resnet_block(64, 128, 2)))
    net.add_module("resnet_block3", nn.Sequential(*resnet_block(128, 256, 2)))
    net.add_module("resnet_block4", nn.Sequential(*resnet_block(256, 512, 2)))
    net.add_module("global_avg_pool", nn.AdaptiveAvgPool2d((1, 1)))
    net.add_module("fc", nn.Sequential(nn.Flatten(), nn.Linear(512, num_classes)))
    return net
```

```python
def train(net, nums_gpu, batch_size, lr):
    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)
    device = [d2l.try_gpu(i) for i in range(nums_gpu)]

    def init_weights(m):
        if type(m) in [nn.Linear, nn.Conv2d]:
            nn.init.normal_(m.weight, std=0.01)

    net.apply(init_weights)
    # 把网络放到多个GPU上, 主要是使用这一个, 使用一个网络, 复制到多个设备, 返回新的网络
    net = nn.DataParallel(net, device_ids=device)
    trainer = torch.optim.SGD(net.parameters(), lr=lr)
    loss = nn.CrossEntropyLoss()
    timer, num_epochs = d2l.Timer(), 10
    animator = d2l.Animator("epoch", "test acc", xlim=[1, num_epochs])
    for epoch in range(num_epochs):
        net.train()
        timer.start()
        for X, y in train_iter:
            trainer.zero_grad()
            X, y = X.to(device[0]), y.to(device[0])
            l = loss(net(X), y)
            l.backward()
            trainer.step()
        timer.stop()
        animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(net, test_iter),))
    print(f"test acc: {animator.Y[0][-1]:.2f}, {timer.avg():.1f} sec/epoch on {str(device)}")
```

![image-20250117160809185](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202501171608314.png)

## 多机器

尽量减少机器之间的通信, 同一个机器里面的不同卡之间先进行计算, 最后再把所有的数据进行汇总, 数据的通信以及数据的计算是可以同步进行的

在进行计算back的时候, 每计算一层发送一次