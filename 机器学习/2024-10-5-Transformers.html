<!DOCTYPE html>

<html lang="zh-CN"  class="">


<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <meta name="keywords" content="">
    
    
    <meta name="description" content="">
    
    <meta name="generator" content="teedoc">
    <meta name="theme" content="teedoc-plugin-theme-default">
    
        
        <meta name="markdown-generator" content="teedoc-plugin-markdown-parser">
        
        <script>
MathJax = {"loader": {"load": ["output/svg"]}, "tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]]}, "svg": {"fontCache": "global"}};
</script>
        
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
        <meta name="html-generator" content="teedoc-plugin-jupyter-notebook-parser">
        
        <script src="/note/static/js/theme_default/pre_main.js"></script>
        
        <link rel="stylesheet" href="/note/static/css/theme_default/prism.min.css" type="text/css"/>
        
        <link rel="stylesheet" href="/note/static/css/theme_default/viewer.min.css" type="text/css"/>
        
        <link rel="stylesheet" href="/note/static/css/theme_default/dark.css" type="text/css"/>
        
        <link rel="stylesheet" href="/note/static/css/theme_default/light.css" type="text/css"/>
        
        <script src="/note/static/js/theme_default/jquery.min.js"></script>
        
        <script src="/note/static/js/theme_default/split.js"></script>
        
        <link rel="stylesheet" href="/note/static/css/search/style.css" type="text/css"/>
        
        <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?4d52982572d5512e9762879ebf063c86";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
        
        <meta name="blog-generator" content="teedoc-plugin-blog">
        
        <link rel="stylesheet" href="/note/static/css/gitalk/gitalk.css" type="text/css"/>
        
        <link rel="stylesheet" href="/note/static/css/gitalk/custom_gitalk.css" type="text/css"/>
        
        <link rel="stylesheet" href="/note/static/css/custom.css" type="text/css"/>
        
    
    
    <title>Transformers - XvSenfeng's Note</title>
    
    <script type="text/javascript">js_vars = {"teedoc-plugin-ad-hint": {"type": "hint", "label": "â˜†", "content": "è¿™æ˜¯ä¸€ä¸ªæ”¯æŒå›½é™…åŒ–çš„æ¶ˆæ¯ç¤ºä¾‹</br>å–œæ¬¢é¡¹ç›®è¯·<a target=\"_blank\" href=\"https://github.com/teedoc/teedoc\">ç‚¹ä¸‹ â˜† star </a>å“¦~ğŸ¦€ğŸ¦€", "show_times": 2, "show_after_s": 432000, "date": "2021-11-16 14:40", "color": "#a0421d", "link_color": "#e53935", "link_bg_color": "#e6ae5c", "bg_color": "#ffcf89", "color_hover": "white", "bg_color_hover": "#f57c00", "close_color": "#eab971"}}</script>
    <script type="text/javascript">metadata = {"tags": ["AI æœºå™¨å­¦ä¹ "], "date": "2026-02-05", "update": [], "ts": 1770297751, "author": "", "brief": "", "cover": "", "layout": "post"}</script>
</head>


<body class="type_doc">
    
    <div id="navbar">
        <div id="navbar_menu">
            <a class="site_title" href="/note/">
                
                    <img class="site_logo" src="/note/static/image/logo.png" alt="XvSenfeng logo">
                
                
                    <h2>XvSenfeng</h2>
                
        </a>
            <a id="navbar_menu_btn"></a>
        </div>
        <div id="navbar_items">
            <div>
                <ul id="nav_left">
<li class=""><a  href="/note/blog/">åšå®¢</a></li>
<li class=""><a  href="/note/Linux/">Linux</a></li>
<li class=""><a  href="/note/ä»£ç åˆ†æ/">ä»£ç åˆ†æ</a></li>
<li class=""><a  href="/note/ä½¿ç”¨è½¯ä»¶/">ä½¿ç”¨è½¯ä»¶</a></li>
<li class=""><a  href="/note/åµŒå…¥å¼/">åµŒå…¥å¼</a></li>
<li class=""><a  href="/note/æ‰‹æœºå®‰å“/">æ‰‹æœºå®‰å“</a></li>
<li class="active"><a  href="/note/æœºå™¨å­¦ä¹ /">æœºå™¨å­¦ä¹ </a></li>
<li class=""><a  href="/note/ç¼–ç¨‹åŸºç¡€/">ç¼–ç¨‹åŸºç¡€</a></li>
<li class=""><a  href="/note/ç½‘ç»œ/">ç½‘ç»œ</a></li>
</ul>

            </div>
            <div>
                <ul id="nav_right">
<li class=""><a target="_blank" href="https://github.com/XuSenfeng/note/">github</a></li>
</ul>

                <ul class="nav_plugins"><li><a id="google_translate_element"><img class="icon" src="/note/static/image/google_translate/translate.svg"/>Translate</a></li></ul><ul class="nav_plugins"><li><a id="themes" class="light"></a></li></ul><ul class="nav_plugins"><li><a id="search"><span class="icon"></span><span class="placeholder">æœç´¢</span>
                            <div id="search_hints">
                                <span id="search_input_hint">è¾“å…¥å…³é”®è¯ï¼Œå¤šå…³é”®è¯ç©ºæ ¼éš”å¼€</span>
                                <span id="search_loading_hint">æ­£åœ¨åŠ è½½ï¼Œè¯·ç¨å€™ã€‚ã€‚ã€‚</span>
                                <span id="search_download_err_hint">ä¸‹è½½æ–‡ä»¶å¤±è´¥ï¼Œè¯·åˆ·æ–°é‡è¯•æˆ–æ£€æŸ¥ç½‘ç»œ</span>
                                <span id="search_other_docs_result_hint">æ¥è‡ªå…¶å®ƒæ–‡æ¡£çš„ç»“æœ</span>
                                <span id="search_curr_doc_result_hint">å½“å‰æ–‡æ¡£æœç´¢ç»“æœ</span>
                            </div></a></li></ul>
            </div>
        </div>
    </div>
    
    <div id="wrapper">
        <div id="sidebar_wrapper">
            <div id="sidebar">
                <div id="sidebar_title">
                    
                </div>
                <ul class="show">
<li class="active with_link"><a href="/note/æœºå™¨å­¦ä¹ /2024-10-5-Transformers.html"><span class="label">2024-10-5-Transformers</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /2024-10-6-Juoyter.html"><span class="label">2024-10-6-Juoyter</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /2024-10-7-Pandasåº“.html"><span class="label">2024-10-7-Pandasåº“</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /2024-10-9-transformså®æˆ˜.html"><span class="label">2024-10-9-transformså®æˆ˜</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /2024-9-21-LLM.html"><span class="label">2024-9-21-LLM</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /2024-9-22-vLLM.html"><span class="label">2024-9-22-vLLM</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /2024-9-22-æ·±åº¦å­¦ä¹ ç¯å¢ƒ.html"><span class="label">2024-9-22-æ·±åº¦å­¦ä¹ ç¯å¢ƒ</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /2024-9-23-01pytorch.html"><span class="label">2024-9-23-01pytorch</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /2024-9-8-æœºå™¨å­¦ä¹ .html"><span class="label">2024-9-8-æœºå™¨å­¦ä¹ </span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /index.html"><span class="label">README</span><span class=""></span></a></li>
<li class="not_active no_link"><a><span class="label">rvæ¨¡å‹éƒ¨ç½²</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /rvæ¨¡å‹éƒ¨ç½²/2025-11-28-00-é©±åŠ¨ç§»æ¤.html"><span class="label">2025-11-28-00-é©±åŠ¨ç§»æ¤</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /rvæ¨¡å‹éƒ¨ç½²/2025-11-28-01-ç¯å¢ƒæ­å»º.html"><span class="label">2025-11-28-01-ç¯å¢ƒæ­å»º</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /rvæ¨¡å‹éƒ¨ç½²/2025-12-2-02-åŸºç¡€æ¦‚å¿µ.html"><span class="label">2025-12-2-02-åŸºç¡€æ¦‚å¿µ</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /rvæ¨¡å‹éƒ¨ç½²/2025-12-3-03-RKNN_Toolkit2.html"><span class="label">2025-12-3-03-RKNN_Toolkit2</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /rvæ¨¡å‹éƒ¨ç½²/2025-12-3-04-RKLLM.html"><span class="label">2025-12-3-04-RKLLM</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /rvæ¨¡å‹éƒ¨ç½²/2026-2-2-05-RKNNå¼€å‘.html"><span class="label">2026-2-2-05-RKNNå¼€å‘</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">vllmä»£ç åˆ†æ</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /vllmä»£ç åˆ†æ/2024-12-31-00-æ•´ä½“æ¡†æ¶.html"><span class="label">2024-12-31-00-æ•´ä½“æ¡†æ¶</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /vllmä»£ç åˆ†æ/2024-12-31-01-collect_env.html"><span class="label">2024-12-31-01-collect_env</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">å…·èº«æ™ºèƒ½</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /å…·èº«æ™ºèƒ½/2026-2-2-åŸºç¡€æ¦‚å¿µ.html"><span class="label">2026-2-2-åŸºç¡€æ¦‚å¿µ</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /å…·èº«æ™ºèƒ½/2026-2-3-02-ä»¿çœŸ.html"><span class="label">2026-2-3-02-ä»¿çœŸ</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /å…·èº«æ™ºèƒ½/2026-2-3-03-å§¿æ€è§£ç®—.html"><span class="label">2026-2-3-03-å§¿æ€è§£ç®—</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /å…·èº«æ™ºèƒ½/2026-2-4-04-SO-ARM101.html"><span class="label">2026-2-4-04-SO-ARM101</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">å®é™…åº”ç”¨</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /å®é™…åº”ç”¨/2025-1-30-ollama.html"><span class="label">2025-1-30-ollama</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /å®é™…åº”ç”¨/2025-12-10-è¯­éŸ³è¯†åˆ«.html"><span class="label">2025-12-10-è¯­éŸ³è¯†åˆ«</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /å®é™…åº”ç”¨/2025-12-22-QLearning.html"><span class="label">2025-12-22-QLearning</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /å®é™…åº”ç”¨/2025-2-16-dify.html"><span class="label">2025-2-16-dify</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /å®é™…åº”ç”¨/2025-2-16-langChain.html"><span class="label">2025-2-16-langChain</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /å®é™…åº”ç”¨/2025-2-23-langchain2.html"><span class="label">2025-2-23-langchain2</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /å®é™…åº”ç”¨/2025-2-26-Loraå¾®è°ƒ.html"><span class="label">2025-2-26-Loraå¾®è°ƒ</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /å®é™…åº”ç”¨/2025-3-20-COZE.html"><span class="label">2025-3-20-COZE</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /å®é™…åº”ç”¨/2025-4-2-MCP.html"><span class="label">2025-4-2-MCP</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">åµŒå…¥å¼ç§»æ¤</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /åµŒå…¥å¼ç§»æ¤/00ç»†èŠ.html"><span class="label">00ç»†èŠ</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /åµŒå…¥å¼ç§»æ¤/2025-12-18-01-æ¨¡å‹é‡åŒ–.html"><span class="label">2025-12-18-01-æ¨¡å‹é‡åŒ–</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">ææ²è¯¾ç¨‹</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/0000-0-0-00åŸºç¡€numpy.html"><span class="label">0000-0-0-00åŸºç¡€numpy</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/0000-0-0-00åŸºç¡€pandas.html"><span class="label">0000-0-0-00åŸºç¡€pandas</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2024-10-31-01.html"><span class="label">2024-10-31-01</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2024-11-03-02æ•°æ®ç±»å‹.html"><span class="label">2024-11-03-02æ•°æ®ç±»å‹</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-08-03åŸºç¡€å‡½æ•°.html"><span class="label">2025-1-08-03åŸºç¡€å‡½æ•°</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-08-04æ•°æ®é›†.html"><span class="label">2025-1-08-04æ•°æ®é›†</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-08-05æ„ŸçŸ¥æœº.html"><span class="label">2025-1-08-05æ„ŸçŸ¥æœº</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-09æ¨¡å‹é€‰æ‹©.html"><span class="label">2025-1-09æ¨¡å‹é€‰æ‹©</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-10-10ä¸¢å¼ƒæ³•.html"><span class="label">2025-1-10-10ä¸¢å¼ƒæ³•</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-11-11æ•°å€¼ç¨³å®šæ€§.html"><span class="label">2025-1-11-11æ•°å€¼ç¨³å®šæ€§</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-12-12å±‚å’Œå—.html"><span class="label">2025-1-12-12å±‚å’Œå—</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-12-GPU.html"><span class="label">2025-1-12-GPU</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-12-å®æˆ˜æ¯”èµ›.html"><span class="label">2025-1-12-å®æˆ˜æ¯”èµ›</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-13-13å·ç§¯.html"><span class="label">2025-1-13-13å·ç§¯</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-13-14æ± åŒ–.html"><span class="label">2025-1-13-14æ± åŒ–</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-14-15LeNet.html"><span class="label">2025-1-14-15LeNet</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-14-16AlexNet.html"><span class="label">2025-1-14-16AlexNet</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-14-17VGG.html"><span class="label">2025-1-14-17VGG</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-14-18NiN.html"><span class="label">2025-1-14-18NiN</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-14-19GoogLeNet.html"><span class="label">2025-1-14-19GoogLeNet</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-15-20æ‰¹é‡å½’ä¸€åŒ–.html"><span class="label">2025-1-15-20æ‰¹é‡å½’ä¸€åŒ–</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-15-21æ®‹å·®ç½‘ç»œResNet.html"><span class="label">2025-1-15-21æ®‹å·®ç½‘ç»œResNet</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-16-èŠ¯ç‰‡.html"><span class="label">2025-1-16-èŠ¯ç‰‡</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-17-22æ•°æ®å¢å¹¿.html"><span class="label">2025-1-17-22æ•°æ®å¢å¹¿</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-17-23å¾®è°ƒ.html"><span class="label">2025-1-17-23å¾®è°ƒ</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-18-24CIFARæ•°æ®é›†.html"><span class="label">2025-1-18-24CIFARæ•°æ®é›†</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-19-25ç›®æ ‡æ£€æµ‹.html"><span class="label">2025-1-19-25ç›®æ ‡æ£€æµ‹</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-23-26åŒºåŸŸå·ç§¯ç¥ç»ç½‘ç»œ.html"><span class="label">2025-1-23-26åŒºåŸŸå·ç§¯ç¥ç»ç½‘ç»œ</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-1-27è¯­ä¹‰åˆ†å‰².html"><span class="label">2025-2-1-27è¯­ä¹‰åˆ†å‰²</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-10-33é•¿çŸ­æœŸè®°å¿†LSTM.html"><span class="label">2025-2-10-33é•¿çŸ­æœŸè®°å¿†LSTM</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-10-34æ·±åº¦å¾ªç¯ç½‘ç»œ.html"><span class="label">2025-2-10-34æ·±åº¦å¾ªç¯ç½‘ç»œ</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-10-35-BPTT.html"><span class="label">2025-2-10-35-BPTT</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-10-36åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œ.html"><span class="label">2025-2-10-36åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œ</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-11-37ç¼–ç å™¨è§£ç å™¨.html"><span class="label">2025-2-11-37ç¼–ç å™¨è§£ç å™¨</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-11-38seq2seq.html"><span class="label">2025-2-11-38seq2seq</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-11-39æŸæœç´¢.html"><span class="label">2025-2-11-39æŸæœç´¢</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-11-40æ³¨æ„åŠ›æœºåˆ¶.html"><span class="label">2025-2-11-40æ³¨æ„åŠ›æœºåˆ¶</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-13-41æ³¨æ„åŠ›seq2seq.html"><span class="label">2025-2-13-41æ³¨æ„åŠ›seq2seq</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-15-42è‡ªæ³¨æ„åŠ›.html"><span class="label">2025-2-15-42è‡ªæ³¨æ„åŠ›</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-15-43Transformer.html"><span class="label">2025-2-15-43Transformer</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-15-44BERT.html"><span class="label">2025-2-15-44BERT</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-5-28æ ·å¼è¿ç§».html"><span class="label">2025-2-5-28æ ·å¼è¿ç§»</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-5-29åºåˆ—æ¨¡å‹.html"><span class="label">2025-2-5-29åºåˆ—æ¨¡å‹</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-6-30æ–‡å­—å¤„ç†.html"><span class="label">2025-2-6-30æ–‡å­—å¤„ç†</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-7-31-å¾ªç¯ç¥ç»ç½‘ç»œRNN.html"><span class="label">2025-2-7-31-å¾ªç¯ç¥ç»ç½‘ç»œRNN</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-9-32-é—¨æ§å¾ªç¯å•å…ƒGRU.html"><span class="label">2025-2-9-32-é—¨æ§å¾ªç¯å•å…ƒGRU</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">è§†è§‰è¯†åˆ«</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/2025-12-12-04-ç»å…¸ç®—æ³•.html"><span class="label">2025-12-12-04-ç»å…¸ç®—æ³•</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/2025-12-12-05-YoloV1.html"><span class="label">2025-12-12-05-YoloV1</span><span class=""></span></a></li>
<li class="not_active no_link"><a><span class="label">K230</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/K230/2026-1-1-01-SDKç¼–è¯‘.html"><span class="label">2026-1-1-01-SDKç¼–è¯‘</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/K230/2026-1-10-03-Linuxä»£ç å¼€å‘.html"><span class="label">2026-1-10-03-Linuxä»£ç å¼€å‘</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/K230/2026-1-3-02-åŸºç¡€åŸç†.html"><span class="label">2026-1-3-02-åŸºç¡€åŸç†</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">MaixCAM</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/MaixCAM/2025-12-11-02-æ¨¡å‹è®­ç»ƒ.html"><span class="label">2025-12-11-02-æ¨¡å‹è®­ç»ƒ</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/MaixCAM/2025-12-11-03-å¸¸ç”¨çš„æ¨¡å‹.html"><span class="label">2025-12-11-03-å¸¸ç”¨çš„æ¨¡å‹</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/MaixCAM/2025-12-14-04-Yoloæ¨¡å‹è½¬æ¢.html"><span class="label">2025-12-14-04-Yoloæ¨¡å‹è½¬æ¢</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/MaixCAM/2025-12-14-05-MaixCDKåŸºç¡€ä½¿ç”¨.html"><span class="label">2025-12-14-05-MaixCDKåŸºç¡€ä½¿ç”¨</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/MaixCAM/2025-12-16-06-maixcdkå·¥å…·.html"><span class="label">2025-12-16-06-maixcdkå·¥å…·</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/MaixCAM/2025-12-17-07-ç»„ä»¶.html"><span class="label">2025-12-17-07-ç»„ä»¶</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/MaixCAM/2025-12-17-08-Cameraç¤ºä¾‹.html"><span class="label">2025-12-17-08-Cameraç¤ºä¾‹</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/MaixCAM/2025-12-17-09-APP.html"><span class="label">2025-12-17-09-APP</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/MaixCAM/2025-12-18-06-æ¨¡å‹ç›¸å…³å‚æ•°.html"><span class="label">2025-12-18-06-æ¨¡å‹ç›¸å…³å‚æ•°</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/MaixCAM/2025-12-19-07-AIç¼–è¯‘å™¨.html"><span class="label">2025-12-19-07-AIç¼–è¯‘å™¨</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/MaixCAM/2025-12-19-08-æ¨¡å‹å®ç°.html"><span class="label">2025-12-19-08-æ¨¡å‹å®ç°</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/MaixCAM/2025-12-20-09-MaixPyç¼–è¯‘.html"><span class="label">2025-12-20-09-MaixPyç¼–è¯‘</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/MaixCAM/2025-12-3-00-èµ„æ–™.html"><span class="label">2025-12-3-00-èµ„æ–™</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/MaixCAM/2025-12-5-01ç¼–è¯‘ä¸‹è½½.html"><span class="label">2025-12-5-01ç¼–è¯‘ä¸‹è½½</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">yolo</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/yolo/2025-12-12-02-åŸºç¡€ä½¿ç”¨.html"><span class="label">2025-12-12-02-åŸºç¡€ä½¿ç”¨</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/yolo/2025-12-12-03-æ•°æ®é›†.html"><span class="label">2025-12-12-03-æ•°æ®é›†</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/yolo/2025-12-12-04-é¢„æµ‹.html"><span class="label">2025-12-12-04-é¢„æµ‹</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/yolo/2025-12-13-05-è®­ç»ƒ.html"><span class="label">2025-12-13-05-è®­ç»ƒ</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/yolo/2025-12-13-06-yolov5åŸºç¡€ä½¿ç”¨.html"><span class="label">2025-12-13-06-yolov5åŸºç¡€ä½¿ç”¨</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/yolo/2025-12-13-07-AutoDLæœåŠ¡å™¨è®­ç»ƒ.html"><span class="label">2025-12-13-07-AutoDLæœåŠ¡å™¨è®­ç»ƒ</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/yolo/2025-12-13-08-yolov5æ¡†æ¶.html"><span class="label">2025-12-13-08-yolov5æ¡†æ¶</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/yolo/2025-12-13-09-ä¿®æ”¹ç½‘ç»œ.html"><span class="label">2025-12-13-09-ä¿®æ”¹ç½‘ç»œ</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/yolo/2025-12-13-10-æ¨¡å‹éƒ¨ç½².html"><span class="label">2025-12-13-10-æ¨¡å‹éƒ¨ç½²</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/yolo/2025-12-4-01-yoloå®‰è£….html"><span class="label">2025-12-4-01-yoloå®‰è£…</span><span class=""></span></a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
        </div>
        <div id="article">
            <div id="menu_wrapper">
                <div id="menu">
                </div>
            </div>
            <div id="content_wrapper">
                <div id="content_body">
                    <div id="article_head">
                        <div id="article_title">
                            
                            <h1>Transformers</h1>
                            
                        </div>
                        <div id="article_tags">
                            <ul>
                            
                                <li>AI æœºå™¨å­¦ä¹ </li>
                            
                            </ul>
                        </div>
                        <div id="article_info">
                        <div id="article_info_left">
                            <span class="article_author">
                                
                            </span>
                            
                                <span class="article_date" title="æœ€åä¿®æ”¹æ—¥æœŸï¼š 2026-02-05">
                                    2026-02-05
                                </span>
                            
                        </div>
                        <div id="article_info_right">
                            
                            <div id="source_link">
                                <a href="https://github.com/XuSenfeng/note/tree/master/doc/æœºå™¨å­¦ä¹ /2024-10-5-Transformers.md" target="_blank">
                                    ç¼–è¾‘æœ¬é¡µ
                                </a>
                            </div>
                            
                        </div>
                        </div>
                    </div>
                    <div id="article_tools">
                        <span></span>
                        <span id="toc_btn"></span>
                    </div>
                    <div id="update_history">
                        
                    </div>
                    <div id="article_content">
                        
                            <h1 id="Transformers">Transformers</h1>
<h2 id="HuggingFace">HuggingFace</h2>
<p>æ³¨å†Œä¸€ä¸ªè´¦æˆ·</p>
<p><a href="https://huggingface.co/welcome"  target="_blank">Hugging Face â€“ The AI community building the future.</a></p>
<h2 id="%E5%AE%89%E8%A3%85%E7%8E%AF%E5%A2%83">å®‰è£…ç¯å¢ƒ</h2>
<p>æ˜¯HuggingFaceå‡ºå“çš„ç›®å‰æœ€ç«çš„è‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·åŒ…, å®ç°å¤§é‡çš„åŸºäºè¿™ä¸€ä¸ªæ¶æ„çš„é¢„è®­ç»ƒæ¨¡å‹ä»¥åŠå…¶ä»–å„ç§æ¨¡å‹</p>
<p>è¿™æ˜¯ä¸€æ•´ä¸ªç”Ÿæ€ç¯å¢ƒ</p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410052300340.jpg" alt="Screenshot_20241005_230004" /></p>

<pre class="language-bash"><code class="language-bash">conda create -n transforms python=3.9

conda activate transforms

pip config set global.index-url https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple

pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124

pip install transformers datasets evaluate peft accelerate gradio optimum sentencepiece

pip install jupyterlab scikit-learn pandas matplotlib tensorboard nltk rouge
</code></pre>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410061016954.png" alt="image-20241006101635925" /></p>
<p>åœ¨ä½¿ç”¨vscodeå¼€å‘jupyterçš„æ—¶å€™, éœ€è¦é€‰æ‹©ä¸€ä¸‹å†…æ ¸</p>
<h3 id="%E7%AE%80%E5%8D%95%E6%B5%8B%E8%AF%95">ç®€å•æµ‹è¯•</h3>

<pre class="language-python"><code class="language-python">import gradio as gr
from transformers import *

gr.Interface.from_pipeline(pipeline(&quot;text-classification&quot;, model=&quot;uer/roberta-base-finetuned-dianping-chinese&quot;)).launch()
</code></pre>
<p>å¯åŠ¨ä¸€ä¸ªè¯„ä»·åˆ†ç±»å™¨, è¯„ä»·è¿™ä¸€ä¸ªè¯„ä»·çš„è¯„åˆ†</p>

<pre class="language-python"><code class="language-python"># å¯¼å…¥gradio
import gradio as gr
# å¯¼å…¥transformersç›¸å…³åŒ…
from transformers import pipeline
# é€šè¿‡InterfaceåŠ è½½pipelineå¹¶å¯åŠ¨é˜…è¯»ç†è§£æœåŠ¡
# å¦‚æœæ— æ³•é€šè¿‡è¿™ç§æ–¹å¼åŠ è½½ï¼Œå¯ä»¥é‡‡ç”¨ç¦»çº¿åŠ è½½çš„æ–¹å¼
gr.Interface.from_pipeline(pipeline(&quot;question-answering&quot;, model=&quot;uer/roberta-base-chinese-extractive-qa&quot;)).launch()
</code></pre>
<blockquote>
<p>ä¸€ä¸ªæœºå™¨é˜…è¯»ç†è§£çš„æ¨¡å‹</p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410061123128.png" alt="image-20241006112326045" /></p>
</blockquote>
<h2 id="%E9%80%89%E5%8F%96">é€‰å–</h2>
<p><a href="https://huggingface.co/tasks"  target="_blank">Tasks - Hugging Face</a></p>
<p><a href="https://huggingface.co/tasks/question-answering"  target="_blank">What is Question Answering? - Hugging Face</a></p>
<p>å¯ä»¥åœ¨è¿™é‡Œé¢é€‰å–ä»–æ¨èçš„æ¨¡å‹, æ•°æ®é›†, æµ‹è¯•ç®—æ³•</p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410081817288.png" alt="image-20241008181712046" /></p>
<h2 id="%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6Pipeline">åŸºç¡€ç»„ä»¶Pipeline</h2>
<p>æŠŠæ•°æ®è¿›è¡Œé¢„å¤„ç†, æ¨¡å‹è°ƒç”¨ä»¥åŠæ¨¡å‹è°ƒç”¨ä»¥åŠç»“æœåå¤„ç†ç»„è£…ä¸ºä¸€ä¸ªæµæ°´çº¿, ä½¿å¾—æˆ‘ä»¬è¾“å…¥çš„æ–‡æœ¬å¯ä»¥è·å–ä¸ºæœ€åçš„ç»“æœ</p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410061132916.png" alt="image-20241006113243862" /></p>
<ul>
<li>å¯ä»¥å¤„ç†çš„ä»»åŠ¡</li>
</ul>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410061140780.png" alt="image-20241006114036732" /></p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410061140248.png" alt="image-20241006114053198" /></p>
<blockquote>
<p>audio-classification ï¼šéŸ³é¢‘åˆ†ç±»<br />
automatic-speech-recognition ï¼šè‡ªåŠ¨è¯­éŸ³è¯†åˆ«<br />
text-to-audio ï¼šæ–‡æœ¬è½¬éŸ³é¢‘<br />
feature-extraction ï¼šç‰¹å¾æå–<br />
text-classification ï¼šæ–‡æœ¬åˆ†ç±»<br />
token-classification ï¼šæ ‡è®°åˆ†ç±»<br />
question-answering ï¼šé—®ç­”ç³»ç»Ÿ<br />
table-question-answering ï¼šè¡¨æ ¼é—®ç­”<br />
visual-question-answering ï¼šè§†è§‰é—®ç­”<br />
document-question-answering ï¼šæ–‡æ¡£é—®ç­”<br />
fill-mask ï¼šå¡«å……æ©ç <br />
summarization ï¼šæ‘˜è¦ç”Ÿæˆ<br />
translation ï¼šç¿»è¯‘<br />
text2text-generation ï¼šæ–‡æœ¬ç”Ÿæˆ<br />
text-generation ï¼šæ–‡æœ¬ç”Ÿæˆ<br />
zero-shot-classification ï¼šé›¶æ ·æœ¬åˆ†ç±»<br />
zero-shot-image-classification ï¼šé›¶æ ·æœ¬å›¾åƒåˆ†ç±»<br />
zero-shot-audio-classification ï¼šé›¶æ ·æœ¬éŸ³é¢‘åˆ†ç±»<br />
image-classification ï¼šå›¾åƒåˆ†ç±»<br />
image-feature-extraction ï¼šå›¾åƒç‰¹å¾æå–<br />
image-segmentation ï¼šå›¾åƒåˆ†å‰²<br />
image-to-text ï¼šå›¾åƒè½¬æ–‡æœ¬<br />
object-detection ï¼šç‰©ä½“æ£€æµ‹<br />
zero-shot-object-detection ï¼šé›¶æ ·æœ¬ç‰©ä½“æ£€æµ‹<br />
depth-estimation ï¼šæ·±åº¦ä¼°è®¡<br />
video-classification ï¼šè§†é¢‘åˆ†ç±»<br />
mask-generation ï¼šæ©ç ç”Ÿæˆ<br />
image-to-image ï¼šå›¾åƒåˆ°å›¾åƒ</p>
</blockquote>

<pre class="language-python"><code class="language-python">from transformers.pipelines import SUPPORTED_TASKS
for k, v in SUPPORTED_TASKS.items():
    print(k, v)
</code></pre>
<blockquote>
<p>å¯ä»¥ä½¿ç”¨ä»¥ä¸Šçš„æ–¹æ³•è¿›è¡ŒæŸ¥çœ‹</p>
</blockquote>
<h3 id="%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E">ä½¿ç”¨è¯´æ˜</h3>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410061259357.png" alt="image-20241006125917307" /></p>

<pre class="language-python"><code class="language-python">class TextClassificationPipeline(Pipeline):
    &quot;&quot;&quot;
    Text classification pipeline using any `ModelForSequenceClassification`. See the [sequence classification
    examples](../task_summary#sequence-classification) for more information.

    Example:

    ```python
    &gt;&gt;&gt; from transformers import pipeline

    &gt;&gt;&gt; classifier = pipeline(model=&quot;distilbert/distilbert-base-uncased-finetuned-sst-2-english&quot;)
    &gt;&gt;&gt; classifier(&quot;This movie is disgustingly good !&quot;)
    [{'label': 'POSITIVE', 'score': 1.0}]

    &gt;&gt;&gt; classifier(&quot;Director tried too much.&quot;)
    [{'label': 'NEGATIVE', 'score': 0.996}]
</code></pre>

<pre class="language-none"><code class="language-none">Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)

This text classification pipeline can currently be loaded from [`pipeline`] using the following task identifier:
`&quot;sentiment-analysis&quot;` (for classifying sequences according to positive or negative sentiments).

If multiple classification labels are available (`model.config.num_labels &gt;= 2`), the pipeline will run a softmax
over the results. If there is a single label, the pipeline will run a sigmoid over the result.

The models that this pipeline can use are models that have been fine-tuned on a sequence classification task. See
the up-to-date list of available models on
[huggingface.co/models](https://huggingface.co/models?filter=text-classification).
&quot;&quot;&quot;

</code></pre>

<pre class="language-none"><code class="language-none">
è¿™ä¸€ä¸ªé‡Œé¢åªæœ‰æœ€ç®€å•çš„æ–¹æ³•è°ƒç”¨, åœ¨å®é™…ä½¿ç”¨å¯ä»¥çœ‹ä»–çš„`__call__`æ–¹æ³•

```python
def __call__(self, inputs, **kwargs):
        &quot;&quot;&quot;
        Classify the text(s) given as inputs.

        Args:
            inputs (`str` or `List[str]` or `Dict[str]`, or `List[Dict[str]]`):
                One or several texts to classify. In order to use text pairs for
                your classification, you can send a
                dictionary containing `{&quot;text&quot;, &quot;text_pair&quot;}` keys, or a list of
                those.
            top_k (`int`, *optional*, defaults to `1`):
                How many results to return.
            function_to_apply (`str`, *optional*, defaults to `&quot;default&quot;`):
                The function to apply to the model outputs in order to retrieve the
                scores. Accepts four different
                values:

                If this argument is not specified, then it will apply the following
                functions according to the number
                of labels:

                - If the model has a single label, will apply the sigmoid function
                on the output.
                - If the model has several labels, will apply the softmax function
                on the output.

                Possible values are:

                - `&quot;sigmoid&quot;`: Applies the sigmoid function on the output.
                - `&quot;softmax&quot;`: Applies the softmax function on the output.
                - `&quot;none&quot;`: Does not apply any function on the output.

        Return:
            A list or a list of list of `dict`: Each result comes as list of 
            dictionaries with the following keys:

            - **label** (`str`) -- The label predicted.
            - **score** (`float`) -- The corresponding probability.

            If `top_k` is used, one such dictionary is returned per label.
        &quot;&quot;&quot;
</code></pre>
<h3 id="%E5%8A%A0%E8%BD%BD">åŠ è½½</h3>

<pre class="language-python"><code class="language-python">from transformers import *
pipe = pipeline('text-classification')
</code></pre>
<blockquote>
<p>ä½¿ç”¨è¿™ä¸€ä¸ªå‘½ä»¤çš„æ—¶å€™ä¼šè‡ªåŠ¨ä¸‹è½½é»˜è®¤ä½¿ç”¨çš„æ¨¡å‹, ä¸€èˆ¬æ˜¯ä¸€ä¸ªè‹±æ–‡æ¨¡å‹</p>

<pre class="language-python"><code class="language-python">pipe(&quot;very good&quot;)
</code></pre>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410061209566.png" alt="image-20241006120905391" /></p>
<p>å…¶ä»–çš„æ¨¡å‹å¯ä»¥åœ¨huggingfaceè¿™ä¸€ä¸ªç½‘é¡µè¿›è¡Œå¯»æ‰¾, ä¹‹å‰ä½¿ç”¨çš„æ˜¯<a href="https://huggingface.co/uer/roberta-base-finetuned-dianping-chinese"  target="_blank">uer/roberta-base-finetuned-dianping-chinese Â· Hugging Face</a>è¿™ä¸€ä¸ªæ¨¡å‹, å¯ä»¥åœ¨æ ‡ç­¾è¿›è¡Œåˆ†ç±»ç­›é€‰</p>
</blockquote>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410061200115.png" alt="image-20241006120056003" /></p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410061207795.png" alt="image-20241006120757759" /></p>
<p>è¿™ä¸ªå°±æ˜¯æ¨¡å‹çš„åå­—</p>

<pre class="language-c"><code class="language-c">pipe = pipeline('text-classification', model='uer/roberta-base-finetuned-dianping-chinese')
</code></pre>
<ul>
<li>æ–¹æ³•äºŒ</li>
</ul>

<pre class="language-python"><code class="language-python">from transformers import *

model = AutoModelForSequenceClassification.from_pretrained('uer/roberta-base-finetuned-dianping-chinese')
tokenizer = AutoTokenizer.from_pretrained('uer/roberta-base-finetuned-dianping-chinese')
pipe = pipeline(&quot;text-classification&quot;, model=model, tokenizer=tokenizer)
</code></pre>
<p>æŠŠæ¨¡å‹ä»¥åŠåˆ†è¯å™¨å•ç‹¬åˆ›å»ºå‡ºæ¥, è¿™ä¸¤ä¸ªéœ€è¦åŒæ—¶æŒ‡å®š</p>
<blockquote>
<p>ä½¿ç”¨è¿™ä¸¤ç§æ–¹æ³•åŠ è½½çš„æ¨¡å‹å®é™…æ˜¯ä½¿ç”¨CPUè¿›è¡Œè¿è¡Œçš„</p>

<pre class="language-python"><code class="language-python">print(pipe.model.device)
</code></pre>
</blockquote>
<h3 id="%E4%BD%BF%E7%94%A8GPU">ä½¿ç”¨GPU</h3>

<pre class="language-python"><code class="language-python">pipe = pipeline('text-classification', model='uer/roberta-base-finetuned-dianping-chinese', device=0)
</code></pre>
<h3 id="%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB%E7%A4%BA%E4%BE%8B">å›¾åƒè¯†åˆ«ç¤ºä¾‹</h3>

<pre class="language-python"><code class="language-python">from transformers import *

checkpoint = &quot;google/owlvit-base-patch32&quot;
detector = pipeline(model=checkpoint, task=&quot;zero-shot-object-detection&quot;)

import requests
from PIL import Image
# è·å–æ£€æµ‹çš„å›¾ç‰‡
url = &quot;https://unsplash.com/photos/oj0zeY2Ltk4/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8MTR8fHBpY25pY3xlbnwwfHx8fDE2Nzc0OTE1NDk&amp;force=true&amp;w=640&quot;
im = Image.open(requests.get(url, stream=True).raw)
im.show()
# è¿›è¡Œæ£€æµ‹
predictions = detector(
    im,
    candidate_labels=[&quot;hat&quot;, &quot;sunglasses&quot;, &quot;book&quot;],
)
print(predictions)

from PIL import ImageDraw
# ç»˜åˆ¶ä¸€ä¸‹ç»“æœ
draw = ImageDraw.Draw(im)

for prediction in predictions:
    box = prediction[&quot;box&quot;]
    label = prediction[&quot;label&quot;]
    score = prediction[&quot;score&quot;]
    xmin, ymin, xmax, ymax = box.values()
    draw.rectangle((xmin, ymin, xmax, ymax), outline=&quot;red&quot;, width=1)
    draw.text((xmin, ymin), f&quot;{label}: {round(score,2)}&quot;, fill=&quot;red&quot;)

im.show()
</code></pre>
<blockquote>

<pre class="language-c"><code class="language-c">class ZeroShotObjectDetectionPipeline(ChunkPipeline):
    &quot;&quot;&quot;
    Zero shot object detection pipeline using `OwlViTForObjectDetection`. This pipeline predicts bounding boxes of
    objects when you provide an image and a set of `candidate_labels`.

    Example:

    ```python
    &gt;&gt;&gt; from transformers import pipeline

    &gt;&gt;&gt; detector = pipeline(model=&quot;google/owlvit-base-patch32&quot;, task=&quot;zero-shot-object-detection&quot;)
    &gt;&gt;&gt; detector(
    ...     &quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;,
    ...     candidate_labels=[&quot;cat&quot;, &quot;couch&quot;],
    ... )
    [{'score': 0.287, 'label': 'cat', 'box': {'xmin': 324, 'ymin': 20, 'xmax': 640, 'ymax': 373}}, {'score': 0.254, 'label': 'cat', 'box': {'xmin': 1, 'ymin': 55, 'xmax': 315, 'ymax': 472}}, {'score': 0.121, 'label': 'couch', 'box': {'xmin': 4, 'ymin': 0, 'xmax': 642, 'ymax': 476}}]

    &gt;&gt;&gt; detector(
    ...     &quot;https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png&quot;,
    ...     candidate_labels=[&quot;head&quot;, &quot;bird&quot;],
    ... )
    [{'score': 0.119, 'label': 'bird', 'box': {'xmin': 71, 'ymin': 170, 'xmax': 410, 'ymax': 508}}]
</code></pre>

<pre class="language-none"><code class="language-none">Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)

This object detection pipeline can currently be loaded from [`pipeline`] using the following task identifier:
`&quot;zero-shot-object-detection&quot;`.

See the list of available models on
[huggingface.co/models](https://huggingface.co/models?filter=zero-shot-object-detection).
&quot;&quot;&quot;
</code></pre>

<pre class="language-none"><code class="language-none">
</code></pre>
</blockquote>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410061319203.png" alt="image-20241006131935059" /></p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410061319339.png" alt="image-20241006131947210" /></p>
<h3 id="%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B">å¤„ç†æµç¨‹</h3>
<ul>
<li>åˆå§‹åŒ–Tokenizer</li>
</ul>

<pre class="language-python"><code class="language-python">tokenizer = AutoTokenizer.from_pretrained('uer/roberta-base-finetuned-dianping-chinese')
</code></pre>
<ul>
<li>åˆå§‹åŒ–Model</li>
</ul>

<pre class="language-python"><code class="language-python">model = AutoModelForSequenceClassification.from_pretrained('uer/roberta-base-finetuned-dianping-chinese')
</code></pre>
<ul>
<li>æ•°æ®é¢„å¤„ç†</li>
</ul>

<pre class="language-python"><code class="language-python">imput_text = &quot;å¯ä»¥&quot;
inputs = tokenizer(input_text, return_tensors='pt') # è¿”å›å€¼æ˜¯ä¸€ä¸ªpytorch tensor
</code></pre>
<ul>
<li>æ¨¡å‹é¢„æµ‹</li>
</ul>

<pre class="language-python"><code class="language-python">res = model(**inputs).logits
</code></pre>
<ul>
<li>ç»“æœåå¤„ç†</li>
</ul>
<p>softmaxè¾“å‡ºçš„æ˜¯æ¦‚ç‡åˆ†å¸ƒï¼ˆæ¯”å¦‚åœ¨å¤šåˆ†ç±»é—®é¢˜ä¸­ï¼ŒSoftmaxè¾“å‡ºçš„æ˜¯æ¯ä¸ªç±»åˆ«å¯¹åº”çš„æ¦‚ç‡ï¼‰</p>
<img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410062224000.png"/>

<pre class="language-python"><code class="language-python">logits = res.logits
logits = torch.softmax(logits, dim=1) # åœ¨ç»´åº¦1è¿›è¡Œè®¡ç®—

pred = torch.argmax(logits, dim=1).item()
</code></pre>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410062228807.png" alt="image-20241006222812760" /></p>
<h2 id="Tokenizer">Tokenizer</h2>
<p>è¿›è¡Œæ•°æ®çš„é¢„å¤„ç†</p>
<ol>
<li>åˆ†è¯: ä½¿ç”¨åˆ†è¯å™¨å¯¹æ–‡æœ¬æ•°æ®è¿›è¡Œåˆ†è¯</li>
<li>æ„å»ºè¯å…¸: æ ¹æ®åˆ†è¯å™¨çš„å¤„ç†ç»“æœ, Step2æ„å»ºè¯å…¸:æ ¹æ®æ•°æ®é›†åˆ†è¯çš„ç»“æœï¼Œæ„å»ºè¯å…¸æ˜ å°„(è¿™ä¸€æ­¥å¹¶ä¸ç»å¯¹ï¼Œå¦‚æœé‡‡ç”¨é¢„è®­ç»ƒè¯å‘é‡ï¼Œè¯å…¸æ˜ å°„è¦æ ¹æ®è¯å‘é‡æ–‡ä»¶è¿›è¡Œå¤„ç†);</li>
<li>æ•°æ®è½¬æ¢:æ ¹æ®æ„å»ºå¥½çš„è¯å…¸ï¼Œå°†åˆ†è¯å¤„ç†åçš„æ•°æ®åšæ˜ å°„ï¼Œå°†æ–‡æœ¬åºåˆ—è½¬æ¢ä¸ºæ•°å­—åºåˆ—;</li>
<li>æ•°æ®å¡«å……ä¸æˆªæ–­:åœ¨ä»¥batchè¾“å…¥åˆ°æ¨¡å‹çš„æ–¹å¼ä¸­ï¼Œéœ€è¦å¯¹è¿‡çŸ­çš„æ•°æ®è¿›è¡Œå¡«å……ï¼Œè¿‡é•¿çš„æ•°æ®è¿›è¡Œæˆªæ–­ï¼Œä¿è¯æ•°æ®é•¿åº¦ç¬¦åˆæ¨¡å‹èƒ½æ¥å—çš„èŒƒå›´ï¼ŒåŒæ—¶batchå†…çš„æ•°æ®ç»´åº¦å¤§å°ä¸€è‡´ã€‚</li>
</ol>
<p>åœ¨ä½¿ç”¨Tokenizerçš„æ—¶å€™å¯ä»¥è¿›è¡Œä»¥ä¸Šçš„æ‰€æœ‰å¤„ç†</p>
<h3 id="%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8">åŸºæœ¬ä½¿ç”¨</h3>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410062234321.png" alt="image-20241006223441164" /></p>

<pre class="language-python"><code class="language-python">from transformers import AutoTokenizer
</code></pre>
<blockquote>
<p>ä¸åŒçš„æ¨¡å‹ä¼šä½¿ç”¨ä¸åŒçš„Tokenizer, æ‰€ä»¥è¿™é‡Œå°è£…äº†ä¸€ä¸ªAutoTokenizer, æ ¹æ®ä¼ å…¥çš„å‚æ•°ç¡®å®šæœ€åçš„ç»“æœ</p>
</blockquote>
<ul>
<li>åŠ è½½åˆ†è¯å™¨</li>
</ul>

<pre class="language-python"><code class="language-python">sen = &quot;æˆ‘å²å‡¯æ­Œæ•¢åƒå±!&quot;
tokenizer = AutoTokenizer.from_pretrained(&quot;uer/roberta-base-finetuned-dianping-chinese&quot;)
</code></pre>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410062239203.png" alt="image-20241006223955140" /></p>
<p>è¿™ä¸€ä¸ªå¯ä»¥ä¿å­˜åœ¨æœ¬åœ°ä»¥åŠä»æœ¬åœ°è¿›è¡ŒåŠ è½½</p>

<pre class="language-python"><code class="language-python">tokenizer.(&quot;./tokenizer&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;./tokenizer&quot;)
</code></pre>
<p>ä¹‹åå¯ä»¥è¿›è¡Œåˆ†è¯</p>

<pre class="language-python"><code class="language-python">tokens = tokenizer.tokenize(sen)
</code></pre>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410062244514.png" alt="image-20241006224426467" /></p>
<ul>
<li>å¯ä»¥æŸ¥çœ‹ä¸€ä¸‹è¿™ä¸€ä¸ªåˆ†è¯å™¨çš„è¯å…¸</li>
</ul>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410062246264.png" alt="image-20241006224655209" /></p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410062247055.png" alt="image-20241006224712011" /></p>
<ul>
<li>æŠŠè¯è¯­è½¬æ¢ä¸ºid</li>
</ul>

<pre class="language-python"><code class="language-python">ids = tokenizer.convert_tokens_to_ids(tokens)
</code></pre>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410062249711.png" alt="image-20241006224944657" /></p>
<blockquote>
<p>ä¹Ÿå¯ä»¥åè¿‡æ¥è½¬æ¢</p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410062250417.png" alt="image-20241006225045369" /></p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410062252671.png" alt="image-20241006225224620" /></p>
</blockquote>
<ul>
<li>å¡«å……æˆªæ–­</li>
</ul>

<pre class="language-python"><code class="language-python"># å¡«å……
ids = tokenizer.encode(sen, add_special_tokens=False, padding='max_length', max_length=10)
</code></pre>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410062259454.png" alt="image-20241006225920394" /></p>

<pre class="language-python"><code class="language-python"># æˆªæ–­
ids = tokenizer.encode(sen, add_special_tokens=False, truncation=True, max_length=5)
</code></pre>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410062300854.png" alt="image-20241006230016792" /></p>
<p>åœ¨æˆªæ–­çš„æ—¶å€™å¦‚æœæ·»åŠ æ ‡å¿—ä½, è¿™ä¸€ä¸ªæ ‡å¿—ä½ä¸ä¼šè¢«æˆªæ–­</p>
<p>å®é™…ä½¿ç”¨çš„æ—¶å€™, éœ€è¦åŒºåˆ†ä¸€ä¸‹é‚£ä¸€éƒ¨åˆ†æ˜¯å¡«å……çš„æ•°æ®, ä»¥åŠåŒºåˆ†ä¸€ä¸‹å¥å­çš„å‰å</p>

<pre class="language-python"><code class="language-python">attention_mask = [1 if idx != 0 else 0 for idx in ids]
token_type_ids = [0] * len(ids)
</code></pre>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410062305927.png" alt="image-20241006230541868" /></p>
<h3 id="%E7%AE%80%E4%BE%BF%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95">ç®€ä¾¿ä½¿ç”¨æ–¹æ³•</h3>
<h4 id="%E5%8D%95%E4%B8%AA%E6%95%B0%E6%8D%AE">å•ä¸ªæ•°æ®</h4>

<pre class="language-python"><code class="language-python">ids = tokenizer.encode(sen)
</code></pre>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410062253211.png" alt="image-20241006225340157" /></p>
<blockquote>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410062254025.png" alt="image-20241006225442970" /></p>
<p>è¿™ä¸€ä¸ªæ¨¡å‹åœ¨å¤„ç†çš„æ—¶å€™ä¼šåŠ å…¥ç”¨äºåŒºåˆ†å¥å­çš„tokenå¯ä»¥ä½¿ç”¨å‚æ•°å–æ¶ˆ</p>

<pre class="language-python"><code class="language-python">ids = tokenizer.encode(sen, add_special_tokens=False)
str = tokenizer.decode(ids, skip_special_tokens=True)
</code></pre>
</blockquote>

<pre class="language-python"><code class="language-python">ids = tokenizer.encode_plus(sen, add_special_tokens=True, max_length=20, padding='max_length')
</code></pre>

<pre class="language-bash"><code class="language-bash">{
'input_ids': [101, 2769, 1380, 1132, 3625, 3140, 1391, 2241, 106, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
}
</code></pre>
<blockquote>
<p>ä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨<code>ids = tokenizer(sen, add_special_tokens=True, max_length=20, padding='max_length')</code></p>
<p>token_type_idsç”¨äºæ ‡è¯†ä¸åŒæ–‡æœ¬ç‰‡æ®µçš„tokenç±»å‹ã€‚åœ¨BERTæ¨¡å‹ä¸­ï¼Œè¾“å…¥æ–‡æœ¬å¯èƒ½åŒ…å«å¤šä¸ªæ–‡æœ¬ç‰‡æ®µï¼Œä¾‹å¦‚é—®é¢˜å’Œç­”æ¡ˆã€‚token_type_idså¯ä»¥ç”¨æ¥åŒºåˆ†ä¸åŒæ–‡æœ¬ç‰‡æ®µçš„tokenï¼Œå¸®åŠ©æ¨¡å‹æ•æ‰æ–‡æœ¬ä¹‹é—´çš„å…³è”ä¿¡æ¯ã€‚é€šè¿‡å°†ä¸åŒæ–‡æœ¬ç‰‡æ®µçš„tokenèµ‹äºˆä¸åŒçš„token_type_idsï¼Œæ¨¡å‹å¯ä»¥æ›´å¥½åœ°ç†è§£æ¯ä¸ªæ–‡æœ¬ç‰‡æ®µä¹‹é—´çš„å…³ç³»ã€‚</p>
<p>attention_maskç”¨äºæ§åˆ¶å“ªäº›tokenå¯¹äºæ¨¡å‹æ˜¯å¯è§çš„ï¼Œå“ªäº›tokenåº”è¯¥è¢«å±è”½æ‰ã€‚åœ¨BERTæ¨¡å‹ä¸­ï¼Œè¾“å…¥æ–‡æœ¬é€šå¸¸ä¼šè¿›è¡Œpaddingä½¿å¾—è¾“å…¥åºåˆ—é•¿åº¦ç›¸åŒã€‚é€šè¿‡åœ¨attention_maskä¸­å°†paddingçš„tokenå¯¹åº”çš„ä½ç½®è®¾ä¸º0ï¼Œæ¨¡å‹å¯ä»¥å¿½ç•¥è¿™äº›padding tokenï¼Œé¿å…å¯¹å…¶è¿›è¡Œä¸å¿…è¦çš„è®¡ç®—ï¼Œæé«˜äº†æ¨¡å‹çš„è®¡ç®—æ•ˆç‡ã€‚</p>
</blockquote>
<h4 id="%E5%A4%84%E7%90%86%E5%A4%9A%E4%B8%AA%E6%95%B0%E6%8D%AE%28%E9%80%9F%E5%BA%A6%E6%9B%B4%E5%BF%AB%29">å¤„ç†å¤šä¸ªæ•°æ®(é€Ÿåº¦æ›´å¿«)</h4>

<pre class="language-c"><code class="language-c">sen = [&quot;æˆ‘å²å‡¯æ­Œæ•¢eat dinner!&quot;, 
       &quot;å²å‡¯æ­Œæˆ‘è¦eat dinner&quot;, 
       &quot;ä¸€å¤©è¦åƒä¸‰æ–¤dinner&quot;]
ids = tokenizer.batch_encode_plus(sen, add_special_tokens=True, max_length=20, padding='max_length')
</code></pre>
<blockquote>

<pre class="language-bash"><code class="language-bash">{
'input_ids': [
[101, 2769, 1380, 1132, 3625, 3140, 1391, 2241, 106, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
[101, 1380, 1132, 3625, 2769, 6206, 1391, 2241, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 671, 1921, 6206, 1391, 676, 3165, 2241, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
], 
'token_type_ids': [
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
], 
'attention_mask': [
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
]}
</code></pre>
</blockquote>
<h3 id="Fast-/-Slow-Tokenizer">Fast / Slow Tokenizer</h3>
<p>Fast Tokenizeræ˜¯åŸºäºRustå®ç°çš„, Slow Tokenizeræ˜¯åŸºäºpythonå®ç°çš„é€Ÿåº¦æ¯”è¾ƒæ…¢</p>

<pre class="language-python"><code class="language-python">tokenizer = AutoTokenizer.from_pretrained(&quot;uer/roberta-base-finetuned-dianping-chinese&quot;, use_fast=False)
</code></pre>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410062318774.png" alt="image-20241006231850680" /></p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410062320847.png" alt="image-20241006232053744" /></p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410062321402.png" alt="image-20241006232109337" /></p>
<p>fast tokenizerä¼šæœ‰æ›´å¤šçš„è¿”å›å€¼, ä¸»è¦æ˜¯åº”ç”¨äºå‘½åå®ä½“è¯†åˆ«ä»¥åŠqa</p>

<pre class="language-python"><code class="language-python">sen = &quot;æˆ‘å²å‡¯æ­Œæ•¢åƒshit! dreaming&quot;
inputs = tokenizer(sen, return_offsets_mapping=True)
</code></pre>
<blockquote>

<pre class="language-bash"><code class="language-bash">{
'input_ids': [101, 2769, 1380, 1132, 3625, 3140, 1391, 11772, 8165, 106, 10252, 8221, 102], 
'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 
'offset_mapping': [(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 9), (9, 10), (10, 11), (12, 17), (17, 20), (0, 0)]
}
</code></pre>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410062327233.png" alt="image-20241006232704170" /></p>
<p>æœ‰æ—¶å€™ä¸€ä¸ªè¯ä¼šè¢«æ‹†åˆ†ä¸ºå¤šä¸ªéƒ¨åˆ†, å¯ä»¥ä½¿ç”¨è¿™ä¸€ä¸ªè¿›è¡Œå¯¹å¯¹åº”, Noneå¯¹åº”çš„æ˜¯(0, 0), è‹±æ–‡çš„æ—¶å€™æ¯”è¾ƒæ˜æ˜¾, ç¬¬(6, 9)ä¸ªå­—ç¬¦å’Œ(9, 10)æ˜¯ä¸€ä¸ªè‹±æ–‡å•è¯è¢«æ‹†åˆ†ä¸ºä¸¤ä¸ª</p>
</blockquote>
<h3 id="%E5%85%B6%E5%AE%83%E5%8F%82%E6%95%B0">å…¶å®ƒå‚æ•°</h3>
<p>åœ¨ä¸‹è½½ä¸€äº›è‡ªä¸»å¼€å‘çš„åˆ†è¯å™¨çš„æ—¶å€™, éœ€è¦æŒ‡å®šä¸€ä¸ªå‚æ•°<code>trust_remote_code=True</code></p>

<pre class="language-python"><code class="language-python">tokenizer = AutoTokenizer.from_pretrained(&quot;THUDM/chatglm-6b&quot;, trust_remote_code=True)
</code></pre>
<h2 id="Model">Model</h2>
<p>åŸå§‹çš„Transformåˆ†ä¸ºç¼–ç å™¨(Encoder)ä»¥åŠè§£ç å™¨(Decoder)æ¨¡å‹, Encoderéƒ¨åˆ†æ¥æ”¶è¾“å…¥å¹¶ä¸”ä¸ºä»–æ„å»ºç‰¹å¾è¡¨ç¤º, Decoderä½¿ç”¨Encoderçš„ç¼–ç ç»“æœä»¥åŠå…¶ä»–çš„è¾“å…¥åºåˆ—ç”Ÿæˆç›®æ ‡åºåˆ—</p>
<p>æ— è®ºæ˜¯ç¼–ç å™¨è¿˜æ˜¯è§£ç å™¨éƒ½æ˜¯å¤šä¸ªTransformsBlockå †å è€Œæˆçš„</p>
<p>TransformsBlockç”±æ³¨æ„åŠ›æœºåˆ¶(Attention)ä»¥åŠFFNç»„æˆ</p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409241053267.png" alt="image-20240924105336210" /></p>
<blockquote>
<p><strong>æ³¨æ„åŠ›æœºåˆ¶</strong>, åœ¨è®¡ç®—å½“å‰çš„è¯çš„ç‰¹å¾è¡¨ç¤ºçš„æ—¶å€™, å¯ä»¥é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶æœ‰é€‰æ‹©æ€§çš„å‘Šè¯‰æ¨¡å¼è¦ä½¿ç”¨å“ªä¸€éƒ¨åˆ†çš„ä¸Šä¸‹æ–‡</p>
</blockquote>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410071010900.png" alt="image-20241007101012759" /></p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410071011461.png" alt="image-20241007101132383" /></p>
<h3 id="AutoModel">AutoModel</h3>
<p>AutoModelæ˜¯Hugging Faceçš„Transformersåº“ä¸­çš„ä¸€ä¸ªéå¸¸å®ç”¨çš„ç±»ï¼Œå®ƒå±äºè‡ªåŠ¨æ¨¡å‹é€‰æ‹©çš„æœºåˆ¶ã€‚è¿™ä¸ªè®¾è®¡å…è®¸ç”¨æˆ·åœ¨ä¸çŸ¥é“å…·ä½“æ¨¡å‹ç»†èŠ‚çš„æƒ…å†µä¸‹ï¼Œæ ¹æ®ç»™å®šçš„æ¨¡å‹åç§°æˆ–æ¨¡å‹ç±»å‹è‡ªåŠ¨åŠ è½½ç›¸åº”çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚å®ƒå‡å°‘äº†ä»£ç çš„é‡å¤æ€§ï¼Œå¹¶æé«˜äº†çµæ´»æ€§ï¼Œä½¿å¾—å¼€å‘è€…å¯ä»¥è½»æ¾åœ°åˆ‡æ¢ä¸åŒçš„æ¨¡å‹è¿›è¡Œå®éªŒæˆ–åº”ç”¨ã€‚</p>
<h3 id="Model-Head">Model Head</h3>
<p>Model Headåœ¨é¢„è®­ç»ƒæ¨¡å‹çš„åŸºç¡€ä¸Šæ·»åŠ ä¸€å±‚æˆ–å¤šå±‚çš„é¢å¤–ç½‘ç»œç»“æ„æ¥é€‚åº”ç‰¹å®šçš„æ¨¡å‹ä»»åŠ¡ï¼Œæ–¹ä¾¿äºå¼€å‘è€…å¿«é€ŸåŠ è½½transformersåº“ä¸­çš„ä¸åŒç±»å‹æ¨¡å‹ï¼Œä¸ç”¨å…³å¿ƒæ¨¡å‹å†…éƒ¨ç»†èŠ‚ã€‚</p>
<p>Model Headæ˜¯Transformersæ¨¡å‹é‡Œçš„ä¸€å±‚ï¼Œé€šå¸¸ç”¨äºå¯¹æ¨¡å‹è¾“å‡ºè¿›è¡Œåç»­å¤„ç†ã€‚å…¶ä½œç”¨æ˜¯æ¥æ”¶æ¨¡å‹çš„è¾“å‡ºï¼Œç„¶åå°†å…¶æ˜ å°„æˆæœ€ç»ˆçš„è¾“å‡ºã€‚è¿™å¯èƒ½æ¶‰åŠåˆ°ä¸€ç³»åˆ—æ“ä½œï¼Œæ¯”å¦‚åˆ†ç±»ã€å›å½’ã€ç”Ÿæˆç­‰ä»»åŠ¡ã€‚Model Headå¯ä»¥å°†æ¨¡å‹è¾“å‡ºè½¬åŒ–ä¸ºé€‚åˆç‰¹å®šä»»åŠ¡çš„æ ¼å¼ï¼Œå¹¶åŠ å…¥é€‚å½“çš„æŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒã€‚</p>
<p>åœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒModel Headå¯èƒ½ä¼šæ¥æ”¶æ¨¡å‹è¾“å‡ºçš„æ¯ä¸ªtokençš„è¡¨ç¤ºï¼Œç„¶åå°†å®ƒä»¬æ±‡æ€»æˆæ•´ä¸ªå¥å­çš„è¡¨ç¤ºï¼Œæœ€ç»ˆé€šè¿‡ä¸€ä¸ªå…¨è¿æ¥å±‚å°†å…¶åˆ†ç±»ä¸ºä¸åŒçš„ç±»åˆ«ã€‚</p>
<p>åœ¨ç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒModel Headå¯èƒ½ä¼šæ¥æ”¶æ¨¡å‹è¾“å‡ºçš„æ¯ä¸ªtokençš„è¡¨ç¤ºï¼Œç„¶åå°†å…¶ä¾æ¬¡è¾“å…¥åˆ°ä¸€ä¸ªè§£ç å™¨ä¸­ç”Ÿæˆæ•´ä¸ªåºåˆ—ã€‚</p>
<p>æ€»çš„æ¥è¯´ï¼ŒModel Headçš„ä½œç”¨æ˜¯å°†æ¨¡å‹è¾“å‡ºè¿›è¡Œæœ€ç»ˆçš„è½¬åŒ–ï¼Œä½¿å¾—æ¨¡å‹å¯ä»¥å®Œæˆå…·ä½“çš„ä»»åŠ¡ã€‚</p>
<ul>
<li>ForCausalLMï¼šå› æœè¯­è¨€æ¨¡å‹å¤´ï¼Œç”¨äºdecoderç±»å‹çš„ä»»åŠ¡ï¼Œä¸»è¦è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼Œç”Ÿæˆçš„æ¯ä¸ªè¯ä¾èµ–äºä¹‹å‰ç”Ÿæˆçš„æ‰€æœ‰è¯ã€‚æ¯”å¦‚GPTã€Qwen</li>
<li>ForMaskedLMï¼šæ©ç è¯­è¨€æ¨¡å‹å¤´ï¼Œç”¨äºencoderç±»å‹çš„ä»»åŠ¡ï¼Œä¸»è¦è¿›è¡Œé¢„æµ‹æ–‡æœ¬ä¸­è¢«æ©ç›–å’Œè¢«éšè—çš„è¯ï¼Œæ¯”å¦‚BERTã€‚</li>
<li>ForSeq2SeqLMï¼šåºåˆ—åˆ°åºåˆ—æ¨¡å‹å¤´ï¼Œç”¨äºencoder-decoderç±»å‹çš„ä»»åŠ¡ï¼Œä¸»è¦å¤„ç†ç¼–ç å™¨å’Œè§£ç å™¨å…±åŒå·¥ä½œçš„ä»»åŠ¡ï¼Œæ¯”å¦‚æœºå™¨ç¿»è¯‘æˆ–æ–‡æœ¬æ‘˜è¦ã€‚</li>
<li>ForQuestionAnsweringï¼šé—®ç­”ä»»åŠ¡æ¨¡å‹å¤´ï¼Œç”¨äºé—®ç­”ç±»å‹çš„ä»»åŠ¡ï¼Œä»ç»™å®šçš„æ–‡æœ¬ä¸­æŠ½å–ç­”æ¡ˆã€‚é€šè¿‡ä¸€ä¸ªencoderæ¥ç†è§£é—®é¢˜å’Œä¸Šä¸‹æ–‡ï¼Œå¯¹ç­”æ¡ˆè¿›è¡ŒæŠ½å–ã€‚</li>
<li>ForSequenceClassificationï¼šæ–‡æœ¬åˆ†ç±»æ¨¡å‹å¤´ï¼Œå°†è¾“å…¥åºåˆ—æ˜ å°„åˆ°ä¸€ä¸ªæˆ–å¤šä¸ªæ ‡ç­¾ã€‚ä¾‹å¦‚ä¸»é¢˜åˆ†ç±»ã€æƒ…æ„Ÿåˆ†ç±»ã€‚</li>
<li>ForTokenClassificationï¼šæ ‡è®°åˆ†ç±»æ¨¡å‹å¤´ï¼Œç”¨äºå¯¹æ ‡è®°è¿›è¡Œè¯†åˆ«çš„ä»»åŠ¡ã€‚å°†åºåˆ—ä¸­çš„æ¯ä¸ªæ ‡è®°æ˜ å°„åˆ°ä¸€ä¸ªæå‰å®šä¹‰å¥½çš„æ ‡ç­¾ã€‚å¦‚å‘½åå®ä½“è¯†åˆ«ï¼Œæ‰“æ ‡ç­¾</li>
<li>ForMultiplechoiceï¼šå¤šé¡¹é€‰æ‹©ä»»åŠ¡æ¨¡å‹å¤´ï¼ŒåŒ…å«å¤šä¸ªå€™é€‰ç­”æ¡ˆçš„è¾“å…¥ï¼Œé¢„æµ‹æ­£ç¡®ç­”æ¡ˆçš„é€‰é¡¹ã€‚</li>
</ul>
<h3 id="%E4%B8%8B%E8%BD%BD%E5%8A%A0%E8%BD%BD%28%E6%97%A0Model-Head%29">ä¸‹è½½åŠ è½½(æ— Model Head)</h3>

<pre class="language-python"><code class="language-python">model = AutoModel.from_pretrained(&quot;hfl/rbt3&quot;)
</code></pre>
<blockquote>
<p>ä½¿ç”¨è¿™ä¸€ä¸ªæ–¹å¼çš„æ—¶å€™å¯ä»¥ä»å®˜ç½‘è¿›è¡Œä¸‹è½½æ¨¡å‹, å¦‚æœè¿™ä¸€ä¸ªå¤±è´¥å¯ä»¥åœ¨</p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410071055312.png" alt="image-20241007105514201" /></p>
<p>è¿™é‡Œä¸‹è½½çš„æ˜¯pytorchç‰ˆæœ¬çš„æ¨¡å‹</p>

<pre class="language-python"><code class="language-python">model = AutoModel.from_pretrained(&quot;E:/JHY/python/2024-10-5-transforms/hlfrbt3&quot;)
</code></pre>
<p>è¿˜å¯ä»¥ä½¿ç”¨gitå…‹éš†çš„æ–¹å¼è¿›è¡Œ, åœ¨trainæŒ‰é’®çš„å·¦ä¾§ä¸‰ä¸ªç‚¹</p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410071104166.png" alt="image-20241007110417069" /></p>
<p>åœ¨ä½¿ç”¨é»˜è®¤çš„å‘½ä»¤çš„æ—¶å€™, ä¼šä¸‹è½½æ‰€æœ‰çš„ä¸‰ä¸ªæ¨¡å‹, æ‰€ä»¥å¯ä»¥ä½¿ç”¨</p>

<pre class="language-bash"><code class="language-bash">git lfs clone &quot;https://huggingface.co/hfl/rbt3&quot; --include=&quot;*.bin&quot;
</code></pre>
</blockquote>
<h3 id="%E9%85%8D%E7%BD%AE">é…ç½®</h3>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410071113850.png" alt="image-20241007111339761" /></p>
<blockquote>
<p>å¯ä»¥åœ¨ä¸‹è½½çš„æ—¶å€™å¯¹æ¨¡å‹è¿›è¡Œé…ç½®</p>
</blockquote>
<p>ä¹Ÿå¯ä»¥ä½¿ç”¨<code>model.config</code>è·å–è¿™ä¸€ä¸ªæ¨¡å‹çš„é…ç½®å‚æ•°, æœ€å…¨çš„å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å‡½æ•°åŠ è½½</p>

<pre class="language-python"><code class="language-python">config = AutoConfig.from_pretrained(&quot;E:/JHY/python/2024-10-5-transforms/hlfrbt3&quot;)
</code></pre>
<p>ä½¿ç”¨è¿™ä¸€ç§æ–¹å¼åŠ è½½çš„æ—¶å€™å‚æ•°å’Œå˜é‡æ˜¯å¯ä»¥æ”¹å˜çš„</p>
<blockquote>

<pre class="language-bash"><code class="language-bash">BertConfig {
  &quot;_name_or_path&quot;: &quot;E:/JHY/python/2024-10-5-transforms/hlfrbt3&quot;,
  &quot;architectures&quot;: [
    &quot;BertForMaskedLM&quot;
  ],
  &quot;attention_probs_dropout_prob&quot;: 0.1,
  &quot;classifier_dropout&quot;: null,
  &quot;directionality&quot;: &quot;bidi&quot;,
  &quot;hidden_act&quot;: &quot;gelu&quot;,
  &quot;hidden_dropout_prob&quot;: 0.1,
  &quot;hidden_size&quot;: 768,
  &quot;initializer_range&quot;: 0.02,
  &quot;intermediate_size&quot;: 3072,
  &quot;layer_norm_eps&quot;: 1e-12,
  &quot;max_position_embeddings&quot;: 512,
  &quot;model_type&quot;: &quot;bert&quot;,
  &quot;num_attention_heads&quot;: 12,
  &quot;num_hidden_layers&quot;: 3,
  &quot;output_past&quot;: true,
  &quot;pad_token_id&quot;: 0,
  &quot;pooler_fc_size&quot;: 768,
  &quot;pooler_num_attention_heads&quot;: 12,
  &quot;pooler_num_fc_layers&quot;: 3,
  &quot;pooler_size_per_head&quot;: 128,
  &quot;pooler_type&quot;: &quot;first_token_transform&quot;,
  &quot;position_embedding_type&quot;: &quot;absolute&quot;,
  &quot;transformers_version&quot;: &quot;4.44.2&quot;,
  &quot;type_vocab_size&quot;: 2,
  &quot;use_cache&quot;: true,
  &quot;vocab_size&quot;: 21128
}
</code></pre>
</blockquote>
<p>è¾“å‡ºæ˜¯ä¸€ä¸ªBertConfigç±», è¿™ä¸ªç±»ç»§æ‰¿äºPretrainedConfigç±», æœ‰å¾ˆå¤šçš„å‚æ•°æ˜¯åœ¨ä¸Šé¢æ²¡æœ‰æ˜¾ç¤ºçš„</p>
<p>å®é™…åŠŸèƒ½</p>

<pre class="language-python"><code class="language-python">sen = &quot;æˆ‘çˆ±åŒ—äº¬å¤©å®‰é—¨&quot;
tokenizer = AutoTokenizer.from_pretrained(&quot;E:/JHY/python/2024-10-5-transforms/hlfrbt3&quot;)
inputs = tokenizer(sen, return_tensors=&quot;pt&quot;)
output = model(**inputs)
</code></pre>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410071128164.png" alt="image-20241007112810082" /></p>
<blockquote>

<pre class="language-bash"><code class="language-bash">          9.1205e-01, -9.9994e-01, -3.5651e-01,  9.9433e-01,  9.3075e-01,
         -3.3699e-01,  9.9916e-01, -1.0331e-01]], grad_fn=&lt;TanhBackward0&gt;), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)
</code></pre>
</blockquote>
<blockquote>
<p>è¿™æ—¶å€™çš„è¾“å‡ºæœ‰ä¸€éƒ¨åˆ†çš„æ•°æ®æ˜¯æ²¡æœ‰å‚æ•°çš„, å¯ä»¥åœ¨åŠ è½½çš„æ—¶å€™åŠ ä¸€ä¸ªå‚æ•°</p>

<pre class="language-c"><code class="language-c">model = AutoModel.from_pretrained(&quot;E:/JHY/python/2024-10-5-transforms/hlfrbt3&quot;, output_attentions=True)
</code></pre>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410071131938.png" alt="image-20241007113159856" /></p>
</blockquote>
<p>è¿™ä¸€ä¸ªæ¨¡å‹æ˜¯ä¸€ä¸ªä¸å¸¦Model Headerçš„</p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410071133143.png" alt="image-20241007113346059" /></p>
<p>ä¸€æ¡æ•°æ®, é•¿åº¦ä¸º9çš„inputids</p>
<h3 id="%E6%9C%89Model-Head">æœ‰Model Head</h3>

<pre class="language-python"><code class="language-python">from transformers import AutoModelForSequenceClassification
clz_model = AutoModelForSequenceClassification.from_pretrained(&quot;E:/JHY/python/2024-10-5-transforms/hlfrbt3&quot;, num_labels=2)
clz_model(**inputs)
</code></pre>
<blockquote>

<pre class="language-bash"><code class="language-bash">SequenceClassifierOutput(loss=None, logits=tensor([[ 0.0296, -0.4734]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)
</code></pre>
<p>æ²¡æœ‰ä¼ label, æ‰€ä»¥è¿™é‡Œçš„loss=None</p>
</blockquote>
<p>å¯ä»¥ä½¿ç”¨å‚æ•°num_labelsè®¾ç½®è¾“å‡ºçš„åˆ†ç±»ä¸ªæ•°</p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410071144912.png" alt="image-20241007114416817" /></p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410071143951.png" alt="image-20241007114337851" /></p>
<p>è¿™é‡Œçš„bertæ˜¯ä¸€ä¸ªæ¨¡å‹ç¼–ç å™¨<a href="https://blog.csdn.net/yjw123456/article/details/120211601#:~:text=%E6%9C%AC%E6%96%87%E8%AF%A6%E7%BB%86%E8%A7%A3%E6%9E%90BERT"  target="_blank">ã€ç†è®ºç¯‡ã€‘æ˜¯æ—¶å€™å½»åº•å¼„æ‡‚BERTæ¨¡å‹äº†(æ”¶è—)-CSDNåšå®¢</a></p>
<h3 id="%E5%BE%AE%E8%B0%83%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B">å¾®è°ƒä»£ç ç¤ºä¾‹</h3>
<p>ä½¿ç”¨æ•°æ®é›†<a href="https://github.com/SophonPlus/ChineseNlpCorpus"  target="_blank">SophonPlus/ChineseNlpCorpus</a></p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410071235397.png" alt="image-20241007123535302" /></p>
<h4 id="%E8%8E%B7%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%A5%E5%8F%8A%E5%A4%84%E7%90%86">è·å–æ•°æ®é›†ä»¥åŠå¤„ç†</h4>

<pre class="language-python"><code class="language-python"># æ–‡æœ¬åˆ†ç±»æ¨¡å‹å¾®è°ƒçš„ç¤ºä¾‹
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# åŠ è½½æ•°æ®
import pandas as pd

# data = pd.read_csv(&quot;../dataset/ChnSentiCorp_htl_all.csv&quot;)
# data.head()

# data = data.dropna()    # åˆ é™¤ç¼ºå¤±å€¼
# data.iloc[1]

# åˆ›å»ºdataset
from torch.utils.data import Dataset

class MyDataset(Dataset):
    def __init__(self):
        super().__init__()
        self.data = pd.read_csv(&quot;../dataset/ChnSentiCorp_htl_all.csv&quot;)
        self.data = self.data.dropna()

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data.iloc[idx][&quot;review&quot;], self.data.iloc[idx][&quot;label&quot;]

    dataset = MyDataset()

for i in range(5):
    print(dataset[i])
&quot;&quot;&quot;
('è·ç¦»å·æ²™å…¬è·¯è¾ƒè¿‘,ä½†æ˜¯å…¬äº¤æŒ‡ç¤ºä¸å¯¹,å¦‚æœæ˜¯&quot;è”¡é™†çº¿&quot;çš„è¯,ä¼šéå¸¸éº»çƒ¦.å»ºè®®ç”¨åˆ«çš„è·¯çº¿.æˆ¿é—´è¾ƒä¸ºç®€å•.', 1)
('å•†åŠ¡å¤§åºŠæˆ¿ï¼Œæˆ¿é—´å¾ˆå¤§ï¼ŒåºŠæœ‰2Må®½ï¼Œæ•´ä½“æ„Ÿè§‰ç»æµå®æƒ ä¸é”™!', 1)
('æ—©é¤å¤ªå·®ï¼Œæ— è®ºå»å¤šå°‘äººï¼Œé‚£è¾¹ä¹Ÿä¸åŠ é£Ÿå“çš„ã€‚é…’åº—åº”è¯¥é‡è§†ä¸€ä¸‹è¿™ä¸ªé—®é¢˜äº†ã€‚æˆ¿é—´æœ¬èº«å¾ˆå¥½ã€‚', 1)
('å®¾é¦†åœ¨å°è¡—é“ä¸Šï¼Œä¸å¤§å¥½æ‰¾ï¼Œä½†è¿˜å¥½åŒ—äº¬çƒ­å¿ƒåŒèƒå¾ˆå¤š~å®¾é¦†è®¾æ–½è·Ÿä»‹ç»çš„å·®ä¸å¤šï¼Œæˆ¿é—´å¾ˆå°ï¼Œç¡®å®æŒºå°ï¼Œä½†åŠ ä¸Šä½ä»·ä½å› ç´ ï¼Œè¿˜æ˜¯æ— è¶…æ‰€å€¼çš„ï¼›ç¯å¢ƒä¸é”™ï¼Œå°±åœ¨å°èƒ¡åŒå†…ï¼Œå®‰é™æ•´æ´ï¼Œæš–æ°”å¥½è¶³-_-||ã€‚ã€‚ã€‚å‘µè¿˜æœ‰ä¸€å¤§ä¼˜åŠ¿å°±æ˜¯ä»å®¾é¦†å‡ºå‘ï¼Œæ­¥è¡Œä¸åˆ°ååˆ†é’Ÿå°±å¯ä»¥åˆ°æ¢…å…°èŠ³æ•…å±…ç­‰ç­‰ï¼Œäº¬å‘³å°èƒ¡åŒï¼ŒåŒ—æµ·è·ç¦»å¥½è¿‘å‘¢ã€‚æ€»ä¹‹ï¼Œä¸é”™ã€‚æ¨èç»™èŠ‚çº¦æ¶ˆè´¹çš„è‡ªåŠ©æ¸¸æœ‹å‹~æ¯”è¾ƒåˆ’ç®—ï¼Œé™„è¿‘ç‰¹è‰²å°åƒå¾ˆå¤š~', 1)
('CBDä¸­å¿ƒ,å‘¨å›´æ²¡ä»€ä¹ˆåº—é“º,è¯´5æ˜Ÿæœ‰ç‚¹å‹‰å¼º.ä¸çŸ¥é“ä¸ºä»€ä¹ˆå«ç”Ÿé—´æ²¡æœ‰ç”µå¹é£', 1)
&quot;&quot;&quot;
# åˆ’åˆ†æ•°æ®é›†
from torch.utils.data import random_split

trainset, validset = random_split(dataset, lengths=[0.9, 0.1])   # ä¹±åºåˆ’åˆ†æ•°æ®é›†

print(len(trainset), len(validset))
&quot;&quot;&quot;
(6989, 776)
&quot;&quot;&quot;

# å»ºç«‹DataLoader
from torch.utils.data import DataLoader
trainloader = DataLoader(trainset, batch_size=32, shuffle=True)  # ä¹±åº, ä¸€ç»„å¤§å°ä¸º32
validloader = DataLoader(validset, batch_size=32, shuffle=False) # ä¸ä¹±åº
</code></pre>
<blockquote>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410071307339.png" alt="image-20241007130723273" /></p>
<p>ä¹Ÿå¯ä»¥ä½¿ç”¨<code>next(enumerate(trainloader))</code></p>
<p>é»˜è®¤çš„æ—¶å€™ä½¿ç”¨è¿™ä¸€ä¸ªå‡½æ•°è¿›è¡Œèšåˆçš„ç»“æœæ˜¯æ–‡å­—èšé›†ä¸ºä¸€ä¸ªå…ƒç»„, æ•°å­—æ˜¯ä¸€ä¸ªtensor, å¦‚æœæƒ³è¦ä½¿ç”¨Tokenizerè¿›è¡Œå¤„ç†è¿™ä¸€ä¸ªæ•°æ®, æ–‡å­—ä¸ºä¸€ä¸ªlistå¯ä»¥é‡å†™ä¸€ä¸‹`collate_fn, input_idsæ˜¯Tokenizerå­—å…¸é‡Œé¢çš„ä¸€é¡¹çš„åå­—, ç”±äºåœ¨Bertæ¨¡å‹é‡Œé¢labelsæ˜¯å…¶ä¸­å¦ä¸€ä¸ªå‚æ•°, æ‰€ä»¥æŠŠè¿™ä¸¤ä¸ªæ‰“åŒ…</p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410071333543.png" alt="image-20241007133339446" /></p>

<pre class="language-python"><code class="language-python">import torch 
tokenizer = AutoTokenizer.from_pretrained(&quot;../hlfrbt3&quot;)
def j_collate_fn(batch):
     texts, labels = [], []
     for text, label in batch:
         texts.append(text)
         labels.append(label)
     # 128æ˜¯æ¨¡å‹çš„æœ€å¤§é•¿åº¦
     inputs = tokenizer(texts, padding=&quot;max_length&quot;, truncation=True, max_length=128,
                        return_tensors=&quot;pt&quot;) 
     inputs[&quot;labels&quot;] = torch.tensor(labels)
    # è¿”å›ä¸€ä¸ªå­—å…¸é‡Œé¢ä¸¤ä¸ªValueä¸ºtensoræ ¼å¼
     return inputs
# å»ºç«‹DataLoader
from torch.utils.data import DataLoader
# ä¹±åº, ä¸€ç»„å¤§å°ä¸º32
trainloader = DataLoader(trainset, batch_size=32, shuffle=True, collate_fn=j_collate_fn)
# ä¸ä¹±åº
validloader= DataLoader(validset, batch_size=32, shuffle=False, collate_fn=j_collate_fn)
</code></pre>
</blockquote>
<h4 id="%E5%AF%BC%E5%85%A5%E6%A8%A1%E5%9E%8B%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96">å¯¼å…¥æ¨¡å‹ä»¥åŠä¼˜åŒ–</h4>

<pre class="language-python"><code class="language-python">from torch.optim import AdamW
# å¯¼å…¥æ¨¡å‹
model = AutoModelForSequenceClassification.from_pretrained(&quot;../hlfrbt3&quot;)
if torch.cuda.is_available():
    model = model.cuda()
# å®šä¹‰ä¼˜åŒ–å™¨
optimizer = AdamW(model.parameters(), lr=1e-5) # 1e-5æ˜¯å­¦ä¹ ç‡, è¿ç§»å­¦ä¹ ä½¿ç”¨çš„ä¸€èˆ¬æ¯”è¾ƒä½
</code></pre>
<h4 id="%E5%AE%9E%E9%99%85%E8%AE%AD%E7%BB%83%E5%87%BD%E6%95%B0">å®é™…è®­ç»ƒå‡½æ•°</h4>

<pre class="language-python"><code class="language-python">def evaluate():
    &quot;&quot;&quot;
    Description: è¯„ä¼°æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„æ€§èƒ½
    Returns:
        æ¨¡å‹çš„å‡†ç¡®ç‡
    &quot;&quot;&quot;
    model.eval()
    acc_num = 0
    with torch.no_grad():
        for batch in validloader:
            if torch.cuda.is_available():
                batch = {k:v.cuda() for k, v in batch.items()}
            outputs = model(**batch)
            pred = outputs.logits.argmax(dim=-1) # é¢„æµ‹çš„ç±»åˆ«
            acc_num += (pred == batch[&quot;labels&quot;].long()).float().sum().item()
    return acc_num / len(validset)

def train(epoch=3, log_step=100):
    &quot;&quot;&quot;
    Description: è®­ç»ƒæ¨¡å‹
    Args:
        epoch (int, optional): è®­ç»ƒçš„æ¬¡æ•°. Defaults to 3.
        log_step (int, optional): æ‰“å°logçš„æ­¥é•¿. Defaults to 100.
    &quot;&quot;&quot;
    global_step=0
    for ep in range(epoch):
        model.train()
        # éå†è®­ç»ƒé›†
        for batch in trainloader:
            # å°†æ•°æ®æ”¾åˆ°cudaä¸Š
            if torch.cuda.is_available():
                batch = {k:v.cuda() for k, v in batch.items()}
            optimizer.zero_grad()
            outputs = model(**batch)
            outputs.loss.backward()
            optimizer.step()
            global_step += 1
            if global_step % log_step == 0:
                print(f&quot;epoch={ep}, global_step={global_step}, loss={outputs.loss.item()}&quot;)
        # æ¯ä¸ªepochç»“æŸè¯„ä¼°ä¸€æ¬¡
        acc = evaluate()
        print(f&quot;epoch={ep}, acc={acc}&quot;)
</code></pre>

<pre class="language-python"><code class="language-python">print(f'before train {evaluate()}')
train()
</code></pre>
<blockquote>

<pre class="language-bash"><code class="language-bash">before train 0.31056701030927836
epoch=0, global_step=100, loss=0.293989360332489
epoch=0, global_step=200, loss=0.31614530086517334
epoch=0, acc=0.8904639175257731
epoch=1, global_step=300, loss=0.1351868063211441
epoch=1, global_step=400, loss=0.17762571573257446
epoch=1, acc=0.8865979381443299
epoch=2, global_step=500, loss=0.17976289987564087
epoch=2, global_step=600, loss=0.19925124943256378
epoch=2, acc=0.8853092783505154
</code></pre>
</blockquote>
<h4 id="%E5%AE%9E%E9%99%85%E4%BD%BF%E7%94%A8">å®é™…ä½¿ç”¨</h4>

<pre class="language-python"><code class="language-python">sen = &quot;æˆ‘è§‰å¾—è¿™å®¶é¥­åº—çš„é¥­å¾ˆå¥½åƒ, ä½“éªŒå¾ˆå¥½!&quot;
with torch.inference_mode():
    inputs = tokenizer(sen, padding=&quot;max_length&quot;, truncation=True, max_length=128, return_tensors=&quot;pt&quot;)
    if torch.cuda.is_available():
        inputs = {k:v.cuda() for k, v in inputs.items()}
    outputs = model(**inputs)
    pred = outputs.logits.argmax(dim=-1)
    print(pred.item())
</code></pre>
<ul>
<li>ä½¿ç”¨pipeè¿›è¡Œ</li>
</ul>

<pre class="language-python"><code class="language-python">from transformers import pipeline
model.config.id2label = id2label # è¿™ä¸€æ­¥ä¹Ÿå¯ä»¥åœ¨æ¨¡å‹åŠ è½½çš„æ—¶å€™å®ç°
pipe = pipeline(&quot;text-classification&quot;, model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)
pipe(sen)
</code></pre>
<h2 id="Dataset">Dataset</h2>
<h3 id="%E5%8A%A0%E8%BD%BD">åŠ è½½</h3>
<p>ä¸€ä¸ªå¯ä»¥æ–¹ä¾¿çš„ä»HuggingfaceåŠ è½½æ•°æ®é›†çš„åº“</p>
<p>è¿™é‡Œä½¿ç”¨çš„æ¨¡å‹æ˜¯<a href="https://huggingface.co/datasets/madao33/new-title-chinese"  target="_blank">madao33/new-title-chinese Â· Datasets at Hugging Face</a></p>

<pre class="language-python"><code class="language-python">from datasets import *
dataset = load_dataset(&quot;madao33/new-title-chinese&quot;)
</code></pre>
<blockquote>

<pre class="language-bash"><code class="language-bash">DatasetDict({
    train: Dataset({
        features: ['title', 'content'],
        num_rows: 5850
    })
    validation: Dataset({
        features: ['title', 'content'],
        num_rows: 1679
    })
})
</code></pre>
</blockquote>
<p>å¯¹äºå…¶ä»–çš„ä¸€äº›æ•°æ®é›†, æ¯”å¦‚glueæ˜¯ä¸€ä¸ªä»»åŠ¡çš„é›†åˆ, éœ€è¦å†åŠ ä¸€ä¸ªå‚æ•°æŒ‡å®šå®é™…åŠ è½½çš„ä»»åŠ¡</p>

<pre class="language-python"><code class="language-python">boolq_dataset = load_dataset(&quot;super_glue&quot;, &quot;boolq&quot;, trust_remote_code=True)
boolq_dataset
</code></pre>
<blockquote>
<p><a href="https://huggingface.co/datasets/aps/super_glue"  target="_blank">aps/super_glue Â· Datasets at Hugging Face</a></p>

<pre class="language-bash"><code class="language-bash">Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.12M/4.12M [00:01&lt;00:00, 3.20MB/s]
Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9427/9427 [00:00&lt;00:00, 14841.26 examples/s]
Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3270/3270 [00:00&lt;00:00, 28511.03 examples/s]
Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3245/3245 [00:00&lt;00:00, 29980.43 examples/s]
DatasetDict({
 train: Dataset({
     features: ['question', 'passage', 'idx', 'label'],
     num_rows: 9427
 })
 validation: Dataset({
     features: ['question', 'passage', 'idx', 'label'],
     num_rows: 3270
 })
 test: Dataset({
     features: ['question', 'passage', 'idx', 'label'],
     num_rows: 3245
 })
})
</code></pre>
</blockquote>
<p>ä¸€ä¸ªæ•°æ®é›†åˆ†ä¸ºæ•°æ®é›†, éªŒè¯é›†, æµ‹è¯•é›†ä¸‰éƒ¨åˆ†, åªæƒ³åŠ è½½ä¸€éƒ¨åˆ†çš„è¯</p>

<pre class="language-python"><code class="language-python">boolq_dataset_train = load_dataset(&quot;super_glue&quot;, &quot;boolq&quot;, trust_remote_code=True, split=&quot;train&quot;)
</code></pre>
<p>ä¹Ÿå¯ä»¥è¿›ä¸€æ­¥æ‹†åˆ†</p>

<pre class="language-python"><code class="language-python">boolq_dataset_train = load_dataset(&quot;super_glue&quot;, &quot;boolq&quot;, trust_remote_code=True, split=&quot;train[:100]&quot;)
boolq_dataset_train = load_dataset(&quot;super_glue&quot;, &quot;boolq&quot;, trust_remote_code=True, split=&quot;train[:10%]&quot;)
boolq_dataset_train = load_dataset(&quot;super_glue&quot;, &quot;boolq&quot;, trust_remote_code=True, split=[&quot;train[:10%]&quot;, &quot;validation[:10%]&quot;])
</code></pre>
<h3 id="%E6%9F%A5%E7%9C%8B">æŸ¥çœ‹</h3>

<pre class="language-python"><code class="language-python">dataset[&quot;train&quot;][:2]
dataset[&quot;train&quot;][&quot;title&quot;][:2]
</code></pre>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410072225595.png" alt="image-20241007222540350" /></p>

<pre class="language-python"><code class="language-python">dataset[&quot;train&quot;].features
dataset.column_names
</code></pre>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410072227852.png" alt="image-20241007222715762" /></p>
<blockquote>
<p>ä½¿ç”¨è¿™ç§ç›´æ¥é€‰å–çš„æ–¹å¼è·å–çš„æ•°æ®æ˜¯ä¸€ä¸ªå­—å…¸çš„æ¨¡å¼</p>
</blockquote>
<h3 id="%E5%88%92%E5%88%86">åˆ’åˆ†</h3>
<p>å–ä¸€ä¸ªæ•°æ®é›†æŒ‰æ¯”ä¾‹åˆ’åˆ†ä¸ºè®­ç»ƒé›†ä»¥åŠæµ‹è¯•é›†</p>

<pre class="language-python"><code class="language-python">dataset[&quot;train&quot;].train_test_split(test_size=0.1, stratify_by_column=&quot;lable&quot;)
</code></pre>
<blockquote>
<p>åœ¨å®é™…åˆ’åˆ†çš„æ—¶å€™æŒ‡å®šå‚æ•°stratify_by_columnä¸ºä»¥åŠlableå¯ä»¥æŒ‰ç…§è¿™ä¸€ä¸ªæ ‡ç­¾åˆ’åˆ†çš„æ¯”è¾ƒå‡è¡¡</p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410072234130.png" alt="image-20241007223443044" /></p>
</blockquote>
<h3 id="%E9%80%89%E5%8F%96%E4%BB%A5%E5%8F%8A%E8%BF%87%E6%BB%A4">é€‰å–ä»¥åŠè¿‡æ»¤</h3>

<pre class="language-python"><code class="language-python">dataset[&quot;train&quot;].select([1, 2])
dataset[&quot;train&quot;].filter(lambda example: &quot;ä¸­å›½&quot; in example[&quot;title&quot;])
</code></pre>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410072238368.png" alt="image-20241007223822283" /></p>
<h3 id="%E6%95%B0%E6%8D%AE%E6%98%A0%E5%B0%84">æ•°æ®æ˜ å°„</h3>
<p>å¯¹æ¯ä¸€æ¡æ•°æ®è¿›è¡ŒåŒä¸€ä¸ªå¤„ç†</p>

<pre class="language-python"><code class="language-python">def add_title(example):
    example[&quot;title&quot;] = &quot;Predix: &quot; + example[&quot;title&quot;]
    return example

prefix_dataset = dataset.map(add_title)
prefix_dataset[&quot;train&quot;][:2][&quot;title&quot;]
</code></pre>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410072243728.png" alt="image-20241007224325640" /></p>
<blockquote>
<p>ä¸ªæ¯ä¸€ä¸ªæ ‡é¢˜åŠ ä¸€ä¸ªå‰ç¼€</p>
</blockquote>

<pre class="language-python"><code class="language-python">from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-chinese&quot;)
def preprocess_function(examples):
    model_inputs = tokenizer(examples[&quot;content&quot;], padding=&quot;max_length&quot;, truncation=True,
                             max_length=512)
    labels = tokenizer(examples[&quot;title&quot;], padding=&quot;max_length&quot;, truncation=True,
                       max_length=32)
    model_inputs[&quot;labels&quot;] = labels[&quot;input_ids&quot;]
    return model_inputs

processed_dataset = dataset.map(preprocess_function)
</code></pre>
<blockquote>

<pre class="language-bash"><code class="language-bash">DatasetDict({
    train: Dataset({
        features: ['title', 'content', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],
        num_rows: 5850
    })
    validation: Dataset({
        features: ['title', 'content', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],
        num_rows: 1679
    })
})
</code></pre>
</blockquote>

<pre class="language-python"><code class="language-python">processed_dataset = dataset.map(preprocess_function, batched=True)
processed_dataset
</code></pre>
<blockquote>
<p>å¦‚æœå°† <code>batched=True</code>ï¼Œåˆ™ <code>dataset</code> ä¸­çš„æ•°æ®å°†æŒ‰ç…§è®¾ç½®çš„æ‰¹å¤§å°è¿›è¡Œå¤„ç†ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡å¤„ç†ä¸€ä¸ªæ ·æœ¬ã€‚</p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410072258436.png" alt="image-20241007225823344" /></p>
</blockquote>
<p>ä¹Ÿå¯ä»¥ä½¿ç”¨å¤šçº¿ç¨‹çš„æ–¹å¼è¿›è¡ŒåŠ è½½</p>

<pre class="language-python"><code class="language-python">processed_dataset = dataset.map(preprocess_function, num_proc=4)
processed_dataset
</code></pre>
<blockquote>
<p>åœ¨ä½¿ç”¨æµ‹ä¸€ä¸ªå‡½æ•°çš„æ—¶å€™, tokenizeræ˜¯ä¸å¯ä»¥ä¼ é€’åˆ°å­è¿›ç¨‹çš„, æ‰€ä»¥å¤„ç†çš„å‡½æ•°éœ€è¦æ”¹å˜ä¸€ä¸‹</p>

<pre class="language-python"><code class="language-python">from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-chinese&quot;)
def preprocess_function(examples, tokenizer=tokenizer):
    model_inputs = tokenizer(examples[&quot;content&quot;], padding=&quot;max_length&quot;, truncation=True, max_length=512)
    labels = tokenizer(examples[&quot;title&quot;], padding=&quot;max_length&quot;, truncation=True, max_length=32)
    model_inputs[&quot;labels&quot;] = labels[&quot;input_ids&quot;]
    return model_inputs
</code></pre>
</blockquote>
<p>å¯ä»¥åœ¨è¿™ä¸€ä¸ªå‡½æ•°é‡Œé¢ä½¿ç”¨å‚æ•°æŠŠä¸éœ€è¦çš„è¾“å‡ºå­—æ®µè¿›è¡Œåˆ é™¤</p>

<pre class="language-python"><code class="language-python">processed_dataset = dataset.map(preprocess_function, batched=True,
                                remove_columns=dataset[&quot;train&quot;].column_names)
processed_dataset
</code></pre>
<blockquote>
<p>å»é™¤åŸå§‹å­—æ®µ</p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410072304567.png" alt="image-20241007230448456" /></p>
</blockquote>
<h3 id="%E4%BF%9D%E5%AD%98%E4%BB%A5%E5%8F%8A%E5%8A%A0%E8%BD%BD">ä¿å­˜ä»¥åŠåŠ è½½</h3>

<pre class="language-python"><code class="language-python">processed_dataset.save_to_disk(&quot;my_dataset&quot;)
processed_dataset = load_from_disk(&quot;my_dataset&quot;)
</code></pre>
<h3 id="%E5%8A%A0%E8%BD%BD%E8%87%AA%E5%B7%B1%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86">åŠ è½½è‡ªå·±çš„æ•°æ®é›†</h3>
<p>å¦‚æœæ˜¯ä¸€ä¸ªcsvæ ¼å¼çš„æ•°æ®é›†</p>

<pre class="language-python"><code class="language-python">dataset = load_dataset(&quot;csv&quot;, data_files=&quot;../dataset/ChnSentiCorp_htl_all.csv&quot;)
</code></pre>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410072308817.png" alt="image-20241007230845712" /></p>
<p>å¦‚æœä¸å¸Œæœ›æœ‰è¿™ä¸€ä¸ªé»˜è®¤çš„åˆ†ç»„, å¯ä»¥åŠ ä¸€ä¸ªå‚æ•°å–æ¶ˆ</p>

<pre class="language-python"><code class="language-python">dataset = load_dataset(&quot;csv&quot;, data_files=&quot;../dataset/ChnSentiCorp_htl_all.csv&quot;, split=&quot;train&quot;)
</code></pre>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410072310661.png" alt="image-20241007231055560" /></p>
<p>ä¹Ÿå¯ä»¥ä½¿ç”¨å¦ä¸€ä¸ªå‡½æ•°åŠ è½½</p>

<pre class="language-python"><code class="language-python">dataset = Dataset.from_csv(&quot;E:/JHY/python/2024-10-5-transforms/dataset/ChnSentiCorp_htl_all.csv&quot;)
</code></pre>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410072314904.png" alt="image-20241007231431791" /></p>
<p>åœ¨å®é™…åŠ è½½çš„æ—¶å€™å¯ä»¥æŒ‰ç…§ä¸€ä¸ªæ–‡ä»¶å¤¹è¿›è¡ŒåŠ è½½, ä¹Ÿå¯ä»¥æŠŠdata_filesæŒ‡å®šä¸ºä¸€ä¸ªæ•°ç»„</p>

<pre class="language-python"><code class="language-python">dataset = load_dataset(&quot;csv&quot;, data_dir=&quot;../dataset&quot;, split=&quot;train&quot;)
</code></pre>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410072316772.png" alt="image-20241007231614570" /></p>
<p>å¦‚æœæ˜¯æŒ‰ç…§pandasåŠ è½½çš„æ•°æ®</p>

<pre class="language-python"><code class="language-python">import pandas as pd
df = pd.read_csv(&quot;../dataset/ChnSentiCorp_htl_all.csv&quot;)
dataset  = Dataset.from_pandas(df)
</code></pre>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410080919550.png" alt="image-20241008090421592" /></p>
<p>ä½¿ç”¨from_listçš„æ—¶å€™å®é™…æ˜¯ä»ä¸€ä¸ªå­—å…¸çš„listé‡Œé¢è¿›è¡ŒåŠ è½½</p>

<pre class="language-python"><code class="language-python">list = [{&quot;text&quot;: &quot;abc&quot;}, {&quot;text&quot;: &quot;bcd&quot;}]
</code></pre>
<p>å¦‚æœæ•°æ®éå¸¸çš„å¤æ‚, éœ€è¦é€šè¿‡è„šæœ¬çš„æ–¹å¼è¿›è¡ŒåŠ è½½, ä¹Ÿå¯ä»¥ä½¿ç”¨load_datasetè¿›è¡Œè„šæœ¬åŠ è½½</p>
<p>è¿™é‡Œä½¿ç”¨çš„æµ‹è¯•çš„æ–‡æœ¬æ˜¯cmrc2018</p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410080918749.png" alt="image-20241008091829631" /></p>
<p>åœ¨ç›´æ¥åŠ è½½çš„æ—¶å€™, éœ€è¦æŒ‡å®šå®é™…çš„æ•°æ®çš„ä½ç½®, å¦åˆ™åŠ è½½çš„æ•°æ®ä¼šåˆ†ç»„å‡ºé”™</p>

<pre class="language-python"><code class="language-python">dataset_json = load_dataset(&quot;json&quot;, data_files=&quot;../dataset/cmrc2018_trial.json&quot;, field=&quot;data&quot;)
</code></pre>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410080925401.png" alt="image-20241008092505281" /></p>
<p>å®é™…åŠ è½½çš„ä¹Ÿä¸æ˜¯å¾ˆå®Œæ•´</p>

<pre class="language-python"><code class="language-python">import json
import datasets
from datasets import DownloadManager, DatasetInfo

class CMRC2018TRIAL(datasets.GeneratorBasedBuilder):

    def _info(self) -&gt; DatasetInfo:
        &quot;&quot;&quot;
            infoæ–¹æ³•, å®šä¹‰æ•°æ®é›†çš„ä¿¡æ¯,è¿™é‡Œè¦å¯¹æ•°æ®çš„å­—æ®µè¿›è¡Œå®šä¹‰
        :return:
        &quot;&quot;&quot;
        return datasets.DatasetInfo(
            description=&quot;CMRC2018 trial&quot;,
            features=datasets.Features({
                    &quot;id&quot;: datasets.Value(&quot;string&quot;),
                    &quot;context&quot;: datasets.Value(&quot;string&quot;),
                    &quot;question&quot;: datasets.Value(&quot;string&quot;),
                    &quot;answers&quot;: datasets.features.Sequence(
                        {
                            &quot;text&quot;: datasets.Value(&quot;string&quot;),
                            &quot;answer_start&quot;: datasets.Value(&quot;int32&quot;),
                        }
                    )
                })
        )

    def _split_generators(self, dl_manager: DownloadManager):
        &quot;&quot;&quot;
            è¿”å›datasets.SplitGenerator
            æ¶‰åŠä¸¤ä¸ªå‚æ•°: nameå’Œgen_kwargs
            name: æŒ‡å®šæ•°æ®é›†çš„åˆ’åˆ†
            gen_kwargs: æŒ‡å®šè¦è¯»å–çš„æ–‡ä»¶çš„è·¯å¾„, ä¸_generate_examplesçš„å…¥å‚æ•°ä¸€è‡´
        :param dl_manager:
        :return: [ datasets.SplitGenerator ]
        &quot;&quot;&quot;
        return [datasets.SplitGenerator(name=datasets.Split.TRAIN, 
                                        gen_kwargs={&quot;filepath&quot;: &quot;../dataset/cmrc2018_trial.json&quot;})]

    def _generate_examples(self, filepath):
        &quot;&quot;&quot;
            ç”Ÿæˆå…·ä½“çš„æ ·æœ¬, ä½¿ç”¨yield
            éœ€è¦é¢å¤–æŒ‡å®škey, idä»0å¼€å§‹è‡ªå¢å°±å¯ä»¥
        :param filepath:
        :return:
        &quot;&quot;&quot;
        # Yields (key, example) tuples from the dataset
        with open(filepath, encoding=&quot;utf-8&quot;) as f:
            data = json.load(f)
            for example in data[&quot;data&quot;]:
                for paragraph in example[&quot;paragraphs&quot;]:
                    context = paragraph[&quot;context&quot;].strip()
                    for qa in paragraph[&quot;qas&quot;]:
                        question = qa[&quot;question&quot;].strip()
                        id_ = qa[&quot;id&quot;]

                        answer_starts = [answer[&quot;answer_start&quot;] for answer in qa[&quot;answers&quot;]]
                        answers = [answer[&quot;text&quot;].strip() for answer in qa[&quot;answers&quot;]]
    				  # è¿”å›å€¼çš„ç¬¬ä¸€ä¸ªå­—æ®µæ˜¯ä¸€ä¸ªid, ä¹‹åæ˜¯å’Œ_split_generators
                        # å­—æ®µé‡Œé¢çš„å£°æ˜ä¸€æ ·çš„æ•°æ®
                        yield id_, {
                            &quot;context&quot;: context,
                            &quot;question&quot;: question,
                            &quot;id&quot;: id_,
                            &quot;answers&quot;: {
                                &quot;answer_start&quot;: answer_starts,
                                &quot;text&quot;: answers,
                            },
                        }

</code></pre>

<pre class="language-python"><code class="language-python">dataset_json = load_dataset(&quot;../python-src/load_script.py&quot;, split=&quot;train&quot;, trust_remote_code=True)
num = 0
for example in dataset_json:
    num += 1
    print(example)
    if num == 5:
        break
</code></pre>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410080947382.png" alt="image-20241008094720254" /></p>
<h3 id="Dataset-with-DataCollator">Dataset with DataCollator</h3>
<p>transformså†…ç½®ä¸€éƒ¨åˆ†çš„DataCollator, æ˜¯ä¸éœ€è¦æˆ‘ä»¬è‡ªå·±å†™çš„</p>
<p>è¿™æ—¶å€™çš„æ•°æ®è¿˜æ˜¯list, å¸Œæœ›ä½¿ç”¨dataloaderæŠŠæ•°æ®æ‹¼æ¥ä¸ºbatchçš„tensor</p>
<p><strong>æ³¨: </strong>ä½¿ç”¨è¿™ä¸€ä¸ªçš„æ—¶å€™é‡Œé¢çš„æ•°æ®åªèƒ½æœ‰transformåŸå§‹çš„å­—æ®µ</p>

<pre class="language-bash"><code class="language-bash">{'label': [1, 1, 1], 'input_ids': [[101, 6655, 4895, 2335, 3763, 1062, 6662, 6772, 6818, 117, 852, 3221, 1062, 769, 2900, 4850, 679, 2190, 117, 1963, 3362, 3221, 107, 5918, 7355, 5296, 107, 4638, 6413, 117, 833, 7478, 2382, 7937, 4172, 119, 2456, 6379, 4500, 1166, 4638, 6662, 5296, 119, 2791, 7313, 6772, 711, 5042, 1296, 119, 102], [101, 1555, 1218, 1920, 2414, 2791, 8024, 2791, 7313, 2523, 1920, 8024, 2414, 3300, 100, 2160, 8024, 3146, 860, 2697, 6230, 5307, 3845, 2141, 2669, 679, 7231, 106, 102], [101, 3193, 7623, 1922, 2345, 8024, 3187, 6389, 1343, 1914, 2208, 782, 8024, 6929, 6804, 738, 679, 1217, 7608, 1501, 4638, 511, 6983, 2421, 2418, 6421, 7028, 6228, 671, 678, 6821, 702, 7309, 7579, 749, 511, 2791, 7313, 3315, 6716, 2523, 1962, 511, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}
</code></pre>

<pre class="language-python"><code class="language-python">collator = DataCollatorWithPadding(tokenizer=tokenizer)
from torch.utils.data import DataLoader
data_loader = DataLoader(tokenized_datasets, collate_fn=collator, batch_size=8, shuffle=True)
</code></pre>

<pre class="language-bash"><code class="language-bash">{'input_ids': tensor([[ 101,  677, 1453,  ...,    0,    0,    0],
        [ 101, 4384, 1862,  ...,    0,    0,    0],
        [ 101, 2791, 7313,  ...,    0,    0,    0],
        ...,
        [ 101, 4289, 5401,  ...,    0,    0,    0],
        [ 101, 1168, 6809,  ..., 3613,  738,  102],
        [ 101, 2791, 7313,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 1, 1, 1, 0, 1, 0, 1])}
</code></pre>
<blockquote>
<p>å®é™…å¡«å……ä¼šæŠŠæ•°æ®å¡«å……åˆ°è¿™ä¸€ä¸ªæ‰¹æ¬¡é‡Œé¢æœ€é•¿çš„</p>
</blockquote>
<h3 id="%E5%AE%9E%E9%99%85%E4%BD%BF%E7%94%A8">å®é™…ä½¿ç”¨</h3>

<pre class="language-python"><code class="language-python">from transformers import DataCollatorWithPadding

dataset = load_dataset(&quot;csv&quot;, data_files=&quot;../dataset/ChnSentiCorp_htl_all.csv&quot;, split=&quot;train&quot;)
dataset = dataset.filter(lambda example: example[&quot;review&quot;] is not None)
# for data in dataset:
#     print(data)

def process_function(examples):
    # æš‚æ—¶ä¸å¡«å……, ç»„æˆbatchæ—¶å†å¡«å……
    tokenized_example = tokenizer(examples[&quot;review&quot;], max_length=128, truncation=True) 
    tokenized_example[&quot;label&quot;] = examples[&quot;label&quot;]
    return tokenized_example

tokenized_datasets = dataset.map(process_function, batched=True, 
                                 remove_columns=dataset.column_names)
print(tokenized_datasets[:3])
</code></pre>

<pre class="language-bash"><code class="language-bash">{'label': [1, 1, 1], 
'input_ids': [
[101, 6655, 4895, 2335, 3763, 1062, 6662, 6772, 6818, 117, 852, 3221, 1062, 769, 2900, 4850, 679, 2190, 117, 1963, 3362, 3221, 107, 5918, 7355, 5296, 107, 4638, 6413, 117, 833, 7478, 2382, 7937, 4172, 119, 2456, 6379, 4500, 1166, 4638, 6662, 5296, 119, 2791, 7313, 6772, 711, 5042, 1296, 119, 102], 
[101, 1555, 1218, 1920, 2414, 2791, 8024, 2791, 7313, 2523, 1920, 8024, 2414, 3300, 100, 2160, 8024, 3146, 860, 2697, 6230, 5307, 3845, 2141, 2669, 679, 7231, 106, 102], 
[101, 3193, 7623, 1922, 2345, 8024, 3187, 6389, 1343, 1914, 2208, 782, 8024, 6929, 6804, 738, 679, 1217, 7608, 1501, 4638, 511, 6983, 2421, 2418, 6421, 7028, 6228, 671, 678, 6821, 702, 7309, 7579, 749, 511, 2791, 7313, 3315, 6716, 2523, 1962, 511, 102]
], 
'token_type_ids': [
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
], 
'attention_mask': [
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
]
}
</code></pre>
<p>è¿™æ—¶å€™çš„æ•°æ®è¿˜æ˜¯list, å¸Œæœ›ä½¿ç”¨dataloaderæŠŠæ•°æ®æ‹¼æ¥ä¸ºbatchçš„tensor</p>

<pre class="language-python"><code class="language-python">collator = DataCollatorWithPadding(tokenizer=tokenizer)
from torch.utils.data import DataLoader
data_loader = DataLoader(tokenized_datasets, collate_fn=collator, batch_size=8, shuffle=True)
</code></pre>
<p>æŠŠæ•°æ®è½¬ä¸ºbatch tensorä»¥åŠè¿›è¡Œå¡«å……</p>
<h3 id="%E4%BB%A3%E7%A0%81%E4%BC%98%E5%8C%96">ä»£ç ä¼˜åŒ–</h3>

<pre class="language-python"><code class="language-python"># æ–‡æœ¬åˆ†ç±»æ¨¡å‹å¾®è°ƒçš„ç¤ºä¾‹
from transformers import AutoTokenizer, AutoModelForSequenceClassification
# åŠ è½½æ•°æ®é›†
from datasets import load_dataset

dataset = load_dataset(&quot;csv&quot;, data_files=&quot;../dataset/ChnSentiCorp_htl_all.csv&quot;, split=&quot;train&quot;)
dataset = dataset.filter(lambda example: example[&quot;review&quot;] is not None and example[&quot;label&quot;] is not None)
print(dataset)
&quot;&quot;&quot;
Dataset({
    features: ['label', 'review'],
    num_rows: 7765
})
&quot;&quot;&quot;

# åˆ’åˆ†æ•°æ®é›†, è·å–è®­ç»ƒé›†ä»¥åŠæµ‹è¯•é›†
datasets = dataset.train_test_split(test_size=0.1)
datasets

&quot;&quot;&quot;
DatasetDict({
    train: Dataset({
        features: ['label', 'review'],
        num_rows: 6988
    })
    test: Dataset({
        features: ['label', 'review'],
        num_rows: 777
    })
})
&quot;&quot;&quot;

import torch

tokenizer = AutoTokenizer.from_pretrained(&quot;../hlfrbt3&quot;)

def process_function(examples):
    # æš‚æ—¶ä¸å¡«å……, ç»„æˆbatchæ—¶å†å¡«å……
    tokenized_example = tokenizer(examples[&quot;review&quot;], max_length=128, truncation=True) 
    tokenized_example[&quot;label&quot;] = examples[&quot;label&quot;]
    return tokenized_example

# å¤„ç†æ•°æ®é›†, æŠŠæ•°æ®é›†è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥å¤„ç†çš„æ ¼å¼(åˆ†è¯å™¨ç¼–ç åçš„æ ¼å¼)
tokenized_datasets = datasets.map(process_function, batched=True, remove_columns=datasets[&quot;train&quot;].column_names)
tokenized_datasets

&quot;&quot;&quot;
DatasetDict({
    train: Dataset({
        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],
        num_rows: 6988
    })
    test: Dataset({
        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],
        num_rows: 777
    })
})
&quot;&quot;&quot;


# å»ºç«‹DataLoader
from torch.utils.data import DataLoader
from transformers import DataCollatorWithPadding
trainset = tokenized_datasets[&quot;train&quot;]
validset = tokenized_datasets[&quot;test&quot;]
# ä¹±åº, ä¸€ç»„å¤§å°ä¸º32, è¿›è¡Œå¡«å……
trainloader = DataLoader(trainset, batch_size=32, shuffle=True, collate_fn=DataCollatorWithPadding(tokenizer))  
validloader = DataLoader(validset, batch_size=32, shuffle=False, collate_fn=DataCollatorWithPadding(tokenizer)) # ä¸ä¹±åº
</code></pre>
<h2 id="Evaluate">Evaluate</h2>
<p>æ˜¯ä¸€ä¸ªæœºå™¨å­¦ä¹ çš„æ¨¡å‹è¯„ä¼°å‡½æ•°åº“, åªéœ€è¦ä¸€è¡Œä»£ç å¯ä»¥åŠ è½½å„ç§ä»»åŠ¡çš„è¯„ä¼°å‡½æ•°</p>
<p><a href="https://huggingface.co/docs/evaluate/index"  target="_blank">ğŸ¤— Evaluate (huggingface.co)</a></p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410081151914.png" alt="image-20241008115134639" /></p>

<pre class="language-python"><code class="language-python">import evaluate
evaluate.list_evaluation_modules()
</code></pre>
<blockquote>
<p>å¯ä»¥ä½¿ç”¨è¿™ä¸ªå‡½æ•°è·å–å¯ä»¥ä½¿ç”¨çš„è¯„ä¼°å‡½æ•°, è¿™é‡Œé¢æœ‰ä¸€éƒ¨åˆ†Huggingfaceå®ç°, å¦ä¸€éƒ¨åˆ†æ˜¯ç¤¾åŒºå®ç°çš„, ä¸æƒ³çœ‹ç¤¾åŒºå®ç°çš„æ—¶å€™å¯ä»¥åŠ ä¸€ä¸ªå‚æ•°<code>include_community=False</code></p>
<p>å¯ä»¥ä½¿ç”¨å‚æ•°<code>with_details=True</code>è·å–æ›´è¯¦ç»†çš„ä¿¡æ¯</p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410081708806.png" alt="image-20241008170841605" /></p>
</blockquote>

<pre class="language-python"><code class="language-python">accuracy = evaluate.load('accuracy') # åŠ è½½
print(accuracy.description) # è·å–æè¿°ä»¥åŠè®¡ç®—æ–¹å¼
&quot;&quot;&quot;
Accuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with:
Accuracy = (TP + TN) / (TP + TN + FP + FN)
 Where:
TP: True positive
TN: True negative
FP: False positive
FN: False negative
&quot;&quot;&quot;
print(accuracy.inputs_description)
&quot;&quot;&quot;
Args:
    predictions (`list` of `int`): Predicted labels.
    references (`list` of `int`): Ground truth labels.
    normalize (`boolean`): If set to False, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples. Defaults to True.
    sample_weight (`list` of `float`): Sample weights Defaults to None.

Returns:
    accuracy (`float` or `int`): Accuracy score. Minimum possible value is 0. Maximum possible value is 1.0, or the number of examples input, if `normalize` is set to `True`.. A higher score means higher accuracy.

Examples:

    Example 1-A simple example
        &gt;&gt;&gt; accuracy_metric = evaluate.load(&quot;accuracy&quot;)
        &gt;&gt;&gt; results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])
        &gt;&gt;&gt; print(results)
        {'accuracy': 0.5}

    Example 2-The same as Example 1, except with `normalize` set to `False`.
        &gt;&gt;&gt; accuracy_metric = evaluate.load(&quot;accuracy&quot;)
        &gt;&gt;&gt; results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)
        &gt;&gt;&gt; print(results)
        {'accuracy': 3.0}

    Example 3-The same as Example 1, except with `sample_weight` set.
        &gt;&gt;&gt; accuracy_metric = evaluate.load(&quot;accuracy&quot;)
        &gt;&gt;&gt; results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], sample_weight=[0.5, 2, 0.7, 0.5, 9, 0.4])
        &gt;&gt;&gt; print(results)
        {'accuracy': 0.8778625954198473}
&quot;&quot;&quot;
</code></pre>
<blockquote>
<p>ç›´æ¥æ‰“å°çš„æ—¶å€™ä¼šæŠŠæ‰€æœ‰çš„æ•°æ®æ‰“å°å‡ºæ¥</p>
</blockquote>

<pre class="language-python"><code class="language-python">result = accuracy.compute(references=[0, 1, 1, 0], predictions=[0, 1, 0, 1])
result
</code></pre>
<p>åœ¨å®é™…åº”ç”¨çš„æ—¶å€™æ•°æ®å¯èƒ½ä¸æ˜¯ä¸€æ¬¡æ€§ä¼ è¿›æ¥çš„</p>

<pre class="language-python"><code class="language-python">for ref, pred in zip([0, 1, 1, 0], [0, 1, 1, 0]):
    accuracy.add(references=ref, predictions=pred)
accuracy.compute()
</code></pre>

<pre class="language-python"><code class="language-python">for ref, pred in zip([[0, 1, 1, 0], [0, 1, 1, 0]], [[0, 1, 0, 1], [0, 1, 1, 0]]):
    accuracy.add_batch(references=ref, predictions=pred)
accuracy.compute()
</code></pre>
<blockquote>

<pre class="language-python"><code class="language-python">zip_data = zip([[0, 1, 1, 0], [0, 1, 1, 0]], [[0, 1, 0, 1], [0, 1, 1, 0]])
list(zip_data)
&quot;&quot;&quot;
[([0, 1, 1, 0], [0, 1, 0, 1]), ([0, 1, 1, 0], [0, 1, 1, 0])]
&quot;&quot;&quot;
for ref, pred in zip([[0, 1, 1, 0], [0, 1, 1, 0]], [[0, 1, 0, 1], [0, 1, 1, 0]]):
    print(ref, pred)
&quot;&quot;&quot;
[0, 1, 1, 0] [0, 1, 0, 1]
[0, 1, 1, 0] [0, 1, 1, 0]
&quot;&quot;&quot;
</code></pre>
</blockquote>
<h3 id="%E5%A4%9A%E6%8C%87%E6%A0%87%E8%AF%84%E4%BC%B0%E5%87%BD%E6%95%B0">å¤šæŒ‡æ ‡è¯„ä¼°å‡½æ•°</h3>
<p>å¯ä»¥åœ¨åŒæ—¶è¿›è¡Œå¤šä¸ªè¯„ä¼°å‡½æ•°</p>

<pre class="language-python"><code class="language-python">clf_metrics = evaluate.combine(['accuracy', 'precision', 'recall', 'f1'])
clf_metrics.compute(references=[0, 1, 1, 0], predictions=[0, 1, 0, 1])
&quot;&quot;&quot;
{'accuracy': 0.5, 'precision': 0.5, 'recall': 0.5, 'f1': 0.5}
&quot;&quot;&quot;
</code></pre>
<h3 id="%E8%AF%84%E4%BC%B0%E7%BB%93%E6%9E%9C%E5%8F%AF%E8%A7%86%E5%8C%96">è¯„ä¼°ç»“æœå¯è§†åŒ–</h3>
<p>è¿™ä¸€ä¸ªåº“åªæœ‰ä¸€ä¸ªé›·è¾¾å›¾çš„æ–¹å¼è¿›è¡Œå¯¹æ¯”ä¸åŒæ¨¡å‹çš„ç»“æœ</p>

<pre class="language-python"><code class="language-python">from  evaluate.visualization import radar_plot
data = [
    {&quot;accuracy&quot;: 0.98, &quot;precision&quot;: 0.97, &quot;recall&quot;: 0.99, &quot;f1&quot;: 0.98},
    {&quot;accuracy&quot;: 0.96, &quot;precision&quot;: 0.99, &quot;recall&quot;: 0.97, &quot;f1&quot;: 0.96},
    {&quot;accuracy&quot;: 0.92, &quot;precision&quot;: 0.96, &quot;recall&quot;: 0.99, &quot;f1&quot;: 0.97},
]
model_names = [&quot;model1&quot;, &quot;model2&quot;, &quot;model3&quot;]
plot = radar_plot(data, model_names)
</code></pre>
<h3 id="%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8">å®é™…åº”ç”¨</h3>

<pre class="language-python"><code class="language-python">import evaluate
clf_metrics = evaluate.combine(['accuracy', 'f1'])

def evaluate():
    &quot;&quot;&quot;
    Description: è¯„ä¼°æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„æ€§èƒ½
    Returns:
        æ¨¡å‹çš„å‡†ç¡®ç‡
    &quot;&quot;&quot;
    model.eval()
    with torch.no_grad():
        for batch in validloader:
            if torch.cuda.is_available():
                batch = {k:v.cuda() for k, v in batch.items()}
            outputs = model(**batch)
            pred = outputs.logits.argmax(dim=-1) # é¢„æµ‹çš„ç±»åˆ«
            clf_metrics.add_batch(predictions=pred.long(), references=batch[&quot;labels&quot;].long())
    return clf_metrics.compute()
</code></pre>
<h2 id="Trainer">Trainer</h2>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410081908319.png" alt="image-20241008190813102" /></p>
<p><a href="https://huggingface.co/docs/transformers/trainer"  target="_blank">Trainer (huggingface.co)</a></p>

<pre class="language-python"><code class="language-python">from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
# è·å–å‚æ•°é›†
train_args = TrainingArguments(output_dir=&quot;./checkpoints&quot;, save_safetensors=True)
train_args

from transformers import DataCollatorWithPadding
# è®­ç»ƒæ¨¡å‹
# args:
#   model: æ¨¡å‹
#   args: è®­ç»ƒå‚æ•°
#   train_dataset: è®­ç»ƒæ•°æ®é›†
#   eval_dataset: è¯„ä¼°æ•°æ®é›†
#   data_collator: æ•°æ®æ”¶é›†å™¨
#   compute_metrics: è¯„ä¼°æŒ‡æ ‡
trainer = Trainer(model=model, args=train_args, 
                  train_dataset=tokenized_datasets[&quot;train&quot;], 
                  eval_dataset=tokenized_datasets[&quot;test&quot;],
                  data_collator=DataCollatorWithPadding(tokenizer=tokenizer),
                  compute_metrics=eval_metrics)
trainer.train()
</code></pre>
<p>åœ¨ä½¿ç”¨è¿™ä¸€ä¸ªçš„æ—¶å€™å°±ä¸å†éœ€è¦dataloaderäº†, æ•°æ®ç»è¿‡é¢„å¤„ç†å°±å¯ä»¥ä½¿ç”¨äº†, ä¹Ÿä¸å†éœ€è¦ä½¿ç”¨cudaåˆ¤å®š</p>
<p>åœ¨æ¯”è¾ƒé«˜çš„ç‰ˆæœ¬ä¼šå‡ºç°è®­ç»ƒå¤±è´¥çš„é—®é¢˜, å¯ä»¥é€šè¿‡é™ä½ç‰ˆæœ¬è§£å†³</p>

<pre class="language-bash"><code class="language-bash">pip install -U transforners==4.42.4
</code></pre>
<p>ä½¿ç”¨æœ€å°‘çš„å‚æ•°çš„æ—¶å€™ä¸ä¼šè¿›è¡Œæµ‹è¯„, å¯ä»¥ä½¿ç”¨<code>trainer.evaluate()</code>è¿›è¡Œæµ‹è¯„, å‚æ•°å¯ä»¥å•ç‹¬æŒ‡å®šä½¿ç”¨çš„æµ‹è¯•é›†</p>
<p>ä¹Ÿå¯ä»¥ä½¿ç”¨<code>trainer.predict(tokenized_datasets[&quot;test&quot;])</code>è¿›è¡Œé¢„æµ‹</p>
<h3 id="%E4%B8%BB%E8%A6%81%E7%9A%84%E5%8F%82%E6%95%B0">ä¸»è¦çš„å‚æ•°</h3>
<h4 id="%E6%95%B0%E6%8D%AE%E9%9B%86">æ•°æ®é›†</h4>

<pre class="language-python"><code class="language-python">per_device_train_batch_size=64,per_device_eval_batch_size=128
</code></pre>
<blockquote>
<p>æ”¹å˜ä¸€ä¸‹è®­ç»ƒçš„æ—¶å€™ä½¿ç”¨batch_size</p>
</blockquote>
<h4 id="log">log</h4>

<pre class="language-python"><code class="language-python">logging_steps=100
</code></pre>
<p>è®¾ç½®ä¸º100æ­¥æ‰“å°ä¸€æ¬¡log</p>
<h4 id="%E8%AF%84%E4%BC%B0">è¯„ä¼°</h4>

<pre class="language-python"><code class="language-python">evaluation_strategy=&quot;epoch&quot;
</code></pre>
<p>æ¯ä¸€è½®è¿›è¡Œä¸€æ¬¡è¯„ä¼°</p>

<pre class="language-python"><code class="language-python">evaluation_strategy=&quot;steps&quot;,eval_steps=100
</code></pre>
<p>æ¯100æ­¥ä¸€æ¬¡</p>

<pre class="language-python"><code class="language-python">metric_for_best_model=&quot;f1&quot;
</code></pre>
<blockquote>
<p>ä½¿ç”¨å“ªä¸€ä¸ªè¯„ä»·è¿™ä¸€ä¸ªæ¨¡å‹æœ€å¥½</p>
</blockquote>
<h4 id="%E4%BF%9D%E5%AD%98">ä¿å­˜</h4>

<pre class="language-python"><code class="language-python">save_strategy=&quot;epoch&quot;
</code></pre>
<p>ä¿å­˜çš„ç­–ç•¥ä¸ºæ¯ä¸€è½®</p>

<pre class="language-python"><code class="language-python">save_total_limit=3
</code></pre>
<p>æœ€å¤šè®°å½•çš„æ¨¡å‹è½®æ•°</p>

<pre class="language-python"><code class="language-python">load_best_model_at_end=True
</code></pre>
<p>åœ¨æœ€åç•™ä¸‹æ¥è®­ç»ƒçš„æœ€å¥½çš„(ä¹‹åçš„æ¨¡å‹åŠ è½½çš„æ˜¯è¿™ä¸€è½®é‡Œé¢çš„å‚æ•°)</p>
<h4 id="%E8%AE%AD%E7%BB%83">è®­ç»ƒ</h4>

<pre class="language-python"><code class="language-python">learning_rate=2e-5, weight_decay=0.01
</code></pre>
<blockquote>
<p>è®­ç»ƒçš„å­¦ä¹ é€Ÿç‡ä»¥åŠæƒé‡è¡°å‡(é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆ)</p>
</blockquote>
<h4 id="%E7%BB%93%E6%9E%9C">ç»“æœ</h4>
<p>ç”Ÿæˆçš„runsè¿™ä¸€ä¸ªæ–‡ä»¶å¤¹å¯ä»¥ä½¿ç”¨tensorboardæŸ¥çœ‹</p>

<pre class="language-bash"><code class="language-bash">tensorboard --logdir dir
</code></pre>
<p>ä¹Ÿå¯ä»¥ä½¿ç”¨vscodeå¯åŠ¨</p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410091950114.png" alt="image-20241009195025104" /></p>
<h2 id="%E6%9C%80%E7%BB%88%E7%9A%84%E7%A4%BA%E4%BE%8B">æœ€ç»ˆçš„ç¤ºä¾‹</h2>
<ul>
<li>åŠ è½½æ•°æ®</li>
</ul>

<pre class="language-python"><code class="language-python"># æ–‡æœ¬åˆ†ç±»æ¨¡å‹å¾®è°ƒçš„ç¤ºä¾‹
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
# åŠ è½½æ•°æ®é›†
from datasets import load_dataset

# ä½¿ç”¨CSVæ–‡ä»¶åŠ è½½æ•°æ®é›†, é»˜è®¤çš„æ—¶å€™æ²¡æœ‰åˆ†ä¸ºä¸åŒè®­ç»ƒé›†
dataset = load_dataset(&quot;csv&quot;,
               data_files=&quot;../dataset/ChnSentiCorp_htl_all.csv&quot;,
               split=&quot;train&quot;)
# å»é™¤æ•°æ®é›†é‡Œé¢çš„æ— çº¿æ•°æ®
dataset = dataset.filter(lambda example: 
                         example[&quot;review&quot;] is not None 
                         and example[&quot;label&quot;] is not None)
# åˆ’åˆ†æ•°æ®é›†, æ•°æ®é›†çš„0.1ä¸ºæµ‹è¯•é›†
datasets = dataset.train_test_split(test_size=0.1)
</code></pre>
<ul>
<li>åˆ†è¯å™¨</li>
</ul>

<pre class="language-python"><code class="language-python">import torch

tokenizer = AutoTokenizer.from_pretrained(&quot;../hlfrbt3&quot;)
# å¯¹æ•°æ®é¢„å¤„ç†, æŠŠæ•°æ®è½¬æ¢ä¸ºtensor, ä»¥åŠåŠ å…¥ç›®æ ‡å€¼, 
# è¿™é‡Œçš„æ•°æ®æ ¼å¼å¦‚ä¸‹
&quot;&quot;&quot;
DatasetDict({
    train: Dataset({
        features: ['label', 'review'],
        num_rows: 6988
    })
    test: Dataset({
        features: ['label', 'review'],
        num_rows: 777
    })
})
&quot;&quot;&quot;
def process_function(examples):
    # æš‚æ—¶ä¸å¡«å……, ç»„æˆbatchæ—¶å†å¡«å……
    tokenized_example = tokenizer(examples[&quot;review&quot;],
                                  max_length=128,
                                  truncation=True) 
    tokenized_example[&quot;labels&quot;] = examples[&quot;label&quot;]
    return tokenized_example

# å¤„ç†æ•°æ®é›†, æŠŠæ•°æ®é›†è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥å¤„ç†çš„æ ¼å¼(åˆ†è¯å™¨ç¼–ç åçš„æ ¼å¼)
# remove_columnså»é™¤åŸå§‹çš„æ•°æ®
tokenized_datasets = datasets.map(process_function,
                  batched=True, 
                  remove_columns=datasets[&quot;train&quot;].column_names)
# æ­¤æ—¶çš„æ•°æ®æ ¼å¼å¦‚ä¸‹
&quot;&quot;&quot;
DatasetDict({
    train: Dataset({
        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],
        num_rows: 6988
    })
    test: Dataset({
        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],
        num_rows: 777
    })
})
&quot;&quot;&quot;
</code></pre>

                        
                    </div>
                </div>
                <div id="previous_next">
                    <div id="previous">
                        
                    </div>
                    <div id="next">
                        
                        <a href="/note/æœºå™¨å­¦ä¹ /2024-10-6-Juoyter.html">
                            <span class="label">2024-10-6-Juoyter</span>
                            <span class="icon"></span>
                        </a>
                        
                    </div>
                </div>
                <div id="comments-container"></div>
            </div>
            <div id="toc_wrapper">
                <div id="toc">
                    <div id="toc_content">
                            
                    </div>
                </div>
            </div>
        </div>
    </div>
    <a id="to_top" href="#"></a>
    <div id="doc_footer">
        <div id="footer">
            <div id="footer_top">
                <ul>
<li><a>é“¾æ¥</a><ul><li><a target="_blank" href="https://teedoc.neucrack.com">ç½‘ç«™ä½¿ç”¨ teedoc ç”Ÿæˆ</a></li>
<li><a target="_blank" href="https://neucrack.com">Copyright Â© 2021 Neucrack</a></li>
<li><a  href="/note/sitemap.xml">ç½‘ç«™åœ°å›¾</a></li>
</ul>
</li>
<li><a>æºç </a><ul><li><a target="_blank" href="https://github.com/XuSenfeng/note/">github</a></li>
<li><a target="_blank" href="https://github.com/teedoc/teedoc">æœ¬ç½‘ç«™æºæ–‡ä»¶</a></li>
</ul>
</li>
</ul>

            </div>
            <div id="footer_bottom">
                <ul>
<li><a target="_blank" href="https://beian.miit.gov.cn">æ¸ICPå¤‡19015320å·</a></li>
<li><a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=44030602004109">ç²¤å…¬ç½‘å®‰å¤‡44030602004109å·</a></li>
</ul>

            </div>
        </div>
    </div>
    
        <script src="/note/teedoc-plugin-markdown-parser/mermaid.min.js"></script>
    
        <script>mermaid.initialize({startOnLoad:true});</script>
    
        <script type="text/javascript">
                var transLoaded = false;
                var loading = false;
                var domain = "translate.google.com";
                var domainDefault = domain;
                var storeDomain = localStorage.getItem("googleTransDomain");
                if(storeDomain){
                    domain = storeDomain;
                    console.log("load google translate domain from local storage:" + domain);
                }
                function getUrl(domain){
                    if(domain == "/")
                        return "/static/js/google_translate/element.js?cb=googleTranslateElementInit";
                    else
                        return "https://" + domain + "/translate_a/element.js?cb=googleTranslateElementInit";
                }
                var url = getUrl(domain);
                console.log("google translate domain:" + domain + ", url: " + url);
                function googleTranslateElementInit() {
                    new google.translate.TranslateElement({pageLanguage: "auto", layout: google.translate.TranslateElement.InlineLayout.SIMPLE}, 'google_translate_element');
                }
                function loadJS( url, callback ){
                    var script = document.createElement('script');
                    fn = callback || function(){ };
                    script.type = 'text/javascript';
                    if(script.readyState){
                        script.onreadystatechange = function(){
                            if( script.readyState == 'loaded' || script.readyState == 'complete' ){
                                script.onreadystatechange = null;
                                fn();
                            }
                        };
                    }else{
                        script.onload = function(){
                            fn();
                        };
                    }
                    script.src = url;
                    document.getElementsByTagName('head')[0].appendChild(script);
                }
                function removeHint(){
                    var hint = document.getElementById("loadingTranslate");
                    if(hint){
                        hint.remove();
                    }
                }
                var btn = document.getElementById("google_translate_element");
                btn.onclick = function(){
                    if(transLoaded) return;
                    if(loading){
                        var flag = confirm("loading from " + domain + ", please wait, or change domain?");
                        if(flag){
                            newDomain = prompt("domain, default: " + domainDefault + ", now: " + domain);
                            if(newDomain){
                                domain = newDomain;
                                console.log(domain);
                                url = getUrl(domain);
                                loadJS(url, function(){
                                    localStorage.setItem("googleTransDomain", domain);
                                    removeHint()
                                    transLoaded = true;
                                });
                            }
                        }
                        return;
                    }
                    btn.innerHTML = '<span id="loadingTranslate"><img class="icon" src="/note/static/image/google_translate/translate.svg"/>Loading ...</span>';
                    loading = true;
                    loadJS(url, function(){
                        localStorage.setItem("googleTransDomain", domain);
                        removeHint()
                        transLoaded = true;
                    });
                }
                </script>
            
    
        <script src="/note/static/js/theme_default/tocbot.min.js"></script>
    
        <script src="/note/static/js/theme_default/main.js"></script>
    
        <script src="/note/static/js/theme_default/viewer.min.js"></script>
    
        <script src="/note/static/css/theme_default/prism.min.js"></script>
    
        <script src="/note/static/js/search/search_main.js"></script>
    
        <script src="/note/static/js/plugin_blog/main.js"></script>
    
        <link rel="stylesheet" href="/note/static/js/add_hint/style.css" type="text/css"/>
    
        <script src="/note/static/js/add_hint/main.js"></script>
    
        <script src="/note/static/js/gitalk/gitalk.min.js"></script>
    
        <script src="/note/static/js/gitalk/main.js"></script>
    
        <script src="/note/static/js/custom.js"></script>
    
</body>

</html>