<!DOCTYPE html>

<html lang="zh-CN"  class="">


<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <meta name="keywords" content="">
    
    
    <meta name="description" content="">
    
    <meta name="generator" content="teedoc">
    <meta name="theme" content="teedoc-plugin-theme-default">
    
        
        <meta name="markdown-generator" content="teedoc-plugin-markdown-parser">
        
        <script>
MathJax = {"loader": {"load": ["output/svg"]}, "tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]]}, "svg": {"fontCache": "global"}};
</script>
        
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
        <meta name="html-generator" content="teedoc-plugin-jupyter-notebook-parser">
        
        <script src="/note/static/js/theme_default/pre_main.js"></script>
        
        <link rel="stylesheet" href="/note/static/css/theme_default/prism.min.css" type="text/css"/>
        
        <link rel="stylesheet" href="/note/static/css/theme_default/viewer.min.css" type="text/css"/>
        
        <link rel="stylesheet" href="/note/static/css/theme_default/dark.css" type="text/css"/>
        
        <link rel="stylesheet" href="/note/static/css/theme_default/light.css" type="text/css"/>
        
        <script src="/note/static/js/theme_default/jquery.min.js"></script>
        
        <script src="/note/static/js/theme_default/split.js"></script>
        
        <link rel="stylesheet" href="/note/static/css/search/style.css" type="text/css"/>
        
        <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?4d52982572d5512e9762879ebf063c86";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
        
        <meta name="blog-generator" content="teedoc-plugin-blog">
        
        <link rel="stylesheet" href="/note/static/css/gitalk/gitalk.css" type="text/css"/>
        
        <link rel="stylesheet" href="/note/static/css/gitalk/custom_gitalk.css" type="text/css"/>
        
        <link rel="stylesheet" href="/note/static/css/custom.css" type="text/css"/>
        
    
    
    <title>Transformerså®æˆ˜ - XvSenfeng's Note</title>
    
    <script type="text/javascript">js_vars = {"teedoc-plugin-ad-hint": {"type": "hint", "label": "â˜†", "content": "è¿™æ˜¯ä¸€ä¸ªæ”¯æŒå›½é™…åŒ–çš„æ¶ˆæ¯ç¤ºä¾‹</br>å–œæ¬¢é¡¹ç›®è¯·<a target=\"_blank\" href=\"https://github.com/teedoc/teedoc\">ç‚¹ä¸‹ â˜† star </a>å“¦~ğŸ¦€ğŸ¦€", "show_times": 2, "show_after_s": 432000, "date": "2021-11-16 14:40", "color": "#a0421d", "link_color": "#e53935", "link_bg_color": "#e6ae5c", "bg_color": "#ffcf89", "color_hover": "white", "bg_color_hover": "#f57c00", "close_color": "#eab971"}}</script>
    <script type="text/javascript">metadata = {"tags": ["AI æœºå™¨å­¦ä¹ "], "date": "2026-02-05", "update": [], "ts": 1770297751, "author": "", "brief": "", "cover": "", "layout": "post"}</script>
</head>


<body class="type_doc">
    
    <div id="navbar">
        <div id="navbar_menu">
            <a class="site_title" href="/note/">
                
                    <img class="site_logo" src="/note/static/image/logo.png" alt="XvSenfeng logo">
                
                
                    <h2>XvSenfeng</h2>
                
        </a>
            <a id="navbar_menu_btn"></a>
        </div>
        <div id="navbar_items">
            <div>
                <ul id="nav_left">
<li class=""><a  href="/note/blog/">åšå®¢</a></li>
<li class=""><a  href="/note/Linux/">Linux</a></li>
<li class=""><a  href="/note/ä»£ç åˆ†æ/">ä»£ç åˆ†æ</a></li>
<li class=""><a  href="/note/ä½¿ç”¨è½¯ä»¶/">ä½¿ç”¨è½¯ä»¶</a></li>
<li class=""><a  href="/note/åµŒå…¥å¼/">åµŒå…¥å¼</a></li>
<li class=""><a  href="/note/æ‰‹æœºå®‰å“/">æ‰‹æœºå®‰å“</a></li>
<li class="active"><a  href="/note/æœºå™¨å­¦ä¹ /">æœºå™¨å­¦ä¹ </a></li>
<li class=""><a  href="/note/ç¼–ç¨‹åŸºç¡€/">ç¼–ç¨‹åŸºç¡€</a></li>
<li class=""><a  href="/note/ç½‘ç»œ/">ç½‘ç»œ</a></li>
</ul>

            </div>
            <div>
                <ul id="nav_right">
<li class=""><a target="_blank" href="https://github.com/XuSenfeng/note/">github</a></li>
</ul>

                <ul class="nav_plugins"><li><a id="google_translate_element"><img class="icon" src="/note/static/image/google_translate/translate.svg"/>Translate</a></li></ul><ul class="nav_plugins"><li><a id="themes" class="light"></a></li></ul><ul class="nav_plugins"><li><a id="search"><span class="icon"></span><span class="placeholder">æœç´¢</span>
                            <div id="search_hints">
                                <span id="search_input_hint">è¾“å…¥å…³é”®è¯ï¼Œå¤šå…³é”®è¯ç©ºæ ¼éš”å¼€</span>
                                <span id="search_loading_hint">æ­£åœ¨åŠ è½½ï¼Œè¯·ç¨å€™ã€‚ã€‚ã€‚</span>
                                <span id="search_download_err_hint">ä¸‹è½½æ–‡ä»¶å¤±è´¥ï¼Œè¯·åˆ·æ–°é‡è¯•æˆ–æ£€æŸ¥ç½‘ç»œ</span>
                                <span id="search_other_docs_result_hint">æ¥è‡ªå…¶å®ƒæ–‡æ¡£çš„ç»“æœ</span>
                                <span id="search_curr_doc_result_hint">å½“å‰æ–‡æ¡£æœç´¢ç»“æœ</span>
                            </div></a></li></ul>
            </div>
        </div>
    </div>
    
    <div id="wrapper">
        <div id="sidebar_wrapper">
            <div id="sidebar">
                <div id="sidebar_title">
                    
                </div>
                <ul class="show">
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /2024-10-5-Transformers.html"><span class="label">2024-10-5-Transformers</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /2024-10-6-Juoyter.html"><span class="label">2024-10-6-Juoyter</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /2024-10-7-Pandasåº“.html"><span class="label">2024-10-7-Pandasåº“</span><span class=""></span></a></li>
<li class="active with_link"><a href="/note/æœºå™¨å­¦ä¹ /2024-10-9-transformså®æˆ˜.html"><span class="label">2024-10-9-transformså®æˆ˜</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /2024-9-21-LLM.html"><span class="label">2024-9-21-LLM</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /2024-9-22-vLLM.html"><span class="label">2024-9-22-vLLM</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /2024-9-22-æ·±åº¦å­¦ä¹ ç¯å¢ƒ.html"><span class="label">2024-9-22-æ·±åº¦å­¦ä¹ ç¯å¢ƒ</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /2024-9-23-01pytorch.html"><span class="label">2024-9-23-01pytorch</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /2024-9-8-æœºå™¨å­¦ä¹ .html"><span class="label">2024-9-8-æœºå™¨å­¦ä¹ </span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /index.html"><span class="label">README</span><span class=""></span></a></li>
<li class="not_active no_link"><a><span class="label">rvæ¨¡å‹éƒ¨ç½²</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /rvæ¨¡å‹éƒ¨ç½²/2025-11-28-00-é©±åŠ¨ç§»æ¤.html"><span class="label">2025-11-28-00-é©±åŠ¨ç§»æ¤</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /rvæ¨¡å‹éƒ¨ç½²/2025-11-28-01-ç¯å¢ƒæ­å»º.html"><span class="label">2025-11-28-01-ç¯å¢ƒæ­å»º</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /rvæ¨¡å‹éƒ¨ç½²/2025-12-2-02-åŸºç¡€æ¦‚å¿µ.html"><span class="label">2025-12-2-02-åŸºç¡€æ¦‚å¿µ</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /rvæ¨¡å‹éƒ¨ç½²/2025-12-3-03-RKNN_Toolkit2.html"><span class="label">2025-12-3-03-RKNN_Toolkit2</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /rvæ¨¡å‹éƒ¨ç½²/2025-12-3-04-RKLLM.html"><span class="label">2025-12-3-04-RKLLM</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /rvæ¨¡å‹éƒ¨ç½²/2026-2-2-05-RKNNå¼€å‘.html"><span class="label">2026-2-2-05-RKNNå¼€å‘</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">vllmä»£ç åˆ†æ</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /vllmä»£ç åˆ†æ/2024-12-31-00-æ•´ä½“æ¡†æ¶.html"><span class="label">2024-12-31-00-æ•´ä½“æ¡†æ¶</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /vllmä»£ç åˆ†æ/2024-12-31-01-collect_env.html"><span class="label">2024-12-31-01-collect_env</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">å…·èº«æ™ºèƒ½</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /å…·èº«æ™ºèƒ½/2026-2-2-åŸºç¡€æ¦‚å¿µ.html"><span class="label">2026-2-2-åŸºç¡€æ¦‚å¿µ</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /å…·èº«æ™ºèƒ½/2026-2-3-02-ä»¿çœŸ.html"><span class="label">2026-2-3-02-ä»¿çœŸ</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /å…·èº«æ™ºèƒ½/2026-2-3-03-å§¿æ€è§£ç®—.html"><span class="label">2026-2-3-03-å§¿æ€è§£ç®—</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /å…·èº«æ™ºèƒ½/2026-2-4-04-SO-ARM101.html"><span class="label">2026-2-4-04-SO-ARM101</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">å®é™…åº”ç”¨</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /å®é™…åº”ç”¨/2025-1-30-ollama.html"><span class="label">2025-1-30-ollama</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /å®é™…åº”ç”¨/2025-12-10-è¯­éŸ³è¯†åˆ«.html"><span class="label">2025-12-10-è¯­éŸ³è¯†åˆ«</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /å®é™…åº”ç”¨/2025-12-22-QLearning.html"><span class="label">2025-12-22-QLearning</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /å®é™…åº”ç”¨/2025-2-16-dify.html"><span class="label">2025-2-16-dify</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /å®é™…åº”ç”¨/2025-2-16-langChain.html"><span class="label">2025-2-16-langChain</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /å®é™…åº”ç”¨/2025-2-23-langchain2.html"><span class="label">2025-2-23-langchain2</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /å®é™…åº”ç”¨/2025-2-26-Loraå¾®è°ƒ.html"><span class="label">2025-2-26-Loraå¾®è°ƒ</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /å®é™…åº”ç”¨/2025-3-20-COZE.html"><span class="label">2025-3-20-COZE</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /å®é™…åº”ç”¨/2025-4-2-MCP.html"><span class="label">2025-4-2-MCP</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">åµŒå…¥å¼ç§»æ¤</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /åµŒå…¥å¼ç§»æ¤/00ç»†èŠ.html"><span class="label">00ç»†èŠ</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /åµŒå…¥å¼ç§»æ¤/2025-12-18-01-æ¨¡å‹é‡åŒ–.html"><span class="label">2025-12-18-01-æ¨¡å‹é‡åŒ–</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">ææ²è¯¾ç¨‹</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/0000-0-0-00åŸºç¡€numpy.html"><span class="label">0000-0-0-00åŸºç¡€numpy</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/0000-0-0-00åŸºç¡€pandas.html"><span class="label">0000-0-0-00åŸºç¡€pandas</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2024-10-31-01.html"><span class="label">2024-10-31-01</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2024-11-03-02æ•°æ®ç±»å‹.html"><span class="label">2024-11-03-02æ•°æ®ç±»å‹</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-08-03åŸºç¡€å‡½æ•°.html"><span class="label">2025-1-08-03åŸºç¡€å‡½æ•°</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-08-04æ•°æ®é›†.html"><span class="label">2025-1-08-04æ•°æ®é›†</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-08-05æ„ŸçŸ¥æœº.html"><span class="label">2025-1-08-05æ„ŸçŸ¥æœº</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-09æ¨¡å‹é€‰æ‹©.html"><span class="label">2025-1-09æ¨¡å‹é€‰æ‹©</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-10-10ä¸¢å¼ƒæ³•.html"><span class="label">2025-1-10-10ä¸¢å¼ƒæ³•</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-11-11æ•°å€¼ç¨³å®šæ€§.html"><span class="label">2025-1-11-11æ•°å€¼ç¨³å®šæ€§</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-12-12å±‚å’Œå—.html"><span class="label">2025-1-12-12å±‚å’Œå—</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-12-GPU.html"><span class="label">2025-1-12-GPU</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-12-å®æˆ˜æ¯”èµ›.html"><span class="label">2025-1-12-å®æˆ˜æ¯”èµ›</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-13-13å·ç§¯.html"><span class="label">2025-1-13-13å·ç§¯</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-13-14æ± åŒ–.html"><span class="label">2025-1-13-14æ± åŒ–</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-14-15LeNet.html"><span class="label">2025-1-14-15LeNet</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-14-16AlexNet.html"><span class="label">2025-1-14-16AlexNet</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-14-17VGG.html"><span class="label">2025-1-14-17VGG</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-14-18NiN.html"><span class="label">2025-1-14-18NiN</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-14-19GoogLeNet.html"><span class="label">2025-1-14-19GoogLeNet</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-15-20æ‰¹é‡å½’ä¸€åŒ–.html"><span class="label">2025-1-15-20æ‰¹é‡å½’ä¸€åŒ–</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-15-21æ®‹å·®ç½‘ç»œResNet.html"><span class="label">2025-1-15-21æ®‹å·®ç½‘ç»œResNet</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-16-èŠ¯ç‰‡.html"><span class="label">2025-1-16-èŠ¯ç‰‡</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-17-22æ•°æ®å¢å¹¿.html"><span class="label">2025-1-17-22æ•°æ®å¢å¹¿</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-17-23å¾®è°ƒ.html"><span class="label">2025-1-17-23å¾®è°ƒ</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-18-24CIFARæ•°æ®é›†.html"><span class="label">2025-1-18-24CIFARæ•°æ®é›†</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-19-25ç›®æ ‡æ£€æµ‹.html"><span class="label">2025-1-19-25ç›®æ ‡æ£€æµ‹</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-1-23-26åŒºåŸŸå·ç§¯ç¥ç»ç½‘ç»œ.html"><span class="label">2025-1-23-26åŒºåŸŸå·ç§¯ç¥ç»ç½‘ç»œ</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-1-27è¯­ä¹‰åˆ†å‰².html"><span class="label">2025-2-1-27è¯­ä¹‰åˆ†å‰²</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-10-33é•¿çŸ­æœŸè®°å¿†LSTM.html"><span class="label">2025-2-10-33é•¿çŸ­æœŸè®°å¿†LSTM</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-10-34æ·±åº¦å¾ªç¯ç½‘ç»œ.html"><span class="label">2025-2-10-34æ·±åº¦å¾ªç¯ç½‘ç»œ</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-10-35-BPTT.html"><span class="label">2025-2-10-35-BPTT</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-10-36åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œ.html"><span class="label">2025-2-10-36åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œ</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-11-37ç¼–ç å™¨è§£ç å™¨.html"><span class="label">2025-2-11-37ç¼–ç å™¨è§£ç å™¨</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-11-38seq2seq.html"><span class="label">2025-2-11-38seq2seq</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-11-39æŸæœç´¢.html"><span class="label">2025-2-11-39æŸæœç´¢</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-11-40æ³¨æ„åŠ›æœºåˆ¶.html"><span class="label">2025-2-11-40æ³¨æ„åŠ›æœºåˆ¶</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-13-41æ³¨æ„åŠ›seq2seq.html"><span class="label">2025-2-13-41æ³¨æ„åŠ›seq2seq</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-15-42è‡ªæ³¨æ„åŠ›.html"><span class="label">2025-2-15-42è‡ªæ³¨æ„åŠ›</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-15-43Transformer.html"><span class="label">2025-2-15-43Transformer</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-15-44BERT.html"><span class="label">2025-2-15-44BERT</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-5-28æ ·å¼è¿ç§».html"><span class="label">2025-2-5-28æ ·å¼è¿ç§»</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-5-29åºåˆ—æ¨¡å‹.html"><span class="label">2025-2-5-29åºåˆ—æ¨¡å‹</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-6-30æ–‡å­—å¤„ç†.html"><span class="label">2025-2-6-30æ–‡å­—å¤„ç†</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-7-31-å¾ªç¯ç¥ç»ç½‘ç»œRNN.html"><span class="label">2025-2-7-31-å¾ªç¯ç¥ç»ç½‘ç»œRNN</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /ææ²è¯¾ç¨‹/2025-2-9-32-é—¨æ§å¾ªç¯å•å…ƒGRU.html"><span class="label">2025-2-9-32-é—¨æ§å¾ªç¯å•å…ƒGRU</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">è§†è§‰è¯†åˆ«</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/2025-12-12-04-ç»å…¸ç®—æ³•.html"><span class="label">2025-12-12-04-ç»å…¸ç®—æ³•</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/2025-12-12-05-YoloV1.html"><span class="label">2025-12-12-05-YoloV1</span><span class=""></span></a></li>
<li class="not_active no_link"><a><span class="label">K230</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/K230/2026-1-1-01-SDKç¼–è¯‘.html"><span class="label">2026-1-1-01-SDKç¼–è¯‘</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/K230/2026-1-10-03-Linuxä»£ç å¼€å‘.html"><span class="label">2026-1-10-03-Linuxä»£ç å¼€å‘</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/K230/2026-1-3-02-åŸºç¡€åŸç†.html"><span class="label">2026-1-3-02-åŸºç¡€åŸç†</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">MaixCAM</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/MaixCAM/2025-12-11-02-æ¨¡å‹è®­ç»ƒ.html"><span class="label">2025-12-11-02-æ¨¡å‹è®­ç»ƒ</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/MaixCAM/2025-12-11-03-å¸¸ç”¨çš„æ¨¡å‹.html"><span class="label">2025-12-11-03-å¸¸ç”¨çš„æ¨¡å‹</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/MaixCAM/2025-12-14-04-Yoloæ¨¡å‹è½¬æ¢.html"><span class="label">2025-12-14-04-Yoloæ¨¡å‹è½¬æ¢</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/MaixCAM/2025-12-14-05-MaixCDKåŸºç¡€ä½¿ç”¨.html"><span class="label">2025-12-14-05-MaixCDKåŸºç¡€ä½¿ç”¨</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/MaixCAM/2025-12-16-06-maixcdkå·¥å…·.html"><span class="label">2025-12-16-06-maixcdkå·¥å…·</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/MaixCAM/2025-12-17-07-ç»„ä»¶.html"><span class="label">2025-12-17-07-ç»„ä»¶</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/MaixCAM/2025-12-17-08-Cameraç¤ºä¾‹.html"><span class="label">2025-12-17-08-Cameraç¤ºä¾‹</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/MaixCAM/2025-12-17-09-APP.html"><span class="label">2025-12-17-09-APP</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/MaixCAM/2025-12-18-06-æ¨¡å‹ç›¸å…³å‚æ•°.html"><span class="label">2025-12-18-06-æ¨¡å‹ç›¸å…³å‚æ•°</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/MaixCAM/2025-12-19-07-AIç¼–è¯‘å™¨.html"><span class="label">2025-12-19-07-AIç¼–è¯‘å™¨</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/MaixCAM/2025-12-19-08-æ¨¡å‹å®ç°.html"><span class="label">2025-12-19-08-æ¨¡å‹å®ç°</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/MaixCAM/2025-12-20-09-MaixPyç¼–è¯‘.html"><span class="label">2025-12-20-09-MaixPyç¼–è¯‘</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/MaixCAM/2025-12-3-00-èµ„æ–™.html"><span class="label">2025-12-3-00-èµ„æ–™</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/MaixCAM/2025-12-5-01ç¼–è¯‘ä¸‹è½½.html"><span class="label">2025-12-5-01ç¼–è¯‘ä¸‹è½½</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">yolo</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/yolo/2025-12-12-02-åŸºç¡€ä½¿ç”¨.html"><span class="label">2025-12-12-02-åŸºç¡€ä½¿ç”¨</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/yolo/2025-12-12-03-æ•°æ®é›†.html"><span class="label">2025-12-12-03-æ•°æ®é›†</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/yolo/2025-12-12-04-é¢„æµ‹.html"><span class="label">2025-12-12-04-é¢„æµ‹</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/yolo/2025-12-13-05-è®­ç»ƒ.html"><span class="label">2025-12-13-05-è®­ç»ƒ</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/yolo/2025-12-13-06-yolov5åŸºç¡€ä½¿ç”¨.html"><span class="label">2025-12-13-06-yolov5åŸºç¡€ä½¿ç”¨</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/yolo/2025-12-13-07-AutoDLæœåŠ¡å™¨è®­ç»ƒ.html"><span class="label">2025-12-13-07-AutoDLæœåŠ¡å™¨è®­ç»ƒ</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/yolo/2025-12-13-08-yolov5æ¡†æ¶.html"><span class="label">2025-12-13-08-yolov5æ¡†æ¶</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/yolo/2025-12-13-09-ä¿®æ”¹ç½‘ç»œ.html"><span class="label">2025-12-13-09-ä¿®æ”¹ç½‘ç»œ</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/yolo/2025-12-13-10-æ¨¡å‹éƒ¨ç½².html"><span class="label">2025-12-13-10-æ¨¡å‹éƒ¨ç½²</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/note/æœºå™¨å­¦ä¹ /è§†è§‰è¯†åˆ«/yolo/2025-12-4-01-yoloå®‰è£….html"><span class="label">2025-12-4-01-yoloå®‰è£…</span><span class=""></span></a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
        </div>
        <div id="article">
            <div id="menu_wrapper">
                <div id="menu">
                </div>
            </div>
            <div id="content_wrapper">
                <div id="content_body">
                    <div id="article_head">
                        <div id="article_title">
                            
                            <h1>Transformerså®æˆ˜</h1>
                            
                        </div>
                        <div id="article_tags">
                            <ul>
                            
                                <li>AI æœºå™¨å­¦ä¹ </li>
                            
                            </ul>
                        </div>
                        <div id="article_info">
                        <div id="article_info_left">
                            <span class="article_author">
                                
                            </span>
                            
                                <span class="article_date" title="æœ€åä¿®æ”¹æ—¥æœŸï¼š 2026-02-05">
                                    2026-02-05
                                </span>
                            
                        </div>
                        <div id="article_info_right">
                            
                            <div id="source_link">
                                <a href="https://github.com/XuSenfeng/note/tree/master/doc/æœºå™¨å­¦ä¹ /2024-10-9-transformså®æˆ˜.md" target="_blank">
                                    ç¼–è¾‘æœ¬é¡µ
                                </a>
                            </div>
                            
                        </div>
                        </div>
                    </div>
                    <div id="article_tools">
                        <span></span>
                        <span id="toc_btn"></span>
                    </div>
                    <div id="update_history">
                        
                    </div>
                    <div id="article_content">
                        
                            <h1 id="Transformers%E5%AE%9E%E6%88%98">Transformerså®æˆ˜</h1>
<h2 id="%E5%9F%BA%E4%BA%8ETransforms%E7%9A%84NLP%E8%A7%A3%E5%86%B3">åŸºäºTransformsçš„NLPè§£å†³</h2>
<blockquote>
<p>NLPè‡ªç„¶è¯­è¨€å¤„ç†, *ModelForSequenceClassification</p>
</blockquote>
<ul>
<li>å¯¼å…¥ç›¸å…³çš„åŒ… General</li>
<li>åŠ è½½æ•°æ®é›† Dataset</li>
<li>æ•°æ®é›†åˆ’åˆ† Dataset</li>
<li>æ•°æ®é›†é¢„å¤„ç† Tokenizer + Dataset</li>
<li>æ„å»ºæ¨¡å‹ Model</li>
<li>è®¾ç½®è¯„ä¼°å‡½æ•° Evaluate</li>
<li>è®¾ç½®è®­ç»ƒå‚æ•° TrainingArguments</li>
<li>åˆ›å»ºè®­ç»ƒå™¨ trainer + Data Collator</li>
<li>è¯„ä¼°æ¨¡å‹, é¢„æµ‹æ•°æ®é›† Trainer</li>
<li>æ¨¡å‹é¢„æµ‹, å•æ¡ Pipe</li>
</ul>
<h2 id="%E6%98%BE%E5%AD%98%E4%BC%98%E5%8C%96">æ˜¾å­˜ä¼˜åŒ–</h2>
<h3 id="%E5%8D%A0%E7%94%A8%E5%88%86%E6%9E%90">å ç”¨åˆ†æ</h3>
<ul>
<li>æ¨¡å‹çš„æƒé‡</li>
</ul>
<p>4Bytes * æ¨¡å‹çš„å‚æ•°é‡</p>
<blockquote>
<p>å¤§æ¨¡å‹æƒé‡æ˜¯æŒ‡æ¨¡å‹ä¸­æ¯ä¸ªç¥ç»å…ƒè¿æ¥çš„å‚æ•°ã€‚è¿™äº›æƒé‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ–­è°ƒæ•´ï¼Œä»¥ä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å‡†ç¡®åœ°é¢„æµ‹è¾“å‡ºã€‚ç®€å•æ¥è¯´ï¼Œæƒé‡å†³å®šäº†è¾“å…¥æ•°æ®å¦‚ä½•é€šè¿‡æ¨¡å‹è¢«å¤„ç†å’Œè½¬æ¢ã€‚</p>
</blockquote>
<ul>
<li>ä¼˜åŒ–å™¨çŠ¶æ€optimizer state</li>
</ul>
<p>adamçš„momentum + variance å  2*4Byte å†²é‡å’Œæ–¹å·®</p>
<p>æ··åˆç²¾åº¦çš„å®ç°ä¸­ï¼Œéœ€è¦å¤åˆ¶ä¸€ä»½fp32çš„å‚æ•°ä½œä¸ºè¢«optimizeræ›´æ–°çš„å‚æ•°ï¼Œ 1*4Byte</p>
<blockquote>
<p>è¿™äº›æ•°æ®åŒ…æ‹¬æ¨¡å‹æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ã€å­¦ä¹ ç‡ã€åŠ¨é‡ç­‰ä¿¡æ¯ï¼Œé€šè¿‡è¿™äº›æ•°æ®ï¼Œä¼˜åŒ–å™¨èƒ½å¤Ÿå¸®åŠ©æ¨¡å‹é€æ­¥æ”¶æ•›åˆ°æœ€ä¼˜è§£ã€‚</p>
<ol>
<li>æ¢¯åº¦æ›´æ–°ï¼šä¼˜åŒ–å™¨èƒ½å¤Ÿæ ¹æ®æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦æ¥æ›´æ–°å‚æ•°å€¼ï¼Œé€šè¿‡ä¸æ–­è¿­ä»£ä¼˜åŒ–å‚æ•°ï¼Œä½¿æ¨¡å‹é€æ¸æ”¶æ•›åˆ°æœ€ä¼˜è§£ã€‚</li>
<li>å­¦ä¹ ç‡è°ƒæ•´ï¼šä¼˜åŒ–å™¨å¯ä»¥æ ¹æ®æ¨¡å‹å½“å‰çš„æ€§èƒ½è¡¨ç°åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ï¼Œä½¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´åŠ ç¨³å®šå’Œé«˜æ•ˆã€‚</li>
<li>åŠ¨é‡è®¡ç®—ï¼šä¼˜åŒ–å™¨è¿˜å¯ä»¥é€šè¿‡è®°å½•çš„åŠ¨é‡ä¿¡æ¯ï¼Œå¸®åŠ©æ¨¡å‹åœ¨æ›´æ–°å‚æ•°æ—¶æ›´å¥½åœ°è·³å‡ºå±€éƒ¨æœ€ä¼˜è§£ï¼Œé¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜è§£ã€‚</li>
<li>è¿‡æ‹Ÿåˆé¿å…ï¼šä¼˜åŒ–å™¨è¿˜å¯ä»¥é€šè¿‡è®°å½•çš„æ•°æ®å¸®åŠ©æ¨¡å‹é¿å…è¿‡æ‹Ÿåˆï¼Œé€šè¿‡æ­£åˆ™åŒ–ç­‰æŠ€æœ¯æ¥ä¿æŒæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
</blockquote>
<ul>
<li>æ¢¯åº¦</li>
</ul>
<p>4 Byte * æ¨¡å‹å‚æ•°é‡</p>
<blockquote>
<p>æ¢¯åº¦æ•°æ®æ˜¯æŒ‡æ¯ä¸ªå‚æ•°å¯¹åº”çš„æ¢¯åº¦å€¼ï¼Œè¡¨ç¤ºäº†ç›®æ ‡å‡½æ•°åœ¨å½“å‰å‚æ•°å€¼å¤„çš„å˜åŒ–ç‡ã€‚</p>
</blockquote>
<ul>
<li>å‘å‰æ¿€æ´»é‡</li>
</ul>
<p>åºåˆ—é•¿åº¦, éšå±‚ç»´åº¦, Batchå¤§å°ç­‰</p>
<blockquote>
<p>ä»è¾“å…¥å±‚å¼€å§‹å‘å‰ä¼ æ’­çš„æ¿€æ´»å€¼ã€‚åœ¨æ·±åº¦ç¥ç»ç½‘ç»œä¸­ï¼Œæ¯ä¸ªç¥ç»å…ƒéƒ½æœ‰ä¸€ä¸ªæ¿€æ´»å‡½æ•°ï¼Œæ¿€æ´»å‡½æ•°å¯¹è¾“å…¥çš„åŠ æƒå’Œè¿›è¡Œéçº¿æ€§å˜æ¢å¹¶äº§ç”Ÿè¾“å‡ºï¼Œè¿™ä¸ªè¾“å‡ºå°±æ˜¯æ¿€æ´»å€¼ã€‚</p>
<p>åœ¨æ¨¡å‹çš„å‘å‰ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œè¾“å…¥æ•°æ®ç»è¿‡å¤šå±‚ç¥ç»å…ƒè¿›è¡ŒåŠ æƒå’Œæ¿€æ´»æ“ä½œï¼Œé€å±‚ä¼ é€’å¹¶äº§ç”Ÿæ–°çš„æ¿€æ´»å€¼ï¼Œæœ€ç»ˆåœ¨è¾“å‡ºå±‚äº§ç”Ÿé¢„æµ‹æˆ–åˆ†ç±»ç»“æœã€‚è¿™ä¸€ç³»åˆ—æ¿€æ´»å€¼ç›¸äº’ä¼ é€’ï¼Œå¹¶åœ¨å„å±‚ä¹‹é—´åæ˜ äº†æ¨¡å‹å¯¹æ•°æ®çš„ç†è§£å’Œæå–çš„ç‰¹å¾ã€‚</p>
<p>éšå±‚ï¼ˆæˆ–éšè—å±‚ï¼‰ç»´åº¦æ˜¯æŒ‡ç¥ç»ç½‘ç»œä¸­çš„ä¸­é—´å±‚çš„ç»´åº¦å¤§å°ã€‚éšå±‚æ˜¯ä»‹äºè¾“å…¥å±‚å’Œè¾“å‡ºå±‚ä¹‹é—´çš„ä¸€å±‚æˆ–å¤šå±‚ç¥ç»å…ƒå±‚ï¼Œè´Ÿè´£å¯¹è¾“å…¥æ•°æ®è¿›è¡Œç‰¹å¾æå–å’Œè¡¨ç¤ºå­¦ä¹ ã€‚éšå±‚çš„ç»´åº¦å†³å®šäº†ç¥ç»ç½‘ç»œå¯ä»¥å­¦ä¹ åˆ°çš„ç‰¹å¾çš„å¤æ‚åº¦å’Œä¸°å¯Œæ€§ã€‚</p>
</blockquote>
<h3 id="%E5%AE%9E%E9%99%85%E4%BC%98%E5%8C%96">å®é™…ä¼˜åŒ–</h3>
<p>ä½¿ç”¨æ¨¡å‹æ˜¯<code>hfl/chinese-macbert-large</code></p>
<p>ä½¿ç”¨çš„å‚æ•°ä¸ºbach=32, maxlength=128</p>
<ul>
<li>å¼€å§‹å‰</li>
</ul>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410111800477.png" alt="image-20241011180016774" /></p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410111801981.png" alt="image-20241011180107925" /></p>
<p>å®é™…çš„è®­ç»ƒéå¸¸çš„æ…¢é¢„ä¼°ä¸€ä¸ªå¤šå°æ—¶<img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410111829757.png" alt="image-20241011182919692" /></p>
<ul>
<li>ä½¿ç”¨bach sizeä¸º1, ä½†æ˜¯gradient accumulationä¸º32, ä¹Ÿå°±æ˜¯æ¯è®¡ç®—32æ¬¡æ›´æ–°ä¸€ä¸‹å‚æ•°, ä¼˜åŒ–å‘å‰æ¿€æ´»å€¼</li>
</ul>

<pre class="language-python"><code class="language-python">gradient_accumulation_steps=32,
</code></pre>
<p>ä½¿ç”¨è¿™ä¸€ä¸ªçš„æ—¶å€™ä½¿ç”¨çš„å†…å­˜ä¸‹é™, ä½†æ˜¯è®­ç»ƒä½¿ç”¨çš„æ—¶é—´å¤§å¤§åŠ é•¿(æˆ‘çš„ç”µè„‘çš„æ˜¾å­˜è¿˜æ˜¯å…¨éƒ¨å ç”¨), ä¸‹é™äº†ä¸€åŠ</p>
<p>GPUä¸€èˆ¬æ¯”è¾ƒæ“…é•¿å¹¶è¡Œçš„è®¡ç®—, æ‰€ä»¥è¿™ä¸€ä¸ªæ²¡æœ‰å……åˆ†åˆ©ç”¨</p>
<ul>
<li>åœ¨è®¡ç®—çš„æ—¶å€™æœ‰å¾ˆå¤šä¸­é—´çš„ç»“æœ, å¯ä»¥ä¸è®°å½•, ä½¿ç”¨çš„æ—¶å€™å†æ¬¡è®¡ç®—Gradient Checkpoints</li>
</ul>

<pre class="language-python"><code class="language-python">gradient_checkpointing=True
</code></pre>
<ul>
<li>ä½¿ç”¨å†…å­˜å ç”¨æ¯”è¾ƒå°çš„ä¼˜åŒ–å™¨, é»˜è®¤ä½¿ç”¨çš„æ˜¯adamw_torchè¿™ä¸€ä¸ªä¼˜åŒ–å™¨</li>
</ul>

<pre class="language-python"><code class="language-python">optim=&quot;adafactor&quot;
</code></pre>
<p>GPUå†…å­˜ä½¿ç”¨çš„å†æ¬¡ä¸‹é™, æ—¶é—´åŠ é•¿</p>
<ul>
<li>åœ¨è®­ç»ƒçš„æ—¶å€™åªè®­ç»ƒä¸€éƒ¨åˆ†çš„å‚æ•°, æ¯”å¦‚åªè®­ç»ƒå…¨è¿æ¥å±‚, ä¸è®­ç»ƒbert</li>
</ul>
<blockquote>
<p>åœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œå…¨è¿æ¥å±‚é€šå¸¸ä½œä¸ºç½‘ç»œçš„æœ€åä¸€å±‚ï¼Œç›´æ¥å°†å…¨è¿æ¥å±‚çš„ç»´åº¦è®¾ä¸ºç±»åˆ«æ•°é‡æˆ–é€šè¿‡Softmaxå‡½æ•°è¾“å‡ºæ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡åˆ†å¸ƒï¼Œä»è€Œå®ç°å¯¹è¾“å…¥æ•°æ®çš„åˆ†ç±»ã€‚å¦‚æœè¯´å·ç§¯å±‚ã€æ± åŒ–å±‚å’Œæ¿€æ´»å‡½æ•°ç­‰æ“ä½œæ˜¯å°†åŸå§‹æ•°æ®æ˜ å°„åˆ°éšå±‚ç‰¹å¾ç©ºé—´çš„è¯ï¼Œå…¨è¿æ¥å±‚åˆ™èµ·åˆ°å°†å­¦åˆ°çš„â€œåˆ†å¸ƒå¼ç‰¹å¾è¡¨ç¤ºâ€æ˜ å°„åˆ°æ ·æœ¬æ ‡è®°ç©ºé—´çš„ä½œç”¨ã€‚</p>
<p><a href="https://zhuanlan.zhihu.com/p/403495863"  target="_blank">è¯»æ‡‚BERTï¼Œçœ‹è¿™ä¸€ç¯‡å°±å¤Ÿäº† - çŸ¥ä¹ (zhihu.com)</a></p>
</blockquote>

<pre class="language-python"><code class="language-python">for name, param in model.bert.named_parameters():
    param.requires_grad = False
</code></pre>
<h2 id="%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB">å‘½åå®ä½“è¯†åˆ«</h2>
<p>NER Named Entity(å®ä½“) Recognitionç”¨äºè¯†åˆ«æ–‡æœ¬é‡Œé¢çš„ç‰¹å®šæ„ä¹‰çš„å®ä½“, äººååœ°å, æœºæ„å, ä¸“æœ‰åè¯ç­‰, ä¸»è¦æ˜¯åŒ…æ‹¬ä¸¤éƒ¨åˆ†(1)å®ä½“è¾¹ç•Œè¯†åˆ«(2)ç¡®å®šå®ä½“è¯†åˆ«(äººå, åœ°å, æœºæ„å)</p>
<blockquote>
<p>*ModelForTokenClassification</p>
</blockquote>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410120958515.png" alt="image-20241012095849325" /></p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410121000892.png" alt="image-20241012100051830" /></p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410121009651.png" alt="image-20241012100903586" /></p>
<h3 id="%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84">æ¨¡å‹ç»“æ„</h3>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410121014290.png" alt="image-20241012101417218" /></p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410121020584.png" alt="image-20241012102025517" /></p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410121023968.png" alt="image-20241012102319901" /></p>
<h3 id="%E5%AE%9E%E9%99%85%E8%AE%AD%E7%BB%83">å®é™…è®­ç»ƒ</h3>
<p>ä½¿ç”¨çš„æ•°æ®é›†æ˜¯<code>peoples_daily_ner</code>, ä½¿ç”¨çš„æ¨¡å‹æ˜¯<code>hfl/chinese-macbert-base</code></p>
<ul>
<li>å¯¼åŒ…</li>
</ul>

<pre class="language-python"><code class="language-python">import evaluate 
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification
</code></pre>
<ul>
<li>è·å–æ•°æ®é›†</li>
</ul>

<pre class="language-python"><code class="language-python">ner_dataset = load_dataset(&quot;peoples_daily_ner&quot;, trust_remote_code=True)
ner_dataset[&quot;train&quot;][0]
</code></pre>
<blockquote>

<pre class="language-python"><code class="language-python">{'id': '0',
 'tokens':['æµ·','é’“','æ¯”','èµ›','åœ°','ç‚¹','åœ¨','å¦','é—¨','ä¸',
           'é‡‘','é—¨','ä¹‹','é—´','çš„','æµ·','åŸŸ','ã€‚'],
 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0]}
</code></pre>

<pre class="language-python"><code class="language-python">ner_dataset[&quot;train&quot;].features
&quot;&quot;&quot;
{'id': Value(dtype='string', id=None),
 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),
 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None), length=-1, id=None)}
 PER: äººå ORG: ç»„ç»‡å LOC: åœ°å
&quot;&quot;&quot;
</code></pre>
</blockquote>

<pre class="language-python"><code class="language-python">label_list = ner_dataset[&quot;train&quot;].features[&quot;ner_tags&quot;].feature.names
</code></pre>
<ul>
<li>å¤„ç†æ•°æ®é›†</li>
</ul>

<pre class="language-python"><code class="language-python">tokenizer = AutoTokenizer.from_pretrained(&quot;hfl/chinese-macbert-base&quot;)
# åœ¨å¤„ç†çš„æ—¶å€™ç”±äºå·²ç»åˆ†è¯, æ‰€ä»¥éœ€è¦ä½¿ç”¨è¿™ä¸€ä¸ªå‚æ•°
tokenizer(ner_dataset[&quot;train&quot;][0][&quot;tokens&quot;], is_split_into_words=True) 
</code></pre>
<blockquote>
<p>ä½†æ˜¯è¿™ä¸€ä¸ªæ‹†åˆ†å¯èƒ½ä¸æ˜¯åˆ†è¯å™¨ä½¿ç”¨çš„æ‹†åˆ†æ–¹å¼, å¯ä»¥ä½¿ç”¨è·å–ä¿¡æ¯word_idsåŒºåˆ†å“ªå‡ ä¸ªToken_idsæ˜¯ä¸€ä¸ªè¯çš„</p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410121106150.png" alt="image-20241012110603090" /></p>
</blockquote>

<pre class="language-python"><code class="language-python">def process_function(examples):
    tokenized_examples = tokenizer(examples[&quot;tokens&quot;], max_length=128, truncation=True, is_split_into_words=True)
    labels = []
    for i, label in enumerate(examples[&quot;ner_tags&quot;]):
        # è·å–ä¸€ä¸ªæ ‡ç­¾
        word_ids = tokenized_examples.word_ids(batch_index=i)
        label_ids = []
        for word_idx in word_ids:
            # è·å–å½“å‰ token æ˜¯å¦æ˜¯ä¸€ä¸ª word çš„ç¬¬ä¸€ä¸ª token
            if word_idx is None:
                label_ids.append(-100) # -100 ä»£è¡¨ä¸æ˜¯ä¸€ä¸ª token
            else:
                label_ids.append(label[word_idx])
        labels.append(label_ids)
    tokenized_examples[&quot;labels&quot;] = labels
    return tokenized_examples

tokenized_datasets = ner_dataset.map(process_function, batched=True, remove_columns=ner_dataset[&quot;train&quot;].column_names)
</code></pre>
<blockquote>

<pre class="language-python"><code class="language-python">tokenized_datasets[&quot;train&quot;][0]
</code></pre>

<pre class="language-python"><code class="language-python">{'input_ids': [101,3862,7157,3683,
  ...,
  1818,511,102],
 'token_type_ids': [0,0,0,
  ...,
  0,0],
 'attention_mask': [1,1,
  ...,
  1],
 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0, -100]}
</code></pre>
</blockquote>
<ul>
<li>æ¨¡å‹</li>
</ul>

<pre class="language-pyrhon"><code class="language-pyrhon">model = AutoModelForTokenClassification.from_pretrained(&quot;hfl/chinese-macbert-base&quot;)
</code></pre>
<ul>
<li>è¯„ä¼°å‡½æ•°</li>
</ul>

<pre class="language-python"><code class="language-python">seqeval = evaluate.load(&quot;seqeval&quot;)
</code></pre>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410121131311.png" alt="image-20241012113140256" /></p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410121135608.png" alt="image-20241012113533547" /></p>

<pre class="language-python"><code class="language-python">import numpy as np
def eval_metric(pred):
    predictions, labels = pred
    predictions = np.argmax(predictions, axis=-1) # è·å–å®é™…çš„é¢„æµ‹å€¼

    # Remove ignored index (special tokens)
    true_predictions = [
        [label_list[p] for (p, l) in zip(prediction, label) if l != -100] # å¯¹åº”æ¯ä¸€ä¸ªæ•°æ®
        for prediction, label in zip(predictions, labels) # éå†ä¸€ä¸ªbatch
    ]
    true_labels = [
        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]

    results = seqeval.compute(true_predictions, true_labels, mode=&quot;strict&quot;, scheme=&quot;BIO2&quot;)

    return {
        &quot;f1&quot;: results[&quot;overall_f1&quot;]
    }
</code></pre>
<ul>
<li>å‚æ•°</li>
</ul>

<pre class="language-python"><code class="language-python">args = TrainingArguments(
    output_dir=&quot;model_for_ner&quot;,
    per_device_eval_batch_size=64,
    per_device_train_batch_size=128,
    evaluation_strategy=&quot;epoch&quot;,
    save_strategy=&quot;epoch&quot;,
    metric_for_best_model=&quot;f1&quot;,
    load_best_model_at_end=True,
    logging_steps=50
)
</code></pre>

<pre class="language-python"><code class="language-python">trainer.train()
trainer.evaluate(eval_dataset=tokenized_datasets[&quot;test&quot;])
</code></pre>
<ul>
<li>æµ‹è¯•</li>
</ul>

<pre class="language-python"><code class="language-python">from transformers import pipeline
model.config.id2label = {idx: label for idx, label in enumerate(label_list)} # ä¿®æ”¹ä¸€ä¸‹æ˜ å°„
&quot;&quot;&quot;é»˜è®¤çš„æ˜ å°„æ˜¯
  &quot;id2label&quot;: {
    &quot;0&quot;: &quot;LABEL_0&quot;,
    &quot;1&quot;: &quot;LABEL_1&quot;,
    &quot;2&quot;: &quot;LABEL_2&quot;,
    &quot;3&quot;: &quot;LABEL_3&quot;,
    &quot;4&quot;: &quot;LABEL_4&quot;,
    &quot;5&quot;: &quot;LABEL_5&quot;,
    &quot;6&quot;: &quot;LABEL_6&quot;
  },
&quot;&quot;&quot;
ner_pipe = pipeline(&quot;token-classification&quot;, 
                    model=model, tokenizer=tokenizer, 
                    device=0, 
                    aggregation_strategy=&quot;simple&quot;) # ä½¿ç”¨èšåˆçš„æ¨¡å¼è¾“å‡º

&quot;&quot;&quot;
[{'entity_group': 'PER',
  'score': 0.99990124,
  'word': 'å² å‡¯ æ­Œ',
  'start': 0,
  'end': 3},
 {'entity_group': 'LOC',
  'score': 0.9998995,
  'word': 'åŒ— äº¬',
  'start': 4,
  'end': 6}]
&quot;&quot;&quot;
</code></pre>
<blockquote>
<p><a href="https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TokenClassificationPipeline"  target="_blank">Pipelines (huggingface.co)</a></p>
<ul>
<li>â€œnoneâ€ : Will simply not do any aggregation and simply return raw results from the model</li>
<li>â€œsimpleâ€ : Will attempt to group entities following the default schema. (A, B-TAG), (B, I-TAG), (C, I-TAG), (D, B-TAG2) (E, B-TAG2) will end up being [{â€œwordâ€: ABC, â€œentityâ€: â€œTAGâ€}, {â€œwordâ€: â€œDâ€, â€œentityâ€: â€œTAG2â€}, {â€œwordâ€: â€œEâ€, â€œentityâ€: â€œTAG2â€}] Notice that two consecutive B tags will end up as different entities. On word based languages, we might end up splitting words undesirably : Imagine Microsoft being tagged as [{â€œwordâ€: â€œMicroâ€, â€œentityâ€: â€œENTERPRISEâ€}, {â€œwordâ€: â€œsoftâ€, â€œentityâ€: â€œNAMEâ€}]. Look for FIRST, MAX, AVERAGE for ways to mitigate that and disambiguate words (on languages that support that meaning, which is basically tokens separated by a space). These mitigations will only work on real words, â€œNew yorkâ€ might still be tagged with two different entities.</li>
<li>â€œfirstâ€ : (works only on word based models) Will use the <code>SIMPLE</code> strategy except that words, cannot end up with different tags. Words will simply use the tag of the first token of the word when there is ambiguity.</li>
<li>â€œaverageâ€ : (works only on word based models) Will use the <code>SIMPLE</code> strategy except that words, cannot end up with different tags. scores will be averaged first across tokens, and then the maximum label is applied.</li>
<li>â€œmaxâ€ : (works only on word based models) Will use the <code>SIMPLE</code> strategy except that words, cannot end up with different tags. Word entity will simply be the token with the maximum score.</li>
</ul>
</blockquote>
<ul>
<li>å»ç©ºæ ¼</li>
</ul>

<pre class="language-python"><code class="language-python">x = &quot;å²å‡¯æ­Œåœ¨åŒ—äº¬åƒå±&quot;
res = ner_pipe(x)
ner_result = {}
for r in res:
    if r[&quot;entity_group&quot;] not in ner_result:
        ner_result[r[&quot;entity_group&quot;]] = []
    ner_result[r[&quot;entity_group&quot;]].append(x[r[&quot;start&quot;]:r[&quot;end&quot;]])
ner_result
</code></pre>
<h2 id="%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3">æœºå™¨é˜…è¯»ç†è§£</h2>
<p>Machine Reading Comprehension ç®€ç§°MRC, æ˜¯ä¸€ä¸ªè®©æœºå™¨å›ç­”ç»™å®šçš„ä¸Šä¸‹æ–‡çš„æ¥æµ‹è¯•æœºå™¨é˜…è¯»ç†è§£è‡ªç„¶è¯­è¨€çš„ç¨‹åº¦çš„ä»»åŠ¡</p>
<p>å®ƒçš„å½¢å¼æ˜¯æ¯”è¾ƒå¤šæ ·åŒ–çš„, å¸¸è§çš„æœ‰å®Œå½¢å¡«ç©º, ç­”æ¡ˆé€‰æ‹©, ç‰‡æ®µæŠ½å–, è‡ªç”±ç”Ÿæˆ</p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410121933591.png" alt="image-20241012193301378" /></p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410121935736.png" alt="image-20241012193555638" /></p>
<h3 id="%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86">æ•°æ®é¢„å¤„ç†</h3>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410121942295.png" alt="image-20241012194222195" /></p>
<h3 id="%E5%8E%9F%E7%90%86">åŸç†</h3>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410121949249.png" alt="image-20241012194951132" /></p>
<p><a href="https://zhuanlan.zhihu.com/p/368920094"  target="_blank">#å½»åº•ç†è§£# pytorch ä¸­çš„ squeeze() å’Œ unsqueeze()å‡½æ•° - çŸ¥ä¹ (zhihu.com)</a></p>
<blockquote>
<p>åœ¨PyTorchä¸­ï¼Œtorch.squeezeå‡½æ•°çš„å‚æ•°ä¸º-1æ—¶è¡¨ç¤ºç§»é™¤ç»´åº¦ä¸º1çš„ç»´åº¦ã€‚å…·ä½“æ¥è¯´ï¼Œå½“ä½¿ç”¨torch.squeeze(-1)æ—¶ï¼Œä¼šå°†å¼ é‡çš„æœ€åä¸€ä¸ªç»´åº¦ä¸º1çš„ç»´åº¦ç§»é™¤ï¼Œä½¿å¾—å¼ é‡çš„ç»´åº¦å‡å°‘1ã€‚</p>
<p>ä¾‹å¦‚ï¼Œå¦‚æœæœ‰ä¸€ä¸ªç»´åº¦ä¸ºï¼ˆ2ï¼Œ1ï¼Œ3ï¼‰çš„å¼ é‡ï¼Œä½¿ç”¨torch.squeeze(-1)åä¼šå¾—åˆ°ä¸€ä¸ªç»´åº¦ä¸ºï¼ˆ2ï¼Œ3ï¼‰çš„å¼ é‡ã€‚</p>
<p>æ€»çš„æ¥è¯´ï¼Œtorch.squeeze(-1)çš„ä½œç”¨å°±æ˜¯ç§»é™¤å¼ é‡ä¸­ç»´åº¦ä¸º1çš„ç»´åº¦ã€‚</p>
</blockquote>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410131039811.png" alt="image-20241013103939723" /></p>
<p>é™å¹…ã€‚å°†inputçš„å€¼é™åˆ¶åœ¨[min, max]ä¹‹é—´ï¼Œå¹¶è¿”å›ç»“æœã€‚out (<a href="https://so.csdn.net/so/search?q=Tensor&amp;spm=1001.2101.3001.7020"  target="_blank">Tensor</a>, optional) â€“ è¾“å‡ºå¼ é‡ï¼Œä¸€èˆ¬ç”¨ä¸åˆ°è¯¥å‚æ•°ã€‚</p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410131050644.png" alt="image-20241013105032554" /></p>
<h3 id="%E5%AE%9E%E9%99%85%E4%BD%BF%E7%94%A8">å®é™…ä½¿ç”¨</h3>
<ul>
<li>å¯¼åŒ…</li>
</ul>

<pre class="language-python"><code class="language-python">from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForQuestionAnswering, Trainer, TrainingArguments, DefaultDataCollator
</code></pre>
<ul>
<li>è·å–æ•°æ®</li>
</ul>

<pre class="language-python"><code class="language-python">dataset = load_dataset(&quot;cmrc2018&quot;, trust_remote_code=True)
</code></pre>
<ul>
<li>æ•°æ®å¤„ç†</li>
</ul>
<p>é¦–å…ˆä»answeré‡Œé¢è·å–æ•°æ®çš„èµ·å§‹ä½ç½®, ä¹‹åé€šè¿‡æ•°æ®çš„é•¿åº¦è®¡ç®—ç»“æŸçš„ä½ç½®, è¿™ä¸ªæ—¶å€™è·å–çš„æ•°æ®æ˜¯åœ¨å­—ç¬¦é‡Œé¢çš„ä½ç½®, éœ€è¦è¿›è¡Œè½¬æ¢è·å–ä»–åœ¨tokené‡Œé¢çš„æ•°æ®, å¯ä»¥ä½¿ç”¨offset_mapping(è®°å½•æ¯ä¸€ä¸ªtokençš„èµ·å§‹ä»¥åŠç»“æŸçš„charçš„ä½ç½®)è¿›è¡Œè½¬æ¢, æŠŠå¾—åˆ°çš„ç»“æœåˆ¤æ–­ä¸€ä¸‹æ˜¯ä¸æ˜¯åœ¨æˆªå–çš„æ•°æ®é‡Œé¢, å¦‚æœæ˜¯çš„è¯è·å–ä¸€ä¸‹å¯¹åº”çš„token, è®°å½•åœ¨è¿”å›å€¼é‡Œé¢</p>

<pre class="language-python"><code class="language-python">tokenizer = AutoTokenizer.from_pretrained(&quot;E:/JHY/python/2024-10-5-transforms/hlfrbt3&quot;)
</code></pre>
<blockquote>
<p>æµ‹è¯•:</p>

<pre class="language-python"><code class="language-python">sample_dataset = dataset[&quot;train&quot;].select([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
tokenized_example = tokenizer(text=sample_dataset[&quot;question&quot;], text_pair=sample_dataset[&quot;context&quot;]) # ä½¿ç”¨åˆ†è¯å™¨å¤„ç†, åŒæ—¶è¿›è¡Œåˆ†ç±»
print(tokenized_example[&quot;input_ids&quot;][0])
print(len(tokenized_example[&quot;input_ids&quot;][0]))

&quot;&quot;&quot;
[101, 5745, 2455, 7563,...]
767
&quot;&quot;&quot;

print(list(zip(tokenized_example[&quot;input_ids&quot;][0], tokenized_example[&quot;token_type_ids&quot;][0])))

&quot;&quot;&quot;
[(101, 0), (5745, 0), (2455, 0), (7563, 0), ...(5745, 1), (2455, 1), (7563, 1), ...]
&quot;&quot;&quot;
</code></pre>
<p>å¯ä»¥å§é—®é¢˜ä»¥åŠæ•°æ®åˆå¹¶åœ¨ä¸€èµ·</p>

<pre class="language-python"><code class="language-python"># åˆå¹¶é—®é¢˜å’Œæ–‡æœ¬ï¼Œæœ€å¤§é•¿åº¦384ï¼Œæˆªæ–­æ–‡æœ¬ï¼Œå¡«å……åˆ°æœ€å¤§é•¿åº¦ï¼Œè¿”å›offsets_mapping(ç”¨äºåç»­å¤„ç†ç­”æ¡ˆçš„ä½ç½®, å¯ä»¥å¯¹åº”Tokençš„å­—ç¬¦offsetä½ç½®)
tokenized_example = tokenizer(text=sample_dataset[&quot;question&quot;],
                              text_pair=sample_dataset[&quot;context&quot;], 
                              max_length=384, truncation=&quot;only_second&quot;,
                              padding=&quot;max_length&quot;, 
                              return_offsets_mapping=True) 
</code></pre>
</blockquote>
<ul>
<li>å®é™…è·å–ç›®æ ‡çš„ç»“æœçš„èµ·å§‹ä»¥åŠç»“æŸtoken</li>
</ul>

<pre class="language-python"><code class="language-python">for idx, offsets in enumerate(offset_mapping):
    # 'answers': {'text': ['1963å¹´'], 'answer_start': [30]}}
    answer = sample_dataset[idx][&quot;answers&quot;] # è·å–ç­”æ¡ˆ
    start_char = answer[&quot;answer_start&quot;][0] # çœŸå®ç­”æ¡ˆåœ¨å­—ç¬¦é‡Œé¢çš„ä½ç½®
    end_char = start_char + len(answer[&quot;text&quot;][0])
    print(answer, start_char, end_char)
    # ä¹‹åå®šä½ç­”æ¡ˆåœ¨tokenä¸­çš„ä½ç½®
    # è·å–contextçš„èµ·å§‹ç»“æŸ, ä¹‹åæ ¹æ®ç­”æ¡ˆçš„èµ·å§‹ç»“æŸä½ç½®ï¼Œæ‰¾åˆ°å¯¹åº”çš„tokenä½ç½®
    context_start = tokenized_example.sequence_ids(idx).index(1)
    context_end = tokenized_example.sequence_ids(idx).index(None, context_start) -1
    print(context_start, context_end)

    # è¯„æ–­æ–‡æœ¬çš„èµ·å§‹ä½ç½®ç»“æŸ(å­—ç¬¦)æ˜¯å¦åœ¨contextä¸­, ä½¿ç”¨offsetè¿›è¡Œtokenåˆ°charçš„è½¬æ¢
    if offsets[context_end][1] &lt;= start_char or offsets[context_start][0] &gt;= end_char:
        # print(&quot;ç­”æ¡ˆä¸åœ¨contextä¸­&quot;)
        start_token_pos = 0
        end_token_pos = 0
    else:
        token_id = context_start
        while token_id &lt;= context_end and offsets[token_id][0] &lt; start_char:
            token_id += 1
        start_token_pos = token_id
        token_id = context_end
        while token_id &gt;= context_start and offsets[token_id][1] &gt; end_char:
            token_id -= 1
        end_token_pos = token_id

    print(start_token_pos, end_token_pos)
    print(&quot;token answer decode: &quot;, tokenizer.decode(tokenized_example[&quot;input_ids&quot;][idx][start_token_pos:end_token_pos+1]))
</code></pre>
<blockquote>
<p>è¿›è¡Œå°è£…å¯ä»¥è·å¾—</p>
</blockquote>

<pre class="language-python"><code class="language-python">def process_func(examples):
    # åˆå¹¶é—®é¢˜å’Œæ–‡æœ¬ï¼Œæœ€å¤§é•¿åº¦384ï¼Œæˆªæ–­æ–‡æœ¬ï¼Œå¡«å……åˆ°æœ€å¤§é•¿åº¦ï¼Œè¿”å›offsets_mapping(ç”¨äºåç»­å¤„ç†ç­”æ¡ˆçš„ä½ç½®, å¯ä»¥å¯¹åº”Tokençš„å­—ç¬¦offsetä½ç½®)
    tokenized_examples = tokenizer(examples[&quot;question&quot;], examples[&quot;context&quot;], 
                                   max_length=512, truncation=&quot;only_second&quot;,
                                   padding=&quot;max_length&quot;, 
                                   return_offsets_mapping=True)

    # ä¿å­˜ç­”æ¡ˆçš„tokenä½ç½®
    offset_mapping = tokenized_examples.pop(&quot;offset_mapping&quot;)
    # 'answers': {'text': ['1963å¹´'], 'answer_start': [30]}}
    start_positions = []
    end_positions = []
    for idx, offsets in enumerate(offset_mapping):    
        answer = examples[&quot;answers&quot;][idx] # è·å–ç­”æ¡ˆ
        start_char = answer[&quot;answer_start&quot;][0] # çœŸå®ç­”æ¡ˆåœ¨å­—ç¬¦é‡Œé¢çš„ä½ç½®
        end_char = start_char + len(answer[&quot;text&quot;][0])
        # ä¹‹åå®šä½ç­”æ¡ˆåœ¨tokenä¸­çš„ä½ç½®
        # è·å–contextçš„èµ·å§‹ç»“æŸ, ä¹‹åæ ¹æ®ç­”æ¡ˆçš„èµ·å§‹ç»“æŸä½ç½®ï¼Œæ‰¾åˆ°å¯¹åº”çš„tokenä½ç½®
        context_start = tokenized_examples.sequence_ids(idx).index(1)
        context_end = tokenized_examples.sequence_ids(idx).index(None, context_start) -1

        # è¯„æ–­æ–‡æœ¬çš„èµ·å§‹ä½ç½®ç»“æŸ(å­—ç¬¦)æ˜¯å¦åœ¨contextä¸­, ä½¿ç”¨offsetè¿›è¡Œtokenåˆ°charçš„è½¬æ¢
        if offsets[context_end][1] &lt;= start_char or offsets[context_start][0] &gt;= end_char:
            # print(&quot;ç­”æ¡ˆä¸åœ¨contextä¸­&quot;)
            start_token_pos = 0
            end_token_pos = 0
        else:
            token_id = context_start
            while token_id &lt;= context_end and offsets[token_id][0] &lt; start_char:
                token_id += 1
            start_token_pos = token_id
            token_id = context_end
            while token_id &gt;= context_start and offsets[token_id][1] &gt; end_char:
                token_id -= 1
            end_token_pos = token_id
        start_positions.append(start_token_pos)
        end_positions.append(end_token_pos)
    # ä¿å­˜ç­”æ¡ˆçš„tokenä½ç½®
    tokenized_examples[&quot;start_positions&quot;] = start_positions
    tokenized_examples[&quot;end_positions&quot;] = end_positions
    return tokenized_examples

tokenized_dataset = dataset.map(process_func, batched=True, remove_columns=dataset[&quot;train&quot;].column_names)
</code></pre>
<ul>
<li>è®­ç»ƒ</li>
</ul>

<pre class="language-python"><code class="language-python">model = AutoModelForQuestionAnswering.from_pretrained(&quot;E:/JHY/python/2024-10-5-transforms/hlfrbt3&quot;)
args = TrainingArguments(
    output_dir=&quot;model_for_qa&quot;,
    per_device_eval_batch_size=32,
    per_device_train_batch_size=32,
    evaluation_strategy=&quot;epoch&quot;,
    save_strategy=&quot;epoch&quot;,
    load_best_model_at_end=True,
    logging_steps=50
)
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_dataset[&quot;train&quot;],
    eval_dataset=tokenized_dataset[&quot;validation&quot;],
    data_collator=DefaultDataCollator(),
    tokenizer=tokenizer
)
trainer.train()
</code></pre>
<ul>
<li>é¢„æµ‹</li>
</ul>

<pre class="language-python"><code class="language-python"># æ¨¡å‹é¢„æµ‹
from transformers import pipeline

pipe = pipeline(&quot;question-answering&quot;, model=model, tokenizer=tokenizer, device=0)
pipe({
    &quot;question&quot;: &quot;ä»€ä¹ˆæ—¶å€™æˆç«‹çš„&quot;,
    &quot;context&quot;: &quot;ä¸­åäººæ°‘å…±å’Œå›½æˆç«‹äº1949å¹´10æœˆ1æ—¥ã€‚&quot;
})

</code></pre>
<h3 id="%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3">æ»‘åŠ¨çª—å£</h3>
<p>ç”¨äºå¤„ç†æ•°æ®æ¯”è¾ƒé•¿çš„æ—¶å€™, å¦‚æœåªæ˜¯ç®€å•çš„æˆªæ–­, ä¼šå‡ºç°ç­”æ¡ˆè¢«æˆªæ–­çš„é—®é¢˜, æ‰€ä»¥åœ¨æˆªæ–­çš„æ—¶å€™ä¸€èˆ¬ä¼šæœ‰ä¸€éƒ¨åˆ†çš„é‡å , é‡å çš„é•¿çŸ­ä¼šä½¿å¾—ä½¿ç”¨è¿™ä¸€ç§çš„æ—¶å€™ä¼šå¯¼è‡´æ•°æ®çš„æ•°é‡å¢å¤§, é‡å éƒ¨åˆ†æ¯”è¾ƒå°çš„æ—¶å€™ä¼šå‡ºç°é•¿ä¸‹æ–‡ä¸¢å¤±ä»¥åŠç­”æ¡ˆä¸å®Œæ•´</p>
<p>æœ€åè·å–åˆ°å¤šä¸ªæ•°æ®çš„é¢„æµ‹ç»“æœè¿›è¡Œèšåˆ</p>
<h4 id="%E5%AE%9E%E9%99%85%E5%AE%9E%E7%8E%B0">å®é™…å®ç°</h4>
<p>ä½¿ç”¨ä¸€ä¸ªnltkåº“</p>
<blockquote>
<p>nltkåº“æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸæœ€ä¸ºçŸ¥åä¸”å¹¿æ³›ä½¿ç”¨çš„Pythonåº“ä¹‹ä¸€ï¼Œå…¶åŠŸèƒ½åŒ…æ‹¬æ–‡æœ¬åˆ†æã€æ ‡æ³¨ã€åˆ†è¯ã€å¥æ³•åˆ†æã€è¯­ä¹‰åˆ†æã€è¯­æ–™åº“ç®¡ç†ç­‰ã€‚nltkåº“è¿˜æä¾›äº†ä¸°å¯Œçš„è¯­è¨€å¤„ç†å·¥å…·å’Œèµ„æºï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·è¿›è¡Œæ–‡æœ¬æŒ–æ˜ã€ä¿¡æ¯æ£€ç´¢ã€æ–‡æœ¬åˆ†ç±»ã€è¯­è¨€æ¨¡å‹ç­‰ä»»åŠ¡ã€‚é€šè¿‡nltkåº“ï¼Œç”¨æˆ·å¯ä»¥è½»æ¾åœ°å¤„ç†æ–‡æœ¬æ•°æ®ï¼Œè¿›è¡Œæ–‡æœ¬åˆ†æå’ŒæŒ–æ˜ï¼Œä»è€Œå®ç°è‡ªç„¶è¯­è¨€å¤„ç†ç›¸å…³çš„å„ç§åº”ç”¨å’Œç ”ç©¶ã€‚</p>
</blockquote>

<pre class="language-bash"><code class="language-bash">pip install nltk
</code></pre>

<pre class="language-python"><code class="language-python">import nltk
nltk.download(&quot;punkt&quot;)
</code></pre>
<ul>
<li>tokenizerå¤„ç†å‡½æ•°æ”¹å˜</li>
</ul>

<pre class="language-python"><code class="language-python">sample_dataset = dataset[&quot;train&quot;].select([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
# åˆå¹¶é—®é¢˜å’Œæ–‡æœ¬ï¼Œæœ€å¤§é•¿åº¦384ï¼Œæˆªæ–­æ–‡æœ¬ï¼Œå¡«å……åˆ°æœ€å¤§é•¿åº¦ï¼Œè¿”å›offsets_mapping(ç”¨äºåç»­å¤„ç†ç­”æ¡ˆçš„ä½ç½®, å¯ä»¥å¯¹åº”Tokençš„å­—ç¬¦offsetä½ç½®)
tokenized_example = tokenizer(text=sample_dataset[&quot;question&quot;],
                              text_pair=sample_dataset[&quot;context&quot;], 
                              max_length=384,
                              truncation=&quot;only_second&quot;, 
                              padding=&quot;max_length&quot;, 
                              return_offsets_mapping=True,
                              return_overflowing_tokens=True) 
tokenized_example.keys()
</code></pre>
<blockquote>
<p>åŠ å…¥å‚æ•°return_overflowing_tokens, é»˜è®¤çš„æ—¶å€™æ˜¯æ²¡æœ‰è¿›è¡Œé‡å æ“ä½œçš„, å¯ä»¥ä½¿ç”¨strideå‚æ•°æŒ‡å®š</p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410151919615.png" alt="image-20241015191930969" /></p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410151922391.png" alt="image-20241015192230343" /></p>
</blockquote>
<ul>
<li>ä¸å¤„ç†å‡½æ•°, å¯¹æ•°æ®è¿›è¡Œå¤„ç†, è·å–éœ€è¦çš„æ•°æ®</li>
</ul>

<pre class="language-python"><code class="language-python">def process_func(examples):
    # åˆå¹¶é—®é¢˜å’Œæ–‡æœ¬ï¼Œæœ€å¤§é•¿åº¦384ï¼Œæˆªæ–­æ–‡æœ¬ï¼Œå¡«å……åˆ°æœ€å¤§é•¿åº¦ï¼Œè¿”å›
    # offsets_mapping(ç”¨äºåç»­å¤„ç†ç­”æ¡ˆçš„ä½ç½®, 
    # å¯ä»¥å¯¹åº”Tokençš„å­—ç¬¦offsetä½ç½®)
    tokenized_examples = tokenizer(examples[&quot;question&quot;],
                                   examples[&quot;context&quot;], 
                                   max_length=384, 
                                   truncation=&quot;only_second&quot;, 
                                   padding=&quot;max_length&quot;, 
                                   return_offsets_mapping=True,
                                   return_overflowing_tokens=True, 
                                   stride=128)
    sample_mapping = tokenized_examples.get(&quot;overflow_to_sample_mapping&quot;)
    # ä¿å­˜ç­”æ¡ˆçš„tokenä½ç½®
    # offset_mapping = tokenized_examples.pop(&quot;offset_mapping&quot;)
    # 'answers': {'text': ['1963å¹´'], 'answer_start': [30]}}
    start_positions = []
    end_positions = []
    example_ids = []
    for idx,_ in enumerate(sample_mapping):
        # è·å–ç­”æ¡ˆ, ä¸€ä¸ªç­”æ¡ˆå¯èƒ½å¯¹åº”å¤šä¸ªæ•°æ®
        answer = examples[&quot;answers&quot;][sample_mapping[idx]] 
        # çœŸå®ç­”æ¡ˆåœ¨å­—ç¬¦é‡Œé¢çš„ä½ç½®
        start_char = answer[&quot;answer_start&quot;][0] 
        end_char = start_char + len(answer[&quot;text&quot;][0])
        # ä¹‹åå®šä½ç­”æ¡ˆåœ¨tokenä¸­çš„ä½ç½®
        # è·å–contextçš„èµ·å§‹ç»“æŸ, ä¹‹åæ ¹æ®ç­”æ¡ˆçš„èµ·å§‹ç»“æŸä½ç½®
        # æ‰¾åˆ°å¯¹åº”çš„tokenä½ç½®, ä½¿ç”¨indexå‡½æ•°è·å–ç¬¬ä¸€ä¸ª1çš„ä½ç½®
        context_start = tokenized_examples.sequence_ids(idx).index(1)
        context_end = tokenized_examples.sequence_ids(idx).index(None, context_start) - 1
        offsets = tokenized_examples.get(&quot;offset_mapping&quot;)[idx]
        # è¯„æ–­æ–‡æœ¬çš„èµ·å§‹ä½ç½®ç»“æŸ(å­—ç¬¦)æ˜¯å¦åœ¨contextä¸­, ä½¿ç”¨offsetè¿›è¡Œtokenåˆ°charçš„è½¬æ¢
        if offsets[context_end][1] &lt;= start_char or offsets[context_start][0] &gt;= end_char:
            # print(&quot;ç­”æ¡ˆä¸åœ¨contextä¸­&quot;)
            start_token_pos = 0
            end_token_pos = 0
        else:
            token_id = context_start
            while token_id &lt;= context_end and offsets[token_id][0] &lt; start_char:
                # ä½¿ç”¨éå†çš„æ–¹æ³•è·å–ä¸€ä¸‹æ•°æ®çš„èµ·å§‹ä½ç½®
                token_id += 1
            start_token_pos = token_id
            token_id = context_end
            while token_id &gt;= context_start and offsets[token_id][1] &gt; end_char:
                # åå‘éå†è·å–ç»“æŸä½ç½®
                token_id -= 1
            end_token_pos = token_id
        start_positions.append(start_token_pos)
        end_positions.append(end_token_pos)
        # è®°å½•æ¯ä¸€ä¸ªæ•°æ®çš„idç”¨äºå¯¹åº”
        example_ids.append(examples[&quot;id&quot;][sample_mapping[idx]])
        tokenized_examples[&quot;offset_mapping&quot;][idx] = [
            # è®°å½•ä¸€ä¸‹æœ‰æ•ˆæ•°æ®çš„tokenå¯¹åº”çš„offset(éé—®é¢˜æ•°æ®çš„ä½ç½®)
            (o if tokenized_examples.sequence_ids(idx)[k] == 1 else None)
            for k, o in enumerate(tokenized_examples[&quot;offset_mapping&quot;][idx])
        ]

    tokenized_examples[&quot;example_ids&quot;] = example_ids
    # ä¿å­˜ç­”æ¡ˆçš„tokenä½ç½®
    tokenized_examples[&quot;start_positions&quot;] = start_positions
    tokenized_examples[&quot;end_positions&quot;] = end_positions
    return tokenized_examples
</code></pre>

<pre class="language-python"><code class="language-python">tokenized_dataset = dataset.map(process_func, batched=True, remove_columns=dataset[&quot;train&quot;].column_names)
</code></pre>
<ul>
<li>è·å–æ•°æ®çš„é¢„æµ‹ä»¥åŠçœŸå®çš„æ•°æ®</li>
</ul>

<pre class="language-python"><code class="language-python">import numpy as np
import collections

def get_result(start_logits, end_logits, examples, features):
    &quot;&quot;&quot;_summary_

    Args:
        start_logits (_type_): æ¨¡å‹é¢„æµ‹çš„ç»“æœèµ·å§‹ä½ç½®
        end_logits (_type_): ç»“æŸä½ç½®çš„é¢„æµ‹ç»“æœ
        examples (_type_): åŸå§‹çš„æ•°æ®é›†
        features (_type_): tokenizerè·å–åˆ°çš„mapping
    &quot;&quot;&quot;
    predictions = {}
    references = {}

    example_to_features = collections.defaultdict(list) # ä¿å­˜æ¯ä¸€ä¸ªexampleå¯¹åº”çš„featureç¼–å·
    for idx, example_id in enumerate(features[&quot;example_ids&quot;]):
        example_to_features[example_id].append(idx) # è®°å½•ä¸€ä¸‹æ¯ä¸€ä¸ªexampleå¯¹åº”çš„è¢«åˆ†å‰²ä»¥åçš„ç¼–å·

    # æœ€ä¼˜ç­”æ¡ˆå€™é€‰æ•°
    n_best = 20
    max_answer_length = 30
    for example in examples:
        example_id = example[&quot;id&quot;]
        context = example[&quot;context&quot;]
        answers = []
        for feature_idx in example_to_features[example_id]:
             # è·å–å¯¹åº”çš„featureçš„é¢„æµ‹ç»“æœ, è¿™ä¸ªç»“æœæ“æ˜¯ä¸€ä¸ªæ•°ç»„
            start_logit = start_logits[feature_idx]
            end_logit = end_logits[feature_idx]
            # è·å–å¯¹åº”çš„offset_mapping
            offset = features[feature_idx][&quot;offset_mapping&quot;] 
            # ä»å¤§åˆ°å°æ’åºï¼Œå–å‰n_best
            start_indexs = np.argsort(start_logit)[::-1][:n_best].tolist() 
            # ä»å¤§åˆ°å°æ’åºï¼Œå–å‰n_best
            end_indexs = np.argsort(end_logit)[::-1][:n_best].tolist() 
            for start_index in start_indexs:
                for end_index in end_indexs:
                    # å¦‚æœé¢„æµ‹çš„ä½ç½®ä¸åœ¨offsetä¸­ï¼Œæˆ–è€…ç»“æŸä½ç½®åœ¨å¼€å§‹ä½ç½®ä¹‹å‰ï¼Œæˆ–è€…é•¿åº¦è¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œéƒ½ä¸è¦
                    if offset[start_index] is None or offset[end_index] is None:
                        continue
                    if start_index &gt; end_index or end_index - start_index + 1 &gt; max_answer_length:
                        continue
                    answers.append({
                        &quot;score&quot;: start_logit[start_index] + end_logit[end_index],
                        &quot;text&quot;: context[offset[start_index][0]:offset[end_index][1]]
                    })
        if len(answers) &gt; 0:
            # è·å–è¯„åˆ†æœ€é«˜çš„é¢„æµ‹ç»“æœ
            best_answer = max(answers, key=lambda x: x[&quot;score&quot;])
            predictions[example_id] = best_answer[&quot;text&quot;]
        else:
            predictions[example_id] = &quot;&quot;
        references[example_id] = example[&quot;answers&quot;][&quot;text&quot;]

    return predictions, references
</code></pre>
<ul>
<li>å®é™…çš„é¢„æµ‹å‡½æ•°</li>
</ul>

<pre class="language-python"><code class="language-python">from cmrc_eval import evaluate_cmrc
def metric(pred):
    start_logits, end_logits = pred[0]
    if start_logits.shape[0] == len(tokenized_dataset[&quot;validation&quot;]):
        p, r = get_result(start_logits, end_logits, dataset[&quot;validation&quot;], tokenized_dataset[&quot;validation&quot;])
    else:
        p, r = get_result(start_logits, end_logits, dataset[&quot;test&quot;], tokenized_dataset[&quot;test&quot;])
    return evaluate_cmrc(p, r)
</code></pre>
<h2 id="%E5%A4%9A%E9%A1%B9%E9%80%89%E6%8B%A9">å¤šé¡¹é€‰æ‹©</h2>
<p>æœºå™¨é˜…è¯»ç†è§£é‡Œé¢çš„ä¸€ä¸ªåˆ†æ”¯, ç»™å®šä¸€ä¸ªæ–‡æ¡£, ä¸€ä¸ªé—®é¢˜ä»¥åŠå¤šä¸ªç­”æ¡ˆ, ä»é‡Œé¢è·å–æ­£ç¡®çš„ç­”æ¡ˆ</p>
<h3 id="%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86">æ•°æ®å¤„ç†</h3>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410222302906.png" alt="image-20241022230218638" /></p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410222303371.png" alt="image-20241022230335193" /></p>
<blockquote>
<p>åœ¨å®é™…å¤„ç†çš„æ—¶å€™éœ€è¦æŠŠæ•°æ®è¿›è¡Œä¸€ä¸ªèšåˆ</p>
</blockquote>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410222305087.png" alt="image-20241022230548880" /></p>
<blockquote>
<p>è¿™é‡Œä½¿ç”¨çš„viewå‡½æ•°ä¼šæŒ‰ç…§æœ€é‡Œé¢ä¸€å±‚çš„å¤§å°å±•å¼€ä¸ºäºŒç»´æ•°ç»„</p>

<pre class="language-python"><code class="language-python">import torch

tensor1 = torch.tensor([[[1, 2], [2, 3], [3, 4]], [[5, 6], [6, 7], [7, 8]]])

print(tensor1.size(-1))
tensor1 = tensor1.view(-1, tensor1.size(-1))
print(tensor1.size())
&quot;&quot;&quot;
2
torch.Size([6, 2])
&quot;&quot;&quot;
</code></pre>
</blockquote>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410222326874.png" alt="image-20241022232644703" /></p>
<h3 id="%E5%AE%9E%E9%99%85%E8%AE%AD%E7%BB%83">å®é™…è®­ç»ƒ</h3>
<p>è¿™é‡Œä½¿ç”¨çš„æ•°æ®é›†clueä¸‹é¢çš„C3æ•°æ®é›†</p>
<p><a href="https://huggingface.co/datasets/clue/clue"  target="_blank">https://huggingface.co/datasets/clue/clue</a></p>
<p><img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202410242259476.png" alt="image-20241024225949337" /></p>
<blockquote>
<p>è¿™é‡Œçš„contextå¯èƒ½æ˜¯ä¸€ä¸ªå¯¹è¯, æ˜¯å¯¹è¯çš„æ—¶å€™è¿™ä¸€ä¸ªçš„æ•°æ®æ˜¯ä¸€ä¸ªåˆ—è¡¨,è¿™é‡Œé¢çš„æ•°æ®ç”±äºtestæ•°æ®é›†æ²¡æœ‰answer, æ‰€ä»¥éœ€è¦å»é™¤</p>
</blockquote>

                        
                    </div>
                </div>
                <div id="previous_next">
                    <div id="previous">
                        
                        <a href="/note/æœºå™¨å­¦ä¹ /2024-10-7-Pandasåº“.html">
                            <span class="icon"></span>
                            <span class="label">2024-10-7-Pandasåº“</span>
                        </a>
                        
                    </div>
                    <div id="next">
                        
                        <a href="/note/æœºå™¨å­¦ä¹ /2024-9-21-LLM.html">
                            <span class="label">2024-9-21-LLM</span>
                            <span class="icon"></span>
                        </a>
                        
                    </div>
                </div>
                <div id="comments-container"></div>
            </div>
            <div id="toc_wrapper">
                <div id="toc">
                    <div id="toc_content">
                            
                    </div>
                </div>
            </div>
        </div>
    </div>
    <a id="to_top" href="#"></a>
    <div id="doc_footer">
        <div id="footer">
            <div id="footer_top">
                <ul>
<li><a>é“¾æ¥</a><ul><li><a target="_blank" href="https://teedoc.neucrack.com">ç½‘ç«™ä½¿ç”¨ teedoc ç”Ÿæˆ</a></li>
<li><a target="_blank" href="https://neucrack.com">Copyright Â© 2021 Neucrack</a></li>
<li><a  href="/note/sitemap.xml">ç½‘ç«™åœ°å›¾</a></li>
</ul>
</li>
<li><a>æºç </a><ul><li><a target="_blank" href="https://github.com/XuSenfeng/note/">github</a></li>
<li><a target="_blank" href="https://github.com/teedoc/teedoc">æœ¬ç½‘ç«™æºæ–‡ä»¶</a></li>
</ul>
</li>
</ul>

            </div>
            <div id="footer_bottom">
                <ul>
<li><a target="_blank" href="https://beian.miit.gov.cn">æ¸ICPå¤‡19015320å·</a></li>
<li><a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=44030602004109">ç²¤å…¬ç½‘å®‰å¤‡44030602004109å·</a></li>
</ul>

            </div>
        </div>
    </div>
    
        <script src="/note/teedoc-plugin-markdown-parser/mermaid.min.js"></script>
    
        <script>mermaid.initialize({startOnLoad:true});</script>
    
        <script type="text/javascript">
                var transLoaded = false;
                var loading = false;
                var domain = "translate.google.com";
                var domainDefault = domain;
                var storeDomain = localStorage.getItem("googleTransDomain");
                if(storeDomain){
                    domain = storeDomain;
                    console.log("load google translate domain from local storage:" + domain);
                }
                function getUrl(domain){
                    if(domain == "/")
                        return "/static/js/google_translate/element.js?cb=googleTranslateElementInit";
                    else
                        return "https://" + domain + "/translate_a/element.js?cb=googleTranslateElementInit";
                }
                var url = getUrl(domain);
                console.log("google translate domain:" + domain + ", url: " + url);
                function googleTranslateElementInit() {
                    new google.translate.TranslateElement({pageLanguage: "auto", layout: google.translate.TranslateElement.InlineLayout.SIMPLE}, 'google_translate_element');
                }
                function loadJS( url, callback ){
                    var script = document.createElement('script');
                    fn = callback || function(){ };
                    script.type = 'text/javascript';
                    if(script.readyState){
                        script.onreadystatechange = function(){
                            if( script.readyState == 'loaded' || script.readyState == 'complete' ){
                                script.onreadystatechange = null;
                                fn();
                            }
                        };
                    }else{
                        script.onload = function(){
                            fn();
                        };
                    }
                    script.src = url;
                    document.getElementsByTagName('head')[0].appendChild(script);
                }
                function removeHint(){
                    var hint = document.getElementById("loadingTranslate");
                    if(hint){
                        hint.remove();
                    }
                }
                var btn = document.getElementById("google_translate_element");
                btn.onclick = function(){
                    if(transLoaded) return;
                    if(loading){
                        var flag = confirm("loading from " + domain + ", please wait, or change domain?");
                        if(flag){
                            newDomain = prompt("domain, default: " + domainDefault + ", now: " + domain);
                            if(newDomain){
                                domain = newDomain;
                                console.log(domain);
                                url = getUrl(domain);
                                loadJS(url, function(){
                                    localStorage.setItem("googleTransDomain", domain);
                                    removeHint()
                                    transLoaded = true;
                                });
                            }
                        }
                        return;
                    }
                    btn.innerHTML = '<span id="loadingTranslate"><img class="icon" src="/note/static/image/google_translate/translate.svg"/>Loading ...</span>';
                    loading = true;
                    loadJS(url, function(){
                        localStorage.setItem("googleTransDomain", domain);
                        removeHint()
                        transLoaded = true;
                    });
                }
                </script>
            
    
        <script src="/note/static/js/theme_default/tocbot.min.js"></script>
    
        <script src="/note/static/js/theme_default/main.js"></script>
    
        <script src="/note/static/js/theme_default/viewer.min.js"></script>
    
        <script src="/note/static/css/theme_default/prism.min.js"></script>
    
        <script src="/note/static/js/search/search_main.js"></script>
    
        <script src="/note/static/js/plugin_blog/main.js"></script>
    
        <link rel="stylesheet" href="/note/static/js/add_hint/style.css" type="text/css"/>
    
        <script src="/note/static/js/add_hint/main.js"></script>
    
        <script src="/note/static/js/gitalk/gitalk.min.js"></script>
    
        <script src="/note/static/js/gitalk/main.js"></script>
    
        <script src="/note/static/js/custom.js"></script>
    
</body>

</html>