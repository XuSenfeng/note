## 基础概念

深度学习模型工作流程大致可分为训练和推理，而在板块上部署模型，具体的步骤如下：

-   **模型训练**，模型训练前需要根据具体项目问题，选择模型，数据采集，然后使用适合的深度学习框架进行模型训练， 其中关于RKNN模型算子的支持请参考 [RKNN_Compiler_Support_Operator_List](https://github.com/airockchip/rknn-toolkit2/tree/master/doc) 。
-   **模型转换**，将训练的深度学习模型会被转化为RKNN格式的模型。
-   **模型评估**，将使用RKNN-Toolkit2工具量化和分析模型性能，包括精度、连板推理性能和内存占用等关键指标， 根据模型的评估尝试修改和优化模型，一些模型的优化可以参考下 [RKNPU_User_Guide_RKNN_SDK](https://github.com/airockchip/rknn-toolkit2/tree/master/doc) 。
-   **板端推理**，将转换的RKNN模型部署到板卡上,具体可以查看下rknpu运行库和 [RKNN-Toolkit-lite2](https://github.com/airockchip/rknn-toolkit2/tree/master/rknn-toolkit-lite2) 的使用。

>   需要使用RKLLM工具把模型转换为RKLLM模型

## RKNN Toolkit2

-   **模型转换**，Toolkit-lite2工具导入原始的Caffe、TensorFlow、TensorFlow Lite、ONNX、Pytorch、MXNet等模型转换成RKNN模型()， 也支持导入RKNN模型然后在NPU平台 上加载推理等。
-   **量化功能**，支持将浮点模型量化为定点模型，目前支持的量化方法为非对称量化（asymmetric_quantized-8），并支持混合量化功能。
-   **模型推理**，能够在PC上模拟NPU运行RKNN模型并获取推理结果；或将RKNN模型分发到指定的NPU设备上进行推理并获取推理结果。
-   **性能和内存评估**，连接板卡，将RKNN模型分发到指定NPU设备上运行，然后评估模型在实际设备上运行时的性能和内存占用情况。
-   **量化精度分析**，该功能将给出模型量化后每一层推理结果与浮点模型推理结果的余弦距离，以分析量化误差是如何出现的，为提高量化模型的精度提供思路。
-   **模型加密功能**，使用指定的加密等级将RKNN模型整体加密，因为RKNN模型的解密是在NPU驱动中完成的，所以使用加密模型时，与普通RKNN模型一样加载即可，NPU驱动会自动对其进行解密

使用Toolkit-lite2，可以运行在PC上，通过模拟器运行模型，然后进行推理，或者模型转换等操作；也可以运行在连接的板卡NPU上， 将RKNN模型传到NPU设备上运行，再从NPU设备上获取推理结果、性能信息等等。 

![image-20251202192600839](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/mac-picture/image-20251202192600839.png)

## 模型下载

1.   使用huggingface网页进行下载
2.   使用huggingface工具里面在线下载

```bash
pip3 install huggingface-cli -i https://mirrors.huaweicloud.com/repository/pypi/simple
```

可以配置加速节点，下载deepseek的huggingface格式模型（有时候也会失效，建议直接官网直接点击下载键来下载模型会更快）执行如下命令：

`export HF_ENDPOINT=https://hf-mirror.com`

执行以下命令下载DeepSeek-R1-Distill-Qwen-1.5B模型

`huggingface-cli download deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B --local-dir . --local-dir-use-symlinks`

## 工具RKNN-Toolkit2

| 特性维度     | **RKNN-Toolkit2**                                            | **RKNN-Toolkit-Lite2**                                       |
| :----------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
| **核心定位** | **模型转换与评估工具**                                       | **模型部署与推理工具**                                       |
| **主要用途** | 在**PC**上将各种AI模型转换为Rockchip NPU专用的RKNN格式，并进行优化、评估和仿真。 | 在**嵌入式开发板**上加载和运行由RKNN-Toolkit2生成的RKNN模型，执行AI推理任务。 |
| **关键功能** | 模型转换、量化、性能/内存评估、精度分析、模型加密、**PC端仿真推理**。 | **仅包含推理功能**，如加载模型、输入数据处理、执行推理、获取结果。 |
| **运行平台** | **x86架构的PC**（如Ubuntu系统）。                            | **ARM架构的Rockchip开发板**（如RK3588, RK3568等）。          |
| **API特性**  | 提供完整的Python API，用于模型构建、配置和转换流程。         | 提供轻量化的Python API，专注于推理接口，依赖库更少，更适合资源受限的环境。 |
| **依赖关系** | 是生成RKNN模型的必需工具。                                   | **必须依赖**由RKNN-Toolkit2生成的RKNN模型文件才能工作。      |

## RKLLM

### RKLLM

**定义**： RKLLM 是 Rockchip 专门为在端侧设备（如开发板、边缘计算盒子）上部署和运行**大语言模型**而推出的一套工具链。

**核心功能**：

- **大模型量化与压缩**： 专门针对参数量巨大的 LLM（如 LLaMA、ChatGLM、Qwen 等）进行极致的低比特量化（如 INT4，甚至更低），以使其能够塞入有限的设备内存中。
- **运行时与优化引擎**： 提供了一个高度优化的推理引擎（RKLLM Runtime），专注于处理 LLM 特有的计算模式（如自回归生成、注意力机制），并针对 Rockchip 芯片的 CPU、NPU 进行深度优化，以提升生成速度（降低 `time-to-first-token` 和 `tokens-per-second`）。
- **专属工具链**： 提供了 `rkllm-convert` 等专用工具，将 Hugging Face 格式的 LLM 转换为可在设备上运行的格式。

**主要应用场景**：

- 端侧大语言模型应用： 智能座舱对话、AIoT设备语音助手、本地化文档处理、离线翻译等。

**特点**：

- **领域专精**： 为解决“大模型”与“小设备”之间的矛盾而生。
- **极致优化**： 追求在有限硬件资源下实现 LLM 的可运行和可用性。
- **新兴技术**： 随着生成式 AI 浪潮兴起，是 RK 工具链的重要补充。

| 特性             | RKNN                                        | RKLLM                                                        |
| :--------------- | :------------------------------------------ | :----------------------------------------------------------- |
| **定位**         | **通用 NPU 推理工具链**                     | **大语言模型专用部署工具链**                                 |
| **核心目标**     | 将广泛的深度学习模型高效部署到瑞芯微 NPU    | 将参数量巨大的 LLM 压缩并优化到端侧设备运行                  |
| **主要输入模型** | TensorFlow, PyTorch, ONNX, Caffe 等标准模型 | Hugging Face 格式的 Transformer 架构 LLM（如 LLaMA, Qwen, ChatGLM） |
| **关键技术**     | 模型转换、通用算子优化、异构调度            | **极低比特量化**（INT4/W4A4）、KV Cache 优化、注意力计算优化 |
| **输出格式**     | `.rknn` 文件                                | 经过特定格式封装和量化的模型文件                             |
| **运行时**       | RKNN Runtime Library                        | RKLLM Runtime Library                                        |
| **典型应用**     | 人脸识别、目标检测、工业质检                | 端侧智能对话、本地知识库、AI Agent                           |