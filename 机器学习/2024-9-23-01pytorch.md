---
 layout: post
title: "pytorch" 
date:   2024-8-5 15:39:08 +0800
tags: AI 机器学习
---

# pytorch

## 基础入门

可以使用dir和help两个函数进行探索使用pytorch

使用dir可以对一个包进行打开操作, 使用help可以获取某一个包的具体使用

![image-20240923172733947](C:\Users\jinhua\AppData\Roaming\Typora\typora-user-images\image-20240923172733947.png)

### 名词

#### 张量

[笔记 | 什么是张量（tensor）& 深度学习 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/48982978#:~:text=在深度学习里，Ten)

在深度学习里，**Tensor实际上就是一个多维数组（multidimensional array）**。

而Tensor的目的是**能够创造更高维度的矩阵、向量**。

![img](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409252313363.jpeg)

举个简单的例子，[彩色图像文件](https://zhida.zhihu.com/search?content_id=9946792&content_type=Article&match_order=1&q=彩色图像文件&zhida_source=entity)（RGB）一般都会处理成3-d tensor，每个2d array中的element表示一个像素，R代表Red，G代表Green，B代表Blue：

![img](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409252314013.webp)

再来看看Tensor对象的3个属性：

1. **rank**：[number of dimensions](https://zhida.zhihu.com/search?content_id=9946792&content_type=Article&match_order=1&q=number+of+dimensions&zhida_source=entity)
2. **shape**: number of rows and columns
3. **type**: data type of tensor's elements

#### 卷积

 https://b23.tv/T4gTHYC

卷积, 实际可以理解为两个函数的相互作用

数学上，其连续函数的解析式写作：

$ F(x) = \int_{-\infty}^{+\infty}f(\tau)\,g(x -\tau)$

$ F(x) = \sum_{\tau=1}^N f(\tau)\,g(x -\tau)$

![在这里插入图片描述](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409252256263.gif)

通常情况下，f ( τ ) 表示被积函数，而 g ( x − τ )  表示卷积核函数。这里多说一句，之所以不使用 f ( x ) 表示原函数而用 f ( τ ) ，而且强调 f ( τ ) 是被积函数，是因为 f ( x )  与 f ( τ ) 之间还存在着如下关系：
$ f(x) = \int_{-\infty}^{+\infty}f(\tau)d\tau$

上式表明的操作在直观上理解是先对卷积核翻转，然后与输入点乘、求和得到输出。在机器学习领域尤其是深度学习中，卷积的实现通常省去了卷积核翻转这一步，因为深度学习中的卷积核参数是不断学习更新，因此有没有翻转并没有性质上的影响。严格定义上说，深度学习中的卷积实际上是另一种操作：[互相关](https://zhida.zhihu.com/search?content_id=105153470&content_type=Article&match_order=1&q=互相关&zhida_source=entity) [Cross-Correlation](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Cross-correlation)。公式表示如下

![image-20240925233420506](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409252334611.png)

![动图](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409252334905.webp)

![image-20240925233512203](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409252335303.png)

不考虑 padding，以 stride 为 1，那么我们进行卷积计算得到的结果为

![image-20240925233556003](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409252335091.png)

#### 卷积的三种模式

深度学习框架中通常会实现三种不同的卷积模式，分别是 SAME、VALID、FULL。这三种模式的核心区别在于**卷积核进行卷积操作的移动区域不同**，进而导致输出的尺寸不同。我们以一个例子来看这三种模式的区别，输入图片的尺寸是 5×5 ，卷积核尺寸是 3×3 ，stride 取 1。

- FULL 模式

FULL 模式下卷积核**从与输入有一个点的相交的地方就开始卷积**。如下图所示，蓝框的位置就是卷积核第一个卷积的地方，灰色部分是为了卷积能够正常进行的 padding（一般填 0）。因此 FULL 模式下卷积核移动区域最大，卷积后输出的尺寸也最大。

![img](https://pica.zhimg.com/80/v2-23ba5f401533b72b0214bd51a091000c_720w.webp)

- VALID 模式

VALID 模式与 FULL 模式相反，**在整个卷积核与输入重叠的地方才开始卷积操作**，因此不需要 padding，输出的尺寸也最小

![img](https://pic4.zhimg.com/80/v2-fc57effd13fdf64eeb375f57e65e309d_720w.webp)

- SAME 模式

SAME 模式是最常用的一种模式，SAME 的意思是卷积后输出的尺寸与输入尺寸保持一致（假定 stride 为 1）。通过将卷积核的中心与输入的第一个点进行对齐确定卷积核起始位置，然后补齐对应 padding 即可。如下图所示，可以看到卷积输出的尺寸与出入保持一致。

![img](https://pic4.zhimg.com/80/v2-a18f53d4f4d60a0eb6d1940d06bd5af5_720w.webp)

SAME 模式下当卷积核边长为偶数时，可以通过在其中一边增加多一行（列）padding，即不对称的 padding 实现输出尺寸与输入尺寸保持一致，如下图所示（卷积核尺寸为 2×2 ）

![img](https://pica.zhimg.com/80/v2-0ace23e8761226979fbe7ecd0a1905c8_720w.webp)

以上三种模式区别在于卷积核进行卷积操作的移动区域不同，其实是确定了所需的 padding。

#### 池化层Pooling layers

它实际上是一种形式的降采样。有多种不同形式的非线性池化函数，而其中“最大池化（Max pooling）”是最为常见的。它是将输入的图像划分为若干个矩形区域，对每个子区域输出最大值。直觉上，这种机制能够有效地原因在于，在发现一个特征之后，它的精确位置远不及它和其他特征的相对位置的关系重要。池化层会不断地减小数据的空间大小，因此参数的数量和计算量也会下降，这在一定程度上也控制了过拟合。通常来说，CNN的卷积层之间都会周期性地插入池化层。
![img](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409262220821.png)

#### 非线性激活

**ReLu**，全称是Rectified Linear Unit，中文名称是线性整流函数，是在神经网络中常用的激活函数。通常意义下，其指代数学中的斜坡函数，即F(X) = max(0, x) 。其对应的函数图像如下所示：

![image-20240926225959744](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409262259886.png)

- **Sigmoid，**是常用的连续、平滑的s型激活函数，也被称为逻辑（Logistic）函数。可以将一个实数映射到 ![(0, 1)](https://www.zhihu.com/equation?tex=%280%2C+1%29&consumer=ZHI_MENG) 的区间，用来做二分类。其函数定义为  $f(x) = \frac{1}{1 + e^{-x}}$，函数图像如下所示：

![img](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409262300639.webp)

- **Tanh函数**称为双曲正切函数，函数定义为 $f(x) = tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$，值域为 (-1,1)(0, 1) ，函数图像如下：

![img](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409262300689.webp)

引入非线性激活函数的目的是提高神经网络的非线性拟合能力，增强模型的表达能力。因此，在表诉过程中，在没有明确指明的情况下，激活函数指代非线性激活函数。经过严格的数学推导，如果网络中没有使用激活函数，每一层的节点的输入都是上层输出的线性函数，无论神经网络中的隐含层有多少，最后的输出结果都是网络输入的线性拟合，即隐含层没有发挥其作用。为此，引入非线性函数作为激活函数来提高模型的表达能力。

举个栗子：如果不使用非线性函数作为激活函数，对于二分类问题，可以使用逻辑回归做简单的线性划分，如下图所示：

![img](https://pic1.zhimg.com/v2-7a857d55584ce83c1b877ae96383c0c6_b.webp?consumer=ZHI_MENG)

使用简单的线性划分无法有效划分数据类别，对此需要引入非线性，为数据集划分提高有效的方式。如下图所示：

![img](https://picx.zhimg.com/v2-e042df59cf85ccb712141d152d958d8d_b.webp?consumer=ZHI_MENG)

由此可见，引入激活函数能够正确有效划分数据类别。引入非线性因素，使得神经网络能够更好地解决复杂问题。

#### Normalization Layer

[一文搞懂Batch Normalization 和 Layer Normalization - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/647813604)

Normalization：规范化或标准化，就是把输入数据X，在输送给神经元之前先对其进行平移和伸缩变换，将X的分布规范化成在固定区间范围的[标准分布](https://zhida.zhihu.com/search?content_id=232088974&content_type=Article&match_order=1&q=标准分布&zhida_source=entity)。

![image-20240927185243068](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409271852225.png)

μ：[平移参数](https://zhida.zhihu.com/search?content_id=232088974&content_type=Article&match_order=1&q=平移参数&zhida_source=entity) ，δ：缩放参数 ，b ：再平移参数， g 再缩放参数，得到的数据符合均值为 b 、方差为g^2 的分布。

Normalization 的作用很明显，把数据拉回[标准正态分布](https://zhida.zhihu.com/search?content_id=232088974&content_type=Article&match_order=1&q=标准正态分布&zhida_source=entity)，因为[神经网络](https://zhida.zhihu.com/search?content_id=232088974&content_type=Article&match_order=1&q=神经网络&zhida_source=entity)的Block大部分都是[矩阵运算](https://zhida.zhihu.com/search?content_id=232088974&content_type=Article&match_order=1&q=矩阵运算&zhida_source=entity)，一个向量经过矩阵运算后值会越来越大，为了网络的稳定性，我们需要及时把值拉回正态分布。

Normalization根据标准化操作的维度不同可以分为batch Normalization和Layer Normalization，不管在哪个维度上做noramlization，本质都是为了让数据在这个维度上归一化，因为在训练过程中，上一层传递下去的值千奇百怪，什么样子的分布都有。BatchNorm就是通过对[batch size](https://zhida.zhihu.com/search?content_id=232088974&content_type=Article&match_order=1&q=batch+size&zhida_source=entity)这个维度归一化来让分布稳定下来。LayerNorm则是通过对[Hidden size](https://zhida.zhihu.com/search?content_id=232088974&content_type=Article&match_order=1&q=Hidden+size&zhida_source=entity)这个维度归一化来让某层的分布稳定。

可以这样理解，深度网络每一层网络是相对独立的，也就是说每一层网络可以单独看成一个Classifier.不停对上一层的输出数据进行分类，每一层输出的数据分布又不一样，这就会出现Internal Covariate Shift（[内部协变量](https://zhida.zhihu.com/search?content_id=232088974&content_type=Article&match_order=1&q=内部协变量&zhida_source=entity)偏移）. 随着网络的层数不断增大，这种误差就会不断积累，最终导致效果欠佳。显然对[数据预处理](https://zhida.zhihu.com/search?content_id=232088974&content_type=Article&match_order=1&q=数据预处理&zhida_source=entity)只能解决第一层的问题，之后需要Normalization等方法来解决。

#### Recuurent Layers循环层

Recurrent Layers的主要作用是在神经网络中提供记忆功能，使得网络能够处理时间序列数据或具有时间相关性的数据。通过在网络中引入循环连接，Recurrent Layers可以将先前的输出作为输入传递给下一个时间步，从而允许网络在处理序列数据时考虑到之前的信息。这种记忆机制能够帮助网络更好地理解和预测序列数据中的模式和趋势，同时也有助于解决短期记忆和梯度消失等问题。因此，Recurrent Layers在语言模型、机器翻译、时间序列预测等任务中具有重要作用。

#### Transform Layer

解决特定问题的时候使用

#### Linear Layer

线性层, 对输入的每一个数据乘以一个权重, 之后相加

![image-20240927191723938](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409271917063.png)

#### Dropout Layers

随机吧一部分的元素变为0, 防止数据过拟合, 防止在实际处理数的时候泛用性不强

#### Sparse Layer

用于自然语言的处理

#### 损失函数和反向传播

Loss: 实际获取的数据和真实的数据之间的差距, 可以使用这一个计算实际输出和目标的差距, 也可以为更新输出提供依据(反向传播)

### 加载数据

读取数据的时候主要有两个类, dataset, dataloader

dataset: 对数据进行分类并进行编号, 获取数据的lable值

dataloader: 对数据进行打包, 为后面的网络提供不同的数据形式

**数据集:** 一般使用一下的标识方法

1. 把不同的数据按照文件夹分类, 文件夹的名字标识这一个数据集的type
2. 使用单独的文件标识图片的lable
3. 之间把lable写在图片的名字

```c
from torch.utils.data import Dataset
help(Dataset)
```

> ```bash
> Help on class Dataset in module torch.utils.data.dataset:
> 
> class Dataset(typing.Generic)
>  |  An abstract class representing a :class:`Dataset`.
>  |
>  |  All datasets that represent a map from keys to data samples should subclass
>  |  it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a
>  |  data sample for a given key. Subclasses could also optionally overwrite
>  |  :meth:`__len__`, which is expected to return the size of the dataset by many
>  |  :class:`~torch.utils.data.Sampler` implementations and the default options
>  |  of :class:`~torch.utils.data.DataLoader`. Subclasses could also
>  |  optionally implement :meth:`__getitems__`, for speedup batched samples
>  |  loading. This method accepts list of indices of samples of batch and returns
>  |  list of samples.
>  |
>  |  .. note::
>  |    :class:`~torch.utils.data.DataLoader` by default constructs an index
>  |    sampler that yields integral indices.  To make it work with a map-style
>  |    dataset with non-integral indices/keys, a custom sampler must be provided.
>  |
>  |  Method resolution order:
>  |      Dataset
>  |      typing.Generic
>  |      builtins.object
>  |
>  |  Methods defined here:
>  |
>  |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'
>  |
>  |  __getitem__(self, index) -> +T_co
>  |
>  |  ----------------------------------------------------------------------
>  |  Data descriptors defined here:
>  |
>  |  __dict__
>  |      dictionary for instance variables
>  |
>  |  __weakref__
>  |      list of weak references to the object
>  |
>  |  ----------------------------------------------------------------------
>  |  Data and other attributes defined here:
>  |
>  |  __annotations__ = {}
>  |
>  |  __orig_bases__ = (typing.Generic[+T_co],)
>  |
>  |  __parameters__ = (+T_co,)
>  |
>  |  ----------------------------------------------------------------------
>  |  Class methods inherited from typing.Generic:
>  |
>  |  __class_getitem__(...)
>  |      Parameterizes a generic class.
>  |
>  |      At least, parameterizing a generic class is the *main* thing this
>  |      method does. For example, for some generic class `Foo`, this is called
>  |      when we do `Foo[int]` - there, with `cls=Foo` and `params=int`.
>  |
>  |      However, note that this method is also called when defining generic
>  |      classes in the first place with `class Foo[T]: ...`.
>  |
>  |  __init_subclass__(...)
>  |      Function to initialize subclasses.
> ```
>
> 也可以使用`Dataset??`这一个方式获取的信息比较清晰
>
> ```bash
> Init signature: Dataset()
> Source:        
> class Dataset(Generic[T_co]):
>     r"""An abstract class representing a :class:`Dataset`.
> 
>     All datasets that represent a map from keys to data samples should subclass
>     it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a
>     data sample for a given key. Subclasses could also optionally overwrite
>     :meth:`__len__`, which is expected to return the size of the dataset by many
>     :class:`~torch.utils.data.Sampler` implementations and the default options
>     of :class:`~torch.utils.data.DataLoader`. Subclasses could also
>     optionally implement :meth:`__getitems__`, for speedup batched samples
>     loading. This method accepts list of indices of samples of batch and returns
>     list of samples.
>     所有的数据集子类需要集成这一个类, 重写__getitem__
> 
>     .. note::
>       :class:`~torch.utils.data.DataLoader` by default constructs an index
>       sampler that yields integral indices.  To make it work with a map-style
>       dataset with non-integral indices/keys, a custom sampler must be provided.
>     """
> 
>     def __getitem__(self, index) -> T_co:
>         raise NotImplementedError("Subclasses of Dataset should implement __getitem__.")
> 
>     # def __getitems__(self, indices: List) -> List[T_co]:
>     # Not implemented to prevent false-positives in fetcher check in
>     # torch.utils.data._utils.fetch._MapDatasetFetcher
> 
>     def __add__(self, other: "Dataset[T_co]") -> "ConcatDataset[T_co]":
>         return ConcatDataset([self, other])
> 
>     # No `def __len__(self)` default?
>     # See NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ]
>     # in pytorch/torch/utils/data/sampler.py
> File:           e:\jhy\python\anaconda-test\.conda\lib\site-packages\torch\utils\data\dataset.py
> Type:           type
> Subclasses:     IterableDataset, TensorDataset, StackDataset, ConcatDataset, Subset, MapDataPipe
> ```

### 实战

![image-20240923194200555](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409231942661.png)

在实际使用的时候由于需要使用到图片的读取, 所以为这里要安装OpenCV` pip install opencv-python`, 也可以使用PIL里面的Image

```python
from torch.utils.data import Dataset
from PIL import Image
import os

# dir_path = "./hymenoptera_data/train/ants"
# img_path_list = os.listdir(dir_path)
# print(img_path_list)

# path = os.path.join(root_dir, lable_dir)
# print(path)
# img_path = "./hymenoptera_data/train/ants/0013035.jpg"
# img = Image.open(img_path)
# print(img.size)
# img.show()
class Mydata(Dataset):
    def __init__(self, read_dir, lable_dir):
        # 初始化
        self.root_dir = read_dir
        self.lable_dir = lable_dir
        self.path = os.path.join(self.root_dir, self.lable_dir)
        self.img_path_list = os.listdir(self.path)



    def __getitem__(self, index):
        """
        Return the image and lable of the index-th sample
        可以使对象使用引用的方式进行调用
        """
        img_name = self.img_path_list[index]
        img_path = os.path.join(self.path, img_name)
        img = Image.open(img_path)
        lable = self.lable_dir
        return img, lable

    def __len__(self):
        return len(self.img_path_list)

# 获取第一个数据集
root_dir = "./hymenoptera_data/train"
ants_lable_dir = "ants"
ants_mydata = Mydata(root_dir, ants_lable_dir)
img, lable = ants_mydata[0]
img.show()

bees_lable_dir = "bees"
bees_mydata = Mydata(root_dir, bees_lable_dir)
bees_img, bees_lable = bees_mydata[0]
bees_img.show()
```

这两个数据集可以使用`+`进行合并

```python
train_dataset = ants_mydata + bees_mydata
```

## tensorboard

TensorBoard 是一组用于数据可视化的工具。它包含在流行的开源机器学习库 Tensorflow 中。TensorBoard 的主要功能包括：

1. 可视化模型的网络架构
2. 跟踪模型指标，如损失和准确性等
3. 检查机器学习工作流程中权重、偏差和其他组件的直方图
4. 显示非表格数据，包括图像、文本和音频
5. 将高维嵌入投影到低维空间

这一个在torch里面有调用模块的, 使用`pip install tensorboard`安装

```c
from torch.utils.tensorboard import SummaryWriter

write = SummaryWriter("logs")  # 是要用这一个文件夹存放数据

# write.add_image()
for i in range(100):
    write.add_scalar("y=x", i, i)   # 画图的时候，y轴是i，x轴是i, 不同的文件需要使用不同的title

write.close()
```

> 之后使用命令`tensorboard  --logdir=logs`即可打开生成的文件(这里的参数是数据保存的那一个文件夹), 可以使用`--port=port`设置端口, 默认的时候会使用网页http://localhost:6006/进行显示

![image-20240923233445226](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409232334463.png)

### 显示图片

```python
from torch.utils.tensorboard import SummaryWriter
import numpy as np
from PIL import Image

write = SummaryWriter("logs")  # 是要用这一个文件夹存放数据
image_path = "hymenoptera_data/train/ants/0013035.jpg"

img =Image.open(image_path)
img_array = np.array(img)
# 在使用np这一个格式的时候需要加上dataformats这一个参数
# 第一个参数是窗口区分, 第二个是图片数据, 第三个是第几步(可以使用滑块换步)
write.add_image("test", img_array, 1, dataformats='HWC')  
```

![image-20240924100139724](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409241001865.png)

### 显示训练过程

```python
# 这里的jiao是训练使用的一个Moudle
# input是输入的数据
writer = SummaryWriter("logs")
writer.add_graph(jiao, input)
writer.close()
```

![image-20240927225748432](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409272257605.png)

![image-20240927225841697](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409272258867.png)

## transforms

### ToTensor

ToTensor数据类型: 使用`from torchvision import transforms`文件下面的ToTensor可以把PIL或者一个`numpy.ndarray`转换为一个tensor数据类型, 这一个数据类型实际是对图片进行一些方便神经网络的封装

```python
from torchvision import transforms
from PIL import Image

image_path = "hymenoptera_data/train/ants/0013035.jpg"
img = Image.open(image_path)

# 1. ToTensor
trans1 = transforms.ToTensor()
img_tensor = trans1(img)
```

```python
import cv2
cv_img = cv2.imread(image_path)
```

> 使用这种方式打开的文件是一个`numpy.ndarray`格式的图片

```python
from torchvision import transforms
from PIL import Image
from torch.utils.tensorboard import SummaryWriter

image_path = "hymenoptera_data/train/ants/0013035.jpg"
img = Image.open(image_path)

writer = SummaryWriter("logs")

# 1. ToTensor
trans1 = transforms.ToTensor()
img_tensor = trans1(img)
print(img_tensor)

writer.add_image("ToTensor", img_tensor)
```

可以使用这一个方式显示图片

![image-20240924122156943](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409241221068.png)

### Compose

把多个transform集合在一起, 在实际使用的时候可以把多个处理的transform放在一起, 依次执行

这一个初始化的时候的参数是一个数列

```python
trans4 = transforms.Compose([
    transforms.Resize((300, 300)),
    transforms.ToTensor()
])
img_tensor = trans4(img)
writer.add_image("Compose", img_tensor, 1)
```

### Normalize

用于处理图片的差异, 参数是均值和标准差, 需要按照通道的个数传进去一个数列

这一个参数数值是需要计算获取的

实际的normal处理事input[channel] = (input[channel] - mean[channel]) / std[channel]

如公式所示，通过减去均值并除以标准差，我们可以将图像数据的分布转换为标准正态分布（均值为0，标准差为1）。这样，模型在训练过程中就可以更容易地学习到数据的特征。

> 数据标准化：如上所述，transforms.Normalize()函数可以对图像数据进行标准化处理，使数据分布符合标准正态分布。这有助于模型更快地收敛，并提高模型的性能。
> 提高模型泛化能力：通过对数据进行标准化，我们可以减少模型对特定数据集的过拟合，从而提高模型在未见过的数据上的泛化能力。
> 加速模型训练：标准化的数据可以使模型在训练过程中更快地学习到数据的特征，从而加速模型的训练速度。

```python
# 2. Normalize
print(img_tensor[0][0][0])
trans2 = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
img_tensor = trans2(img_tensor)
print(img_tensor[0][0][0])
```

### Resize

`size((h, w))`或者`size(int)`第一个会把图像进行变换, 第二个会使得这一个图形比较小的一个边和这一个数字一样

 If the image is torch Tensor, it is expected to have [..., H, W] shape, where ... means a maximum of two leading dimensions

也可以使用PIL格式的数据

> ToTensor的转换格式是
>
> Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]

```python
print(img.size)
trans3 = transforms.Resize((300, 300))
img_tensor = trans3(img)
print(img_tensor)
print(img_tensor.size)
```

## Dataset

这里选取其中一个数据集

![image-20240924184250991](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409241842098.png)

> The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.

```python
import torchvision
from PIL import Image
 
# 获取CIFAR10数据集
train_set = torchvision.datasets.CIFAR10(root="./dataset", train=True, download=True)
test_set = torchvision.datasets.CIFAR10(root="./dataset", train=False, download=True)

# 获取数据集的大小
print(len(train_set))
print(len(test_set))

# 获取数据集的标签
print(train_set.classes)
print(test_set.classes)

print(train_set[1])
print(test_set[1])

# 显示图片
train_set[1][0].show()
test_set[1][0].show()
```

> ```bash
> PS E:\JHY\python\anaconda-test> & e:/JHY/python/anaconda-test/.conda/python.exe e:/JHY/python/anaconda-test/my_dataset.py
> Files already downloaded and verified
> Files already downloaded and verified
> 50000
> 10000
> ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
> ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
> (<PIL.Image.Image image mode=RGB size=32x32 at 0x218909AD370>, 9)
> (<PIL.Image.Image image mode=RGB size=32x32 at 0x218909AD160>, 8)
> ```
>
> ![image-20240924191954360](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409241919607.png)

可以直接使用工具集在获取数据集的时候进行加工

```python
import torchvision
from PIL import Image
from torch.utils.tensorboard import SummaryWriter
# 初始化数据集的转换
dataset_tramsforms = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor()
])


# 获取CIFAR10数据集, 直接进行转换
train_set = torchvision.datasets.CIFAR10(root="./dataset", train=True, download=True, transform=dataset_tramsforms)
test_set = torchvision.datasets.CIFAR10(root="./dataset", train=False, download=True, transform=dataset_tramsforms)

writer = SummaryWriter("logs")
for i in range(10):
    img, lable = train_set[i]
    writer.add_image("train_set", img, i)

for i in range(10):
    img, lable = test_set[i]
    writer.add_image("test_set", img, i)

writer.close()
# print(test_set[0])
```

## Dataloader

一个加载器, 把数据加载到神经网络里面, 从dataset里面取数据, 取的方法和数量是需要使用dataloader进行设置的

### 参数

- **dataset** ([*Dataset*](https://pytorch.org/docs/1.0.0/data.html#torch.utils.data.Dataset)) – dataset from which to load the data.
- **batch_size** ([*int*](https://docs.python.org/3/library/functions.html#int)*,* *optional*) – how many samples per batch to load (default: `1`).一次加载的数量
- **shuffle** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – set to `True` to have the data reshuffled at every epoch (default: `False`).这数据是不是需要打乱, 一般使用true
- **sampler** ([*Sampler*](https://pytorch.org/docs/1.0.0/data.html#torch.utils.data.Sampler)*,* *optional*) – defines the strategy to draw samples from the dataset. If specified, `shuffle` must be False.
- **batch_sampler** ([*Sampler*](https://pytorch.org/docs/1.0.0/data.html#torch.utils.data.Sampler)*,* *optional*) – like sampler, but returns a batch of indices at a time. Mutually exclusive with `batch_size`, `shuffle`, `sampler`, and `drop_last`.
- **num_workers** ([*int*](https://docs.python.org/3/library/functions.html#int)*,* *optional*) – how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: `0`)是不是使用多进程进行加载, Windows下面这一个可能有问题
- **collate_fn** (*callable**,* *optional*) – merges a list of samples to form a mini-batch.
- **pin_memory** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – If `True`, the data loader will copy tensors into CUDA pinned memory before returning them.
- **drop_last** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – set to `True` to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If `False` and the size of dataset is not divisible by the batch size, then the last batch will be smaller. (default: `False`)除不尽的时候剩下的那几张牌是不是要舍去
- **timeout** (*numeric**,* *optional*) – if positive, the timeout value for collecting a batch from workers. Should always be non-negative. (default: `0`)
- **worker_init_fn** (*callable**,* *optional*) – If not `None`, this will be called on each worker subprocess with the worker id (an int in `[0, num_workers - 1]`) as input, after seeding and before data loading. (default: `None`)

```python
import torchvision
from torch.utils.data import DataLoader

# 初始化数据集的转换
dataset_tramsforms = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor()
])

test_set = torchvision.datasets.CIFAR10(root="./dataset", train=False, download=True, transform=dataset_tramsforms)

test_loader = DataLoader(test_set, batch_size=4, shuffle=True, num_workers=0, drop_last=False)

img, targrt = test_set[0]
print(img.shape)
print(targrt)

for data in test_loader:
    imgs, targets = data
    print(imgs.shape)
    print(targets)
    break
```

> ```bash
> PS E:\JHY\python\anaconda-test> & e:/JHY/python/anaconda-test/.conda/python.exe e:/JHY/python/anaconda-test/my_dataloader.py
> Files already downloaded and verified
> torch.Size([3, 32, 32])
> 3
> torch.Size([4, 3, 32, 32])
> tensor([5, 9, 5, 9])
> ```
>
> 这里是把图片按照四个一组进行打包, 石笋这一个返回的是两个tensor数据类型

```python
wrier = SummaryWriter("logs")
step = 0
for data in test_loader:
    imgs, targets = data
    wrier.add_images("test_loader", imgs, step)  # 这里使用images加载多个图片
    step += 1
```

![image-20240925191028788](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409251910926.png)

## 网络搭建

这一个系列的工具主要是在torch.nn里面

+ Container: 一个神经网络的框架
+ Convolution Layers: 卷积层
+ Pooling Layers: 池化层
+ Padding Layers: 
+ ...

### Container

这里面最常用的是Moudle这一个类, 是所有神经网络的基类

实际的处理数据的函数是每一个类里面的forward这一个函数

```python
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
       x = F.relu(self.conv1(x)) # 一次卷积, 一次非线性处理
       return F.relu(self.conv2(x)) # 再次进行卷积非线性获取输出
```

```c
from torch import nn
import torch

class My_Moudle(nn.Module):
    def __init__(self):
        super(My_Moudle, self).__init__()


    def forward(self, x):
        return x + 1
    

if __name__ == "__main__":
    x = torch.tensor(1.0)
    model = My_Moudle()
    y = model(x)
    print(y)
```

```bash
PS E:\JHY\python\anaconda-test> & e:/JHY/python/anaconda-test/.conda/python.exe e:/JHY/python/anaconda-test/my_moudle.py
tensor(2.)
```

### Sequential

```python
# Example of using Sequential
model = nn.Sequential(
          nn.Conv2d(1,20,5),
          nn.ReLU(),
          nn.Conv2d(20,64,5),
          nn.ReLU()
        )

# Example of using Sequential with OrderedDict
model = nn.Sequential(OrderedDict([
          ('conv1', nn.Conv2d(1,20,5)),
          ('relu1', nn.ReLU()),
          ('conv2', nn.Conv2d(20,64,5)),
          ('relu2', nn.ReLU())
        ]))
```

### 层

#### 卷积层Convolution Layers

这里的里面比较常用的是Conv1d, Conv2d, 这一个文件的操作实际是对functional这一个文件的操作进行的封装

[conv_arithmetic:A technical report on convolution arithmetic in the context of deep learning - GitCode](https://gitcode.com/gh_mirrors/co/conv_arithmetic/blob/master/README.md?utm_source=csdn_github_accelerator&isLogin=1)

+ Conv2d

```python
torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)
```

<img src="https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409261824662.png" alt="image-20240926182432173" style="zoom:150%;" />

in_channels

​	输入的通道数

out_channels

​	输出的通道数, 实际是使用几个不同的卷积核生成多个结果

kernel_size
  卷积核的大小，一般我们会使用5x5、3x3这种左右两个数相同的卷积核，因此这种情况只需要写kernel_size = 5这样的就行了。如果左右两个数不同，比如3x5的卷积核，那么写作kernel_size = (3, 5)，注意需要写一个tuple，而不能写一个列表（list）。里面的数字是不需要设置的, 会在训练的时候进行调整的

stride = 1
  卷积核在图像窗口上每次平移的间隔，即所谓的步长。这个概念和Tensorflow等其他框架没什么区别，不再多言。

padding 

​	拓展的大小

dilation空洞卷积, 一般不使用

![img](https://raw.gitcode.com/gh_mirrors/co/conv_arithmetic/files/master/gif/dilation.gif)

groups

bias=True

![image-20240926193519679](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409261935794.png)

> 原理
>
> ```python
> torch.nn.functional.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) 
> ```
>
> **Parameters:**
>
> - **input** – input tensor of shape (minibatch×in_channels×iH×iW)输入的需要是一个四维的数组, 可以使用reship函数进行变换
> - **weight** – filters of shape (out_channels×in_channelsgroups×kH×kW)
> - **bias** – optional bias tensor of shape (out_channelsout_channels). Default: `None`
> - **stride** – the stride of the convolving kernel. Can be a single number or a tuple (sH, sW). Default: 1
> - **padding** – implicit zero paddings on both sides of the input. Can be a single number or a tuple (padH, padW). Default: 0 填充
> - **dilation** – the spacing between kernel elements. Can be a single number or a tuple (dH, dW). Default: 1
> - **groups** – split input into groups, in_channelsin_channels should be divisible by the number of groups. Default: 1
>
> ```python
> import torch
> import torch.nn.functional as F
> 
> input = torch.tensor([[1, 2, 0, 3, 1], 
>                       [0, 1, 2, 3, 1], 
>                       [1, 2, 1, 0, 0], 
>                       [5, 2, 3, 1, 1], 
>                       [2, 1, 0, 1, 1]])
> 
> kernel = torch.tensor([[1, 2, 1],
>                       [0, 1, 0],
>                       [2, 1, 0]])
> """
> 结果
> 1*1 + 2*2 + 0*1 + 0*0 + 1*1 + 0*2 + 1*2 + 2*1 + 0*3 = 10
> 1*2 + 2*0 + 1*3 + 0*1 + 1*2 + 0*3 + 2*2 + 1*3 + 0*0 = 12
> ...
> """
> 
> # 这里的参数是(N, C, H, W), N是batch_size(每一次训练的时候图片的数量), C是通道数(二维向量是1), H是高, W是宽
> input = torch.reshape(input, (1, 1, 5, 5))
> kernel = torch.reshape(kernel, (1, 1, 3, 3))
> 
> print(input.shape)
> print(kernel.shape)
> 
> output = F.conv2d(input, kernel, stride=1, padding=0)  # 这里的stride是步长, padding是填充
> print(output)
> 
> ```
>
> ```bash
> PS E:\JHY\python\anaconda-test> & e:/JHY/python/anaconda-test/.conda/python.exe e:/JHY/python/anaconda-test/my_conv.py
> torch.Size([1, 1, 5, 5])
> torch.Size([1, 1, 3, 3])
> tensor([[[[10, 12, 12],
>           [18, 16, 16],
>           [13,  9,  3]]]])
> ```

```python
import torch
import torchvision
from torch.utils.data import DataLoader
from torch import nn
from torch.utils.tensorboard import SummaryWriter

dateset = torchvision.datasets.CIFAR10(root="./dataset", train=False, download=True, 
                                       transform=torchvision.transforms.ToTensor())

dataloader = DataLoader(dateset, batch_size=64)

class JIAO(nn.Module):
    def __init__(self):
        super(JIAO, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=0)
    
    def forward(self, x):
        x = self.conv1(x)
        return x
    
jiao = JIAO()
"""
JIAO(
  (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1))
)
"""
print(jiao) 

writer = SummaryWriter("logs")
step = 0

for data in dataloader:
    imgs, targets = data
    output = jiao(imgs)
    print(imgs.shape) # torch.Size([64, 3, 32, 32])
    print(output.shape) # torch.Size([64, 6, 30, 30])
    writer.add_images("input", imgs, step)
    ## 这里的3是输出通道数, 30是输出的高和宽, 使用-1是自动计算, 3是可以显示的通道数
    output = torch.reshape(output, (-1, 3, 30, 30)) 
    writer.add_images("output", output, step)
    step += 1

writer.close()
```

![image-20240926193224030](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409261932362.png)

#### 池化层 Pooling layers

MaxPool: 最大池化

MaxUnpool: 上采样

...

最常使用的MaxPool2d

+ MaxPool2d

class torch.nn.MaxPool2d(kernel_size, stride=None, padding=0,  dilation=1,return_indices=False, ceil_mode=False)

> - **kernel_size** – the size of the window to take a max over生成的窗口
> - **stride** – the stride of the window. Default value is `kernel_size`
> - **padding** – implicit zero padding to be added on both sides
> - **dilation** – a parameter that controls the stride of elements in the window
> - **return_indices** – if `True`, will return the max indices along with the outputs. Useful when Unpooling later
> - **ceil_mode** – when True, will use ceil instead of floor to compute the output shape当一个取值范围不能全部覆盖的时候是不是进行舍去操作

#### 垫层Padding Layer

使用数字进行填充, 几乎用不到

#### 非线性激活Non-Linear Activactions

主要使用的是ReLu, Sigmoid, Tanh这几个算法使用的方式和前面的基本一样

#### Normalization Layers

这一个不常使用, 主要用的函数是BatchNorm2d这一个函数, 它的参数mun_features指的是channel的数量

#### Liner Layer

参数实际是input和output的大小, 权重是按照输入的数据计算的

例: 实际的计算可以把一个1x1x4096的数据转换为1x1x1000

#### flatten

把输入的数据展开为一行

#### 损失函数和反向传播

+ L1loss

```python
classtorch.nn.L1Loss(size_average=None, reduce=None, reduction='elementwise_mean')
```

使用这一个可以计算结果和真实值之间的绝对值, 可以使用reduction控制是平均还是求和

```python
>>> loss = nn.L1Loss()
>>> input = torch.randn(3, 5, requires_grad=True)
>>> target = torch.randn(3, 5)
>>> output = loss(input, target)
>>> output.backward()
```

input和target需要是相同的图形

+ MSELOSS

计算平方差的平均数

+ CrossEntorpyLoss交叉熵

![image-20240927232316235](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409272323402.png)

有多个分类的时候, 比如说有三个分类的时候, 一张图片的输出是[0.1, 0.2, 0.3], 分别标准0, 1, 2这三个类, 实际的分类应该是1

实际计算的时候x是获取的[0.1, 0.2, 0.3], class是1

loss(x, class) = -0.2 + log(exp(0.1) + exp(0.2) + exp(0.3)) 

> exp是指数函数$e^x$

 ```python
 x = torch.tensor([0.1, 0.2, 0.3])
 y = torch.tensor([1])
 x = torch.reshape(x, (1, 3))
 loss_cross = nn.CrossEntorpyLoss()
 result_cross = loss_cross(x, y)
 print(result_cross)
 ```

```python

from torch import nn
from torch.nn import Conv2d
from torch.nn import MaxPool2d
from torch.nn import Flatten
import torch
from torch.utils.tensorboard import SummaryWriter
import torchvision
from torchvision import transforms
from torch.utils.data import DataLoader

# 初始化数据集的转换
dataset_tramsforms = transforms.Compose([
    torchvision.transforms.ToTensor()
])

train_set = torchvision.datasets.CIFAR10(root="./dataset", train=False, download=True, transform=dataset_tramsforms)
dataloader = DataLoader(train_set, batch_size=64)


class JIAO(nn.Module):
    def __init__(self):
        super(JIAO, self).__init__()
        self.moudle1 = nn.Sequential(
            Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=4),
            nn.MaxPool2d(kernel_size=2),
            Conv2d(32, 32, 5, padding=2),
            nn.MaxPool2d(2),
            Conv2d(32, 64, 5, padding=2),
            nn.MaxPool2d(2),
            Flatten(),
            nn.Linear(64*4*4, 64),
            nn.Linear(64, 10)
        )
    
    def forward(self, x):
        x = self.moudle1(x)
        return x
    
jiao = JIAO()
loss = nn.CrossEntropyLoss()

for data in dataloader:
    imgs, labels = data
    output = jiao(imgs)
    result_loss = loss(output, labels)
    print(result_loss)
    result_loss.backward() # 反向传播, 可以使用这一个计算出来一个梯度, 之后进一步优化
    break
```

#### 优化器optim

```python
for input, target in dataset:
    optimizer.zero_grad()
    output = model(input)
    loss = loss_fn(output, target)
    loss.backward()
    optimizer.step()
```

> - **params** (*iterable*) – iterable of parameters to optimize or dicts defining parameter groups, 模型的参数
> - **rho** ([*float*](https://docs.python.org/3/library/functions.html#float)*,* *optional*) – coefficient used for computing a running average of squared gradients (default: 0.9)学习速率
> - **eps** ([*float*](https://docs.python.org/3/library/functions.html#float)*,* *optional*) – term added to the denominator to improve numerical stability (default: 1e-6)
> - **lr** ([*float*](https://docs.python.org/3/library/functions.html#float)*,* *optional*) – coefficient that scale delta before it is applied to the parameters (default: 1.0)
> - **weight_decay** ([*float*](https://docs.python.org/3/library/functions.html#float)*,* *optional*) – weight decay (L2 penalty) (default: 0)

#### 现有的网络模型models

[torchvision.models — PyTorch master documentation](https://pytorch.org/docs/0.4.1/torchvision/models.html)

比较常用的时候vgg16, vgg19

```python
torchvision.models.vgg16(pretrained=False, **kwargs)
```

**pretrained** ([*bool*](https://docs.python.org/3/library/functions.html#bool)) – If True, returns a model pre-trained on ImageNet(在ImageNet里面训练出来的数据, 这一个网络里面可以进行1000个分类, 但是由于这一个数据集比较大, 所以方便下载)

```python
vgg16_true.add_module("add_linear", torch.nn.Linear(1000, 10)) # 添加新的层
```



![image-20240928170922809](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409281709126.png)

```python
vgg16_true.classifier.add_module("add_linear", torch.nn.Linear(1000, 10)) # 添加新的层(在现有的层里面)
```

![image-20240928171141329](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409281711469.png)

也可以使用

```python
vgg16_true.classifier[6] = nn.Linear(4096, 10) # 修改原有的层
```

![image-20240928171449315](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409281714434.png)

## 实战

![image-20240927223634375](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409272236547.png)

首先把数据进行一次卷积扩展一下层数, 之后进行一次最大值池化, 之后再次卷积一次, 然后最大值池化, 再重复一下然后展平, 最后使用线性层处理

```python
from torch import nn
from torch.nn import Conv2d
from torch.nn import MaxPool2d
from torch.nn import Flatten
import torch
from torch.utils.tensorboard import SummaryWriter


class JIAO(nn.Module):
    def __init__(self):
        super(JIAO, self).__init__()
        # self.conv1 = Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=4)
        # self.maxpool1 = nn.MaxPool2d(kernel_size=2)
        # self.conv2 = Conv2d(32, 32, 5, padding=2)
        # self.maxpool2 = MaxPool2d(2)
        # self.conv3 = Conv2d(32, 64, 5, padding=2)
        # self.maxpool3 = MaxPool2d(2)
        # self.flatten = Flatten()
        # self.linear1 = nn.Linear(64*4*4, 64)
        # self.linear2 = nn.Linear(64, 10)
        # 可以使用一下的代码进行简化
        self.moudle1 = nn.Sequential(
            Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=4),
            nn.MaxPool2d(kernel_size=2),
            Conv2d(32, 32, 5, padding=2),
            nn.MaxPool2d(2),
            Conv2d(32, 64, 5, padding=2),
            nn.MaxPool2d(2),
            Flatten(),
            nn.Linear(64*4*4, 64),
            nn.Linear(64, 10)
        )
    
    def forward(self, x):
        x = self.moudle1(x)
        return x
    
jiao = JIAO()
print(jiao)

input = torch.randn(64, 3, 32, 32)
output = jiao(input)
print(output.shape)

writer = SummaryWriter("logs")
writer.add_graph(jiao, input)
writer.close()
```

+ 加反馈以及优化器

```python
from torch import nn
from torch.nn import Conv2d
from torch.nn import MaxPool2d
from torch.nn import Flatten
import torch
from torch.utils.tensorboard import SummaryWriter
import torchvision
from torchvision import transforms
from torch.utils.data import DataLoader

# 初始化数据集的转换
dataset_tramsforms = transforms.Compose([
    torchvision.transforms.ToTensor()
])

train_set = torchvision.datasets.CIFAR10(root="./dataset", train=False, download=True, transform=dataset_tramsforms)
dataloader = DataLoader(train_set, batch_size=64)


class JIAO(nn.Module):
    def __init__(self):
        super(JIAO, self).__init__()
        self.moudle1 = nn.Sequential(
            Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=4),
            nn.MaxPool2d(kernel_size=2),
            Conv2d(32, 32, 5, padding=2),
            nn.MaxPool2d(2),
            Conv2d(32, 64, 5, padding=2),
            nn.MaxPool2d(2),
            Flatten(),
            nn.Linear(64*4*4, 64),
            nn.Linear(64, 10)
        )
    
    def forward(self, x):
        x = self.moudle1(x)
        return x
    
jiao = JIAO()
loss = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(jiao.parameters(), lr=0.01)
for i in range(20):
    for data in dataloader:
        imgs, labels = data
        output = jiao(imgs)
        result_loss = loss(output, labels)
        optimizer.zero_grad() # 梯度清零
        result_loss.backward() # 反向传播
        optimizer.step() # 更新参数
        print(result_loss)
        # break

```

## 加载保存

+ 第一种结构加参数

```python
torch.save(vgg16_true, "vgg16.pth") # 保存模型
```

> 记录了模型以及模型里面的参数

```python
torch.load("vgg16.pth")
```

> 使用这一个方法的时候, 如果加载的是一个自己定义的类, 这一个类需要引入一下

+ 方法二只记录参数

```python
torch.save(vgg16_true.state_dict(), "vgg16.pht") # 保存模型
```

恢复

```python
vgg16 = torchvision.models.vgg16(pretrained=False)
vgg16.load_state_dict(torch.load("vgg16.pht")) # 加载模型
```

> **注: **使用这一个方式的时候需要初始化一个Model对象再

## 完整的训练

```python
import torchvision
import torch
from torch.utils.tensorboard import SummaryWriter
# train_data = torchvision.datasets.ImageNet(root="./dataset_ImageNet", train=True, download=True,
#                                            transform=torchvision.transforms.ToTensor())

# vgg16_true = torchvision.models.vgg16(pretrained=True) # 加载预训练模型, 使用数据集训练好的参数
# vgg16_flase = torchvision.models.vgg16(pretrained=False) # 不加载预训练模型, 使用默认参数

# print(vgg16_true)
# print(vgg16_flase)

train_data = torchvision.datasets.CIFAR10(root="./dataset", train=True, download=True,
                                            transform=torchvision.transforms.ToTensor())
test_data = torchvision.datasets.CIFAR10(root="./dataset", train=False, download=True,
                                            transform=torchvision.transforms.ToTensor())
# vgg16_true.add_module("add_linear", torch.nn.Linear(1000, 10)) # 添加新的层
# vgg16_true.classifier.add_module("add_linear", torch.nn.Linear(1000, 10)) # 添加新的层(在现有的层里面)
# vgg16_true.classifier[6] = torch.nn.Linear(4096, 10) # 修改原有的层
# print(vgg16_true)

train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=64)
test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=64)


class JIAO(torch.nn.Module):
    def __init__(self):
        super(JIAO, self).__init__()
        self.moudle1 = torch.nn.Sequential(
            torch.nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=2),
            torch.nn.MaxPool2d(kernel_size=2),
            torch.nn.Conv2d(32, 32, 5, padding=2),
            torch.nn.MaxPool2d(2),
            torch.nn.Conv2d(32, 64, 5, padding=2),
            torch.nn.MaxPool2d(2),
            torch.nn.Flatten(),
            torch.nn.Linear(64*4*4, 64),
            torch.nn.Linear(64, 10)
        )
    
    def forward(self, x):
        x = self.moudle1(x)
        return x

# 执行的是当前文件
if __name__ == "__main__":

    jiao = JIAO()
    #损失函数
    loss_fn = torch.nn.CrossEntropyLoss()
    #优化器
    learn_rate = 0.01 # 1e-2
    optimizer = torch.optim.SGD(jiao.parameters(), lr=learn_rate)

    writer = SummaryWriter("logs")

    #训练
    epoch = 20
    total_step = 0
    for i in range(epoch):
        for data in train_dataloader:
            imgs, labels = data
            output = jiao(imgs)
            result_loss = loss_fn(output, labels)
            optimizer.zero_grad() # 梯度清零
            result_loss.backward() # 反向传播
            optimizer.step() # 更新参数
            # print(result_loss)
            # break
            total_step = total_step + 1
            if total_step % 100 == 0:
                print("epoch: ", i, "step: ", total_step, "loss: ", result_loss)
                writer.add_scalar("train_loss", result_loss, total_step)
                
        # 测试
        total_test_loss = 0
        with torch.no_grad():
            for data in test_dataloader:
                img, target = data
                output = jiao(img)
                loss = loss_fn(output, target)
                total_test_loss += loss
        
        writer.add_scalar("test_loss", total_test_loss, i)
        print("epoch: ", i, "loss: ", total_test_loss)

        torch.save(jiao, "./my_mod/model_{}.pth".format(i))

    writer.close()
```



## 正确率

在实际训练的时候输出的是各个选项的概率, 比如一个只有两个数据的分类, 可能输出1. [0.1, 0.2], 2. [0.3, 0.4]

使用Argmax可以获取到Preds = [1\][1\]

如果Input target是[0\][1\]

使用Preds == Input target可以获取[False, True].sum = 1

```python
import torch

outputs = torch.tensor([[0.1, 0.2], 
                        [0.3, 0.4], 
                        [0.5, 0.2]])

# output: tensor([1, 1, 0])
print(outputs.argmax(dim=1)) # dim=1表示按行取最大值的索引
preds = outputs.argmax(dim=1)
targets = torch.tensor([1, 0, 0])
# output: tensor([ True, False,  True])
print(targets == preds)
print((targets == preds).sum().item()) # 2
```



```python
# import torch

# outputs = torch.tensor([[0.1, 0.2], 
#                         [0.3, 0.4], 
#                         [0.5, 0.2]])

# # output: tensor([1, 1, 0])
# print(outputs.argmax(dim=1)) # dim=1表示按行取最大值的索引
# preds = outputs.argmax(dim=1)
# targets = torch.tensor([1, 0, 0])
# # output: tensor([ True, False,  True])
# print(targets == preds)
# print((targets == preds).sum().item()) # 2

import torch
import torchvision
from my_model_pretraines import JIAO
test_data = torchvision.datasets.CIFAR10(root="./dataset", train=False, download=True,
                                            transform=torchvision.transforms.ToTensor())

test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=64)

module = torch.load("my_mod/model_19.pth")

# 测试一下准确率

correct = 0
total = len(test_data)
with torch.no_grad():
    for data in test_dataloader:
        imgs, labels = data
        outputs = module(imgs)
        preds = outputs.argmax(dim=1)
        correct += (preds == labels).sum().item()

print("准确率: ", correct / total)
```

## 使用GPU

+ 方法一

网络模型, 数据(输入, 标注)以及损失函数有

在模型使用的时候用`model = model.cuda()`, 误差函数也可以使用`loss_fn = loss_fn.cuda()`

```python
def train(jiao : JIAO, epoch : int, learn_rate : float, output_name : str, train_dataloader):
    jiao = jiao.cuda()
    total_step = 0
    #损失函数
    loss_fn = torch.nn.CrossEntropyLoss()
    loss_fn = loss_fn .cuda()
    optimizer = torch.optim.SGD(jiao.parameters(), lr=learn_rate)

    for i in range(epoch):
        for data in train_dataloader:
            imgs, labels = data
            imgs = imgs.cuda()
            labels = labels.cuda()
            output = jiao(imgs)
            result_loss = loss_fn(output, labels)
            optimizer.zero_grad() # 梯度清零
            result_loss.backward() # 反向传播
            optimizer.step()
            total_step = total_step + 1
            if total_step % 100 == 0:
                print("epoch: ", i, "step: ", total_step, "loss: ", result_loss)
    torch.save(jiao, output_name)
```

+ 方法二

```python
device = torch.device("cpu") # cuda / cuda:0 / cuda:1 定义一个设备
# 之后把模型之类的使用to定义到这一个设备
jiao = jiao.to(device)
loss_fn = loss_fn.to(device)
```



## 测试环境

[欢迎使用 Colaboratory - Colab (google.com)](https://colab.research.google.com/)

![image-20240928221225921](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409282212141.png)

![image-20240928221248593](https://picture-01-1316374204.cos.ap-beijing.myqcloud.com/image/202409282212783.png)

## 模型的测试

使用以及训练好的模型, 提供输入, 可以在github上面找, 搜索github

```python
import PIL
import PIL.Image
import torchvision
import torch
from my_model_pretraines import JIAO

img_path = "./img/shi.png"
img = PIL.Image.open(img_path)
classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
print(img)
img = img.convert("RGB")

transform = torchvision.transforms.Compose([
    torchvision.transforms.Resize((32, 32)),
    torchvision.transforms.ToTensor()
])

img = transform(img)
# print(img.shape)
moudle = torch.load("my_mod/model.pth") # 如果gpu训练想在cpu使用，需要加上map_location=torch.device("cpu")
moudle = moudle.cuda()
img = torch.reshape(img, (1, 3, 32, 32))
img = img.cuda()
moudle.eval()   # 把这一个模型设置为测试模式
with torch.no_grad():
    output = moudle(img)
print(output)
result = output.argmax(dim=1)
print("filename = {}, class = {}".format(img_path, classes[result.item()]))

```

